{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hi-there","title":"Hi there \ud83d\udc4b","text":"<p>Welcome To My Blog.</p> <p>The purpose of this personal blog is to host my personal projects, and any articles on subjects I feel comfortable enough to write on.</p> <p>Current Objectives</p> documenting notes documenting personal projects book recommendations"},{"location":"#learning","title":"Learning \ud83e\udd13","text":"<p>Cataloging my learning for future reference</p>"},{"location":"#effective-learning-strategies","title":"Effective Learning Strategies:","text":"<ol> <li>Pre-Learning:<ul> <li>Plan efficiently with clear goals and deadlines.</li> <li>Identify the most efficient learning approach and prioritize key topics.</li> </ul> </li> <li>Learning:<ul> <li>Dedicate focused time to learning.</li> <li>Eliminate distractions for efficient and effective learning.</li> </ul> </li> <li>Study Sessions:<ol> <li>Focus:<ul> <li>Focus by staring at a point before studying.</li> <li>Take random 10-second breaks breaks for better retention.</li> <li>Stay alert with deep breaths or physical activity.</li> <li>Limit study sessions to 90 minutes for intense concentration.</li> </ul> </li> <li>Drilling:<ul> <li>Align drills with the goal context.</li> <li>Constantly target and improve weaknesses for maximum expertise.</li> </ul> </li> </ol> </li> <li>Post-Study Practices:<ul> <li>Use non-sleep deep rest or meditation for 1-10 minutes after studying to improve retention..</li> </ul> </li> </ol>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#_1","title":"","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#_2","title":",","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#d","title":"d","text":"<ul> <li>Post 1</li> <li>Post 1</li> <li>Post 2</li> <li>Post 2</li> </ul>"},{"location":"tags/#g","title":"g","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#o","title":"o","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#t","title":"t","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"AWS/","title":"Notes For AWS Certifications","text":""},{"location":"AWS/AWSBackup/","title":"AWS Backup","text":"<ul> <li><code>fully managed</code>data-protection (backup/restore) service</li> <li><code>consolidate management</code>into one place</li> <li>across accounts and regions</li> <li>supports a wide range of aws products</li> <li>ec2. ebs, efs, rds, ddb, s3 ...etc</li> <li>Backup plans</li> <li><code>frequency</code> : how often to make snapshots</li> <li><code>window</code> : time that backups begin and duration</li> <li><code>lifecycle</code> : transition to cold storage</li> <li><code>vault</code></li> <li><code>region copy</code> : copy backups from one region to another</li> <li>Resources : what resources are backed up</li> <li><code>Vaults</code> : Backup destination</li> <li>assign kms key for encryption</li> <li>default read and write</li> <li><code>vault lock</code> : write once, read many, 72 hour cool off, then even aws can't delete it</li> <li><code>On-Demand</code> : manual backups created as needed</li> <li><code>Point in time recovery</code> : restore resource to a specific date and time</li> </ul>"},{"location":"AWS/DomainNameSystem101/","title":"DNS 101","text":"<ul> <li>a discover service</li> <li>finds the IP address for a given domain name</li> <li>huge and distributed</li> <li>DNS Root Servers ran by 12 large organizations</li> <li>DNS Root Zone managed by IANA</li> <li> <p>a registry is an organization to maintains the zones for top level domains</p> </li> <li> <p><code>DNS Client</code> : software on device being used</p> </li> <li><code>Resolver</code> : software on device or server that queries DNS on your behalf</li> <li><code>Zone</code> : a part of the DNS database</li> <li><code>Zonefile</code> : physical database for a zone</li> <li><code>Nameserver</code> : where zonefiles are hosted</li> </ul> <p>Getting the IP:</p> <pre><code>  sequenceDiagram\n    Client -&gt;&gt; DNS Root Servers: amazon.com\n    DNS Root Servers -&gt;&gt; Client : Go to top-level-domain server (.com) server\n    Client -&gt;&gt; top-level-domain server (.com) server: amazon.com\n    .com server -&gt;&gt; Client : go to nameserver\n    Client --&gt;&gt; name server: amazon.com\n    name server --&gt;&gt; Client : Here is the IP address</code></pre>"},{"location":"AWS/DomainNameSystem101/#dns-record-types","title":"DNS Record Types","text":""},{"location":"AWS/DomainNameSystem101/#nameserver-ns","title":"Nameserver (NS)","text":"<ul> <li>enable delegation end to end</li> </ul> <p><code>root zone --&gt; TLD zone --&gt; domain zone</code></p>"},{"location":"AWS/DomainNameSystem101/#a-and-aaaa-records","title":"A and AAAA Records","text":"<ul> <li>map host names to IP</li> <li>A maps to IPv4</li> <li>AAAA maps to IPv6</li> </ul>"},{"location":"AWS/DomainNameSystem101/#canonical-name-cname-records","title":"Canonical Name (CNAME) Records","text":"<p>When a specific server performs multiple services:   - create multiple names and point them to the same A server record     - so they point the same IP   - can't point to IP address only to names</p>"},{"location":"AWS/DomainNameSystem101/#mx-records","title":"MX Records","text":"<ul> <li>used to find mail servers for a specific domain</li> </ul>"},{"location":"AWS/DomainNameSystem101/#txt-records","title":"TXT Records","text":"<ul> <li>used to prove domain ownership</li> <li>and detecting spam</li> </ul>"},{"location":"AWS/DomainNameSystem101/#time-to-live-ttl","title":"Time To Live TTL","text":"<ul> <li>how long a result is cached at a resolver server (probably ISP)</li> <li>used for quicker access</li> </ul>"},{"location":"AWS/EC2/","title":"Elastic Compute Cloud (EC2)","text":"<ul> <li><code>virtual machines</code> known as instances</li> <li>private service, configured to run in a <code>VPC</code> network</li> <li>is launched into a specific subnet, AZ resilient</li> <li>different sizes and capabilities available</li> <li>on-demand billing per second</li> <li>charged for compute, storage, and external software</li> <li>storage can be local on-host or Elastic Block Store (EBS)</li> <li>will be charged for EBS even if instance is stopped</li> <li>states : running,  stopped, terminated</li> <li>windows rdp port 3389, linux ssh port 22</li> </ul>"},{"location":"AWS/EC2/#amazon-machine-image-ami","title":"Amazon Machine Image (AMI)","text":"<ul> <li>has attached permissions defining who can use it</li> <li>used to create an EC2 or from an EC2</li> <li>contains the root volume, the drive that boots the operating system</li> <li>block device mapping, used to determine which volume is root volume and which is data volume</li> </ul> <p><code>Permission Options:</code> 1. Public Access 2. Owner Only 3. Specific AWS Accounts</p> <ul> <li>used to launch ec2 instances</li> <li>can be created from an existing instance</li> <li>can get from aws or community or marketplace</li> <li>regional</li> </ul> <p><code>Block device mapping</code> - ebs snapshots created from instance ami is created from - block devices mapping is a table that links, the newly created snapshots with the device id the snapshots had in the original ec2 instance - when ami is used to create a new instance   - snapshots are used to create new ebs volumes   - the volumes then are attached to the new instance using the same device ids as the original instance</p> <p><code>Extra</code>: - only exist in <code>one region</code> - <code>ami baking</code> : creating an ami from a configured instance + application - can't be edited, launch instance, update configurations and make a new ami - can be copied between regions, including snapshots - default permissions is owner only</p> <p><code>Billing</code> - storage cost of ebs volumes the ami references</p>"},{"location":"AWS/EC2/#virtualization","title":"Virtualization","text":"<p>Process of running more than one os on a piece of hardware.</p> <p>Without virtualization: - server has hardware, os (kernel), and applications - os runs in privileged state giving it access to the hardware  - applications make a system call to the kernel to get access to the hardware</p> <ul> <li><code>Emulated Virtualization</code></li> <li>software (hypervisor) runs in privileged state</li> <li>guest hosts wrapped in containers called virtual machines</li> <li>vm emulated hardware, and os is unaware of virtualization</li> <li>binary translation :hypervisor intercepts hosts call to emulated hardware, to communicate with real hardware</li> <li><code>Para-virtualization</code></li> <li>guest os aware of virtualization</li> <li>hypercalls : guest os makes call directly to host hypervisor</li> <li><code>Hardware Assisted Virtualization</code></li> <li>host hardware (cpu) aware of virtualization</li> <li>host cpu intercepts calls from guest os and passes it to hypervisor </li> <li><code>SR-IOV</code></li> <li>hardware devices become virtualization aware</li> <li>host network card can present itself as several mini cards</li> <li>no translation required by hypervisor</li> <li>host network card directly connects to host mini card</li> <li>consistent lower latency at high amounts of consistent io</li> </ul>"},{"location":"AWS/EC2/#architecture","title":"Architecture","text":"<ul> <li><code>virtual machine</code> (os + resources)</li> <li>run on <code>EC2 Hosts</code> hardware that aws manages</li> <li><code>shared hosts</code> hardware shared across different aws customers</li> <li><code>dedicated hosts</code> hardware dedicated to customer</li> <li><code>AZ Resilient</code> as hosts run in one AZ</li> <li><code>local storage</code> instance store, temporary storage, lost if instance moves to another host</li> <li><code>storage</code> and <code>data networking</code></li> <li>when instances are provisioned in a specific subnet</li> <li>a primary <code>elastic network interface</code> is provisioned in a subnet and maps to physical hardware of EC2 host</li> <li>can have multiple network interfaces in different subnets in the same AZ</li> <li>cannot connect cross region</li> <li><code>Elastic Block Store (EBS)</code>, remote storage</li> <li>runs in one AZ, can't be accessed cross zone</li> <li><code>volumes</code>, portions of persistent storage allocated to instances in the same AZ</li> <li>instances stay on a host until</li> <li>host fails or is taken down</li> <li>if instance is stopped and then started (not restarted)</li> <li>cannot natively move between AZ</li> <li>create copies and re-provision</li> </ul> <p>Use case: - traditional os + application compute requirements - long-running compute - server style applications waiting for incoming connections - burst or steady-state load requirements - monolithic application stacks - migrating application workloads or disaster recovery</p>"},{"location":"AWS/EC2/#ec2-instance-types","title":"EC2 Instance Types","text":"<p>Different types of instances have different: - raw amount of resources - resource ratios - storage and data network bandwidth - system architecture / vendor - additional features and capabilities like gpu's</p> <ul> <li><code>General Purpose</code> (A, T, M)</li> <li>default</li> <li>diverse workloads, equal resource ratios</li> <li><code>Compute optimized</code> (C)</li> <li>media processing, HPC, scientific modelling, gaming, machine learning</li> <li>mor cpu than memory</li> <li><code>Memory optimized</code> (R, X, Z)</li> <li>processing large in memory datasets</li> <li>more memory than cpu</li> <li><code>Accelerate Computing</code> (P, G, F)</li> <li>hardware gpu, field programmable gate arrays (FPGAs)</li> <li><code>Storage Optimized</code> (I, D, H)</li> <li>large amounts of super fast local storage</li> <li>good for sequential and random io operations</li> <li>databases, data warehousing, elasicsearch, analytic workloads</li> </ul> <p>Instance type Schema: <code>&lt;instance family&gt;&lt;generation&gt;&lt;additional capabilities&gt;.&lt;size&gt;</code></p>"},{"location":"AWS/EC2/#ssh-vs-ec2-instance-connect","title":"SSH vs EC2 Instance Connect","text":"<p>SSH - make sure instance security group allows your ip address</p> <p>Instance connect - uses aws ip to connect and presents it in the browser - make sure aws ip compatible with <code>EC2_INSTANCE_CONNECT</code> in your region is allowed</p> <p>## Storage Basics</p> <ul> <li><code>Direct</code> (local) attached storage</li> <li>directly connected to the EC2 host</li> <li>called the <code>instance store</code></li> <li><code>fast</code> because it is directly attached to the hardware</li> <li>if <code>disk</code> or <code>hardware</code> fails, then the storage can be lost</li> <li>if EC2 moves between hosts the storage can be lost</li> <li><code>Network</code> attached storage</li> <li>called <code>EBS</code></li> <li>highly <code>resilient</code></li> <li> <p>separate from instance hardware so it can survive issues with EC@ host</p> </li> <li> <p><code>Ephemeral</code> Storage</p> </li> <li>temporary storage</li> <li><code>instance store</code></li> <li><code>Persistent</code> storage</li> <li>permanent</li> <li>lives on past the lifetime of the instance</li> <li> <p><code>EBS</code></p> </li> <li> <p><code>Block</code> Storage</p> </li> <li>create <code>volume</code> presented to <code>os</code> as a collection of uniquely addressable blocks</li> <li>no structure</li> <li>like empty hard drive/disk</li> <li><code>os creates a file system on top of the block and mounts it</code><ul> <li>as c drive in windows or root in linux</li> </ul> </li> <li>bootable</li> <li><code>File</code> Storage</li> <li>presented as a file share, has structure accessible over the network</li> <li>mountable not bootable</li> <li><code>Object</code> Storage</li> <li>collection of objects</li> <li>not mountable, not bootable</li> </ul> <p>Storage Performance</p> <ul> <li><code>IO (block) size</code> size of data writing to disk (MB)</li> <li><code>IOPS</code> input output operations per seconds (s)</li> <li><code>Throughput</code> amount of data that can be transferred ina  given second (MB/s)</li> </ul> <p><code>IO X IOPS = Throughput</code> - choose right block size and then maximize iops to maximize throughput</p>"},{"location":"AWS/EC2/#elastic-block-store-ebs","title":"Elastic Block Store (EBS)","text":"<ul> <li><code>block</code> storage, <code>raw</code> disk allocations</li> <li>can be written to or read using a block number</li> <li>can be encrypted using KMS</li> <li><code>AZ resilient</code> provisioned in one AZ</li> <li>in general attached to one over a storage network</li> <li>can be detached and reattached</li> <li>not lifecycle linked to one instant, persistent</li> <li>can create a <code>snapshot</code> (back up) to S3</li> <li>can create a volume from snapshot (migrate between AZ's)</li> <li>has different, physical storage types, sizes, performance profiles</li> <li>billed based GB per month</li> </ul>"},{"location":"AWS/EC2/#ebs-volume-types","title":"EBS Volume Types","text":""},{"location":"AWS/EC2/#general-purpose","title":"General Purpose","text":""},{"location":"AWS/EC2/#gp2","title":"GP2","text":"<ul> <li>default general purpose ssd based storage</li> <li>size range <code>1GB to 16TB</code></li> <li>created with io credit allocation<ul> <li>capacity of <code>5.4million</code> io credits</li> <li>fills at rate of baseline performance that is based on its size</li> <li>min of <code>100</code> credit per second per gb of volume size + 3 credits per second, per GB of volume size</li> <li>max <code>4000 iops</code></li> <li>up to <code>3000 iops burst rate</code> by depleting the bucket faster than it replenishes </li> <li>EBS larger than <code>1 TB</code>, maximum <code>16,000</code> io credit per second</li> <li>no burst, baseline always achieved, don't use credit system</li> </ul> </li> <li>good for boot volumes, low latency applications</li> </ul>"},{"location":"AWS/EC2/#gp3","title":"GP3","text":"<ul> <li>every volume regardless of size starts with 3000 iops &amp; 125 MiB/s</li> <li><code>20% cheaper than GP2</code> at base price</li> <li>up to <code>16,000 iops</code> or <code>1000 MiB/s</code></li> <li>add extra iops explicitly not based on size</li> </ul>"},{"location":"AWS/EC2/#provisioned-iops-ssd","title":"Provisioned IOPS SSD","text":"<ul> <li>io1/2/BlockExpress</li> <li>IOPS can be adjusted independently of size</li> <li>designed for super high performance, low latency, io intensive databases</li> <li>Up to <code>64,000 (256,000)</code> IOPS per volume and <code>1,000 (4,000)</code> MB/s (block express)</li> <li>volume size ranges that are compatible <code>4 GB - 16 TB io1/2 , 4GB - 64TB block express</code></li> <li>max size to performance ratio</li> <li>io1 : <code>50</code>IOPS/GB MAX</li> <li>io2 : <code>500I</code>OPS/GB MAX</li> <li>BlockExpress : <code>1000</code>IOPS/GB MAX</li> <li>pay for size and provisioned iops</li> <li>per instance performance (performance cap for an individual ec2 instance)</li> <li>io1 : <code>260,000</code> IOPS &amp; <code>7,500</code> MB/s</li> <li>io2 : <code>160,000</code> IOPS &amp; <code>4,750</code> MB/s</li> <li>io2 &amp; BlockExpress : <code>260,000</code> IOPS &amp; <code>7,500</code> MB/s</li> <li>cap also depends on the type and size of instance</li> </ul>"},{"location":"AWS/EC2/#hard-disk-drive-hdd","title":"Hard Disk Drive (HDD)","text":"<p>Both options provide less IOPS than SSD. Generally chosen for cost purposes.</p>"},{"location":"AWS/EC2/#st1","title":"st1","text":"<ul> <li>(throughput optimized)</li> <li>A low-cost HDD designed for frequently accessed, throughput-intensive workloads.</li> <li>big data, data warehouses, log processing</li> <li>sequentially accessed data</li> <li><code>125 GB</code> - <code>16 TB</code> in size</li> <li><code>Base : 40MB/s/TB , Burst 250MB/s/TB</code></li> <li><code>max 500 IOPS - 500 MB/s</code></li> </ul>"},{"location":"AWS/EC2/#sc1","title":"sc1","text":"<ul> <li>(cold HDD)</li> <li>The lowest-cost HDD design for less frequently accessed workloads.</li> <li>cold data, archives</li> <li><code>125 GB</code> - <code>16 TB</code> in size</li> <li><code>Base : 12MB/s/TB , Burst 80MB/s/TB</code></li> <li><code>max 250 IOPS - 250 MB/s</code></li> </ul>"},{"location":"AWS/EC2/#instance-store","title":"Instance Store","text":"<p>provide block storage storage devices presented to the os and used as the basis for a file system that can be used as applications</p> <ul> <li>raw volumes that can be attached to an instance </li> <li>physically connected to <code>one ec2 host</code></li> <li>instances on that host can access them</li> <li><code>highest storage performance in aws</code></li> <li>D3 instance type -&gt; 4.6 GB/s throughput</li> <li>I3 -&gt; 16 GB/s throughput</li> <li><code>more IOPS snd throughput vs EBS</code></li> <li>included in the prices, use it or lose it</li> <li>allocated a certain number of volumes based on instance type and size</li> <li><code>attach at launch time</code></li> <li><code>ephemeral (temporary) storage</code></li> <li>if an instance moves between hosts then data stored in instance store volume is lost<ul> <li>if stopped and started, change instance type, or undergoing maintenance</li> </ul> </li> </ul>"},{"location":"AWS/EC2/#ebs-vs-instance-store","title":"EBS vs Instance Store","text":"<p><code>EBS</code> - pair with good size that can give the level of performance - <code>persistent</code> storage - <code>resilient</code> storage - storage <code>isolated</code> from instance lifecycle</p> <ul> <li>Cheap -&gt; ST1 or SC1</li> <li>throughput or streaming -&gt; ST1</li> <li> <p>Boot -&gt; not ST1 or SC1</p> </li> <li> <p>GP2/3 - up to <code>16,000IOPS</code></p> </li> <li> <p>io1/2 up to <code>64,000 IOPS (256,000 IOPS block express)</code></p> </li> <li> <p>take lots of individual ebs volumes to create <code>RAID0</code> set</p> </li> <li>achieve combined performance of all individual volumes</li> <li> <p>RADI0 + EBS up to <code>260,000</code> IOPS (io1/2-BE/GP2/3)</p> </li> <li> <p>keep in mind the <code>performance each volume gives</code>, and the <code>maximum performance of the instance itself</code></p> </li> </ul> <p><code>Instance Store</code> - super high performance - cost -&gt; instance store (often included) - <code>more than 260,000 IOPS</code>, CAN GET MILLIONS IOPS</p> <p>Extra - resilience w/ App in-build replication -&gt; it depends - high performance -&gt; it depends</p>"},{"location":"AWS/EC2/#ebs-snapshots","title":"EBS Snapshots","text":"<p>Efficient way to back up EBS volumes to s3</p> <ul> <li>the first is a full copy of <code>data used</code> on the volume</li> <li>future snaps are incremental</li> <li>difference between previous snapshot and current state of volume</li> <li>if you delete incremental snapshot, newer snapshots will still function</li> <li>each snapshot is self sufficient</li> <li><code>volumes can be created from snapshots</code></li> <li>snapshots allow you to clone a volume</li> <li>new volume can be created in a new AZ, or regions since they are stored in S3</li> <li><code>makes EBS volumes that are AZ resilient, to regionally resilient</code></li> </ul> <p><code>Performance</code> - new EBS -&gt; full performance immediately - snaps <code>restore lazily</code> - fetched gradually - requested blocks are fetched immediately - <code>can force a read of all data immediately</code> from s3 to volume   - using a tool in instance os - <code>Fast Snapshot Restore (FSR)</code> - immediate restore   - up to 50 snaps per region, set on the snap and AZ   - costs extra</p> <p><code>Billing</code> - <code>gigabyte / month</code> - used not allocated data   - charged for <code>changed or new allocation in snapshot</code>   - reference data that is not changed from older snapshots</p>"},{"location":"AWS/EC2/#ebs-encryption","title":"EBS Encryption","text":"<ul> <li>Encryption uses KMS which uses CMK</li> <li>when an encrypted volume is created</li> <li>CMK saves an encrypted DEK onto the volume</li> <li>when the volume is first used</li> <li>EBS asks KMS to use CMK to decrypt the DEK which is then loaded into the memory of the EC2 host using it</li> <li>EC2 instance running on the host can now use the decrypted DEK in the host, to interact with the encrypted EBS</li> <li>cipher text stored at rest</li> <li> <p>snapshots of encrypted volumes, and volumes created from such snapshots all share the same DEK</p> </li> <li> <p>accounts can be set to encrypt by default</p> </li> <li>each volume uses 1 unique DEK</li> <li>can't change a volume to not be encrypted</li> <li>os isn't aware of the encryption </li> <li>(AES256) algo is done on host</li> <li>no performance loss</li> </ul>"},{"location":"AWS/EC2/#network-interfaces-instance-ips-and-dns","title":"Network Interfaces, Instance IPs and DNS","text":"<ul> <li>network interface</li> <li>Every ec2 has an elastic network interface (eni)<ul> <li>can have <code>secondary eni in a separate subnet but same AZ</code></li> </ul> </li> <li>has <ul> <li>a mac address</li> <li>primary ipv4 private ip -&gt; internal dns ip10-x-x-.ec2.internal</li> <li>0 or more secondary ips</li> <li>0 or 1 public ipv4 address</li> <li>dynamic, changes when ec2 is stopped and started in a different host</li> <li>dns -&gt; ec2-x-x-x.compute-1.azmazonaws.com</li> <li>inside vpc resolves to primary private ip</li> <li>outside of vpc it resolves to public ip</li> <li>1 elastic ip per private ipv4 address</li> <li>allocated to an account</li> <li>can be associated with primary or secondary interface<ul> <li><code>if attached to primary, it replaces the public ipv4</code></li> </ul> </li> <li>0 or more ipv6 addresses</li> <li>security groups</li> <li>source/destination check</li> </ul> </li> <li>can detach secondary interfaces and move them to other ec2 instances</li> </ul> <p>Extra: - can move secondary eni mac licensing - multi-homed (subnets)   - use multiple interfaces, with different ips and security groups - os never sees public ipv4, only sees the private   - public ipv4 is handled by internet gateway using nat   - ipv6 public by default - ipv4 public ips are dynamic, stop/start or (change of host) changes ip - public dns resolves to private in vpc   - never leaves the vpc, for instance to instance communication</p>"},{"location":"AWS/EC2/#purchase-options","title":"Purchase Options","text":"<ul> <li>On-Demand (default)</li> <li>isolated, but multiple customer instances run on shared hardware</li> <li>instances of different sizes run on the same ec2 hosts, consuming a defined allocation of resources</li> <li>per-second billing while an instance is running<ul> <li>associated resources such as storage consume capacity, so billed regardless of instance state</li> </ul> </li> <li>no interruptions</li> <li>no capacity reservation</li> <li>predictable pricing</li> <li>no upfront cost</li> <li>no discount</li> <li> <p>choose for short-term or unknown workloads or applications which can't be interrupted</p> </li> <li> <p>Spot</p> </li> <li>aws selling unused ex2 host capacity for up to 90% discount</li> <li>spot price is based on the spare capacity at a given time</li> <li>if spot price goes above your maximum price, then your instances are terminated</li> <li>never use for workloads which can't tolerate interruptions</li> <li> <p>for workloads that are non time critical, can be rerun, cost sensitive, stateless, has bursty capacity need</p> </li> <li> <p>Dedicated Hosts</p> </li> <li>pay for host that contained the instances, no instance charges</li> <li>might have software licensing based on <code>sockets</code> or <code>cores</code></li> <li>host affinity --&gt; if instance is stop and started it remains on the same host</li> <li>only your instances run on the dedicated host</li> <li>specific family and size of instance<ul> <li>nitro can use different size of instances at the same time</li> </ul> </li> <li>on-demand &amp; reserved options for pricing</li> <li>ami limits : rhel, suse linux, and windows amis aren't supported</li> <li>amazon rds instances are not supported</li> <li>placement groups are not supported</li> <li> <p>hosts can be shared with other ORG accounts using Resource Access Manager (RAM)</p> <ul> <li>host own sees all instances running on it, but can only edit ones it owns</li> <li>instances owners, that ar not the host owner, can only see their instances</li> </ul> </li> <li> <p>Dedicated Instances</p> </li> <li>no other customers use the same hardware</li> <li>don't own or share the host</li> <li>hourly fee per region regardless of how many dedicated instances are being used</li> <li>for when you have requirements to not share hardware</li> </ul> <p>On-demand, reserved, and spot</p> <ul> <li>Reserved</li> <li>for long term consistent usage</li> <li>unused reservation are still billed</li> <li>for a particular type of instance and locked to an AZ or Region<ul> <li>reserve based on AZ also reserves capacity</li> </ul> </li> <li>can have a partial effect<ul> <li>ex) reservation for small T3 instances might partially apply to large T3 instances</li> </ul> </li> <li>plans<ul> <li>1 year or 3 year</li> <li>no upfront -&gt; reduce per second fee</li> <li>all upfront -&gt; no per second fee, greatest discount</li> <li>partial upfront</li> </ul> </li> <li>scheduled reserved instances<ul> <li>ideal for long term usage which doesn't run constantly</li> <li>specify frequency, duration, and time</li> <li>ex) batch processing daily for 5 hours at 23:00</li> <li>slightly cheaper than on demand</li> <li>doesn't support all instance types or regions</li> <li>1200 hours per year minimum</li> <li>1 year term minimum</li> </ul> </li> <li> <p>capacity reservation</p> <ul> <li>when major failure results in lack of available capacity in a region or AZ</li> <li>there is a priority list of which purchase options get available instances first</li> <li>reserved purchases -&gt; on-demand -&gt; spot</li> <li>regional reservation</li> <li>billing discount for valid instances launched in any AZ in that region</li> <li>don't reserve capacity, same priority as on-demand</li> <li>1 or 3 year term</li> <li>zonal reservation</li> <li>same discount as regional reservation, but only apply to one az</li> <li>capacity reservation in the az</li> <li>1 or 3 year term</li> <li>on-demand capacity reservation</li> <li>book to enure you always have access to capacity in an az</li> <li>at full on demand price</li> <li>no term limit</li> <li>pay regardless if you consume it</li> </ul> </li> <li> <p>Savings Plans</p> </li> <li>hourly commitment for a 1 or 3 year term</li> <li>general plan<ul> <li>reservation of general compute $ amount, save up to 66%</li> <li>ec2, fargate, lambda</li> </ul> </li> <li>ec2 savings plan<ul> <li>up to 72% savings</li> </ul> </li> <li>products have an on-demand rate and a savings plane rate</li> <li>get savings plan rate, up to to the amount you commit to</li> </ul>"},{"location":"AWS/EC2/#instance-status-checks-auto-recovery","title":"Instance Status Checks &amp; Auto Recovery","text":"<ul> <li>Instance Status Checks</li> <li>each ec2 instance gets two 2 tests<ul> <li>system status</li> <li>loss of system power, network connectivity,, host software/hardware issue </li> <li>instance status</li> <li>corrupted file system, incorrect instance networking, os kernel issue</li> </ul> </li> <li> <p>resolution</p> <ul> <li>can manually stop and start an instance, restart, or terminate and recreate</li> </ul> </li> <li> <p>Auto recovery</p> </li> <li>moves instance to new host with same configs<ul> <li>same instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</li> </ul> </li> <li>can create a cloudwatch alarm triggered when an instance fails a status check<ul> <li>can send message</li> <li>can take action</li> <li>recover (auto-recover), reboot, stop, or terminate</li> </ul> </li> <li> <p>works only with instances with ebs volumes attached</p> </li> <li> <p>Termination protection</p> </li> <li>can be enabled in instance settings, to protect from accidental termination</li> <li>can separate permissions to enable and disable termination protection</li> </ul>"},{"location":"AWS/EC2/#horizontal-and-vertical-scaling","title":"Horizontal and Vertical Scaling","text":"<p>Scaling is what happens when systems have to grow or shrink depending on changes to the load the experience.</p> <ul> <li>Vertical</li> <li><code>resizing the EC2 instance</code></li> <li><code>requires a reboot</code> which can potentially cause disruption</li> <li>generally scale during pre-agreed times (outage windows)<ul> <li>limits how quickly you can respond</li> </ul> </li> <li>larger instances cost more, cost increase scales faster than size increase</li> <li><code>upper cap</code> on performance (instance size)</li> <li>no application modification required</li> <li> <p>works for all applications even monoliths</p> </li> <li> <p>Horizontal</p> </li> <li><code>changes the number of instances</code></li> <li>require a <code>load balancer</code><ul> <li>to distribute traffic across multiple instances</li> </ul> </li> <li>can be shifting across instances</li> <li>requires application support or <code>off-host sessions</code></li> <li><code>no disruption when scaling</code></li> <li>no real limits, keep adding instances</li> <li>often <code>less expensive</code>, no large instance premium</li> <li><code>more granular</code>, by adding smaller instances</li> </ul>"},{"location":"AWS/EC2/#instance-metadata","title":"Instance Metadata","text":"<ul> <li>Service EC2 provides to instances</li> <li>accessible inside all instance</li> <li>at ip <code>http://169.254.169.254/latest/meta-data/</code></li> <li>data about the instance that can be used to configure or manage the instance</li> <li>info on the environment that the instance is in</li> <li>ex) networking, authentication info, user data</li> <li>not authenticated or encrypted</li> </ul>"},{"location":"AWS/EC2/#bootstrapping","title":"Bootstrapping","text":"<p>Process where scripts are run when an instance is first launched. To create an instance with a certain configuration. - enabled using user data</p> <p><code>User data</code> - accessed via the meta-data ip - <code>http://169.254.169.254/latest/user-data/</code> - anything in userdata is executed by the instance Os - only executed on launch - ec2 doesn't interpret, the os needs to understand the user data - it's possible to have a bad config and instance to still run - not secure, don't use for passwords or long term credentials - 16KB size limit   - for larger, make script download larger file - can be modified when instance stopped</p> <p><code>Boot-TimeToService</code> - Basic ami -&gt; launch instance -&gt; manual post launch configuration - Basic ami -&gt; launch instance -&gt; bootstrapping - Baked ami -&gt; launch instance   - less flexible   - do for time intensive processes</p> <p>Ideally combine bootstrapping and baked ami</p> <p><code>Cloudformation Init (CFN-INIT)</code> - a way to pass complex bootstrap configuration to an instance - simple configuration management system   - install packages with version awareness   -  manipulate groups and users   -  download sources    -  create files    -  run commands and check state    -  run services - provided with directives via <code>metadata</code> and <code>AS::CloudFormation::init</code> on a CN resource - `works with stack update</p> <p><code>CloudFormation CreationPolicy and Signals</code> - inform cloudformation if it has been configured correctly - creation policies create a <code>'WAIT STATE'</code> on resources    - not allowing the resource to move to <code>CREATE_COMPLETE</code> until signalled using the cfn-signal tool - reported to the stack, and updates the instance state (OK or not)</p>"},{"location":"AWS/EC2/#instance-roles-and-profiles","title":"Instance Roles and Profiles","text":"<ul> <li>Roles that an instance can assume.</li> <li>Anything running in the instance has the permissions granted by the role</li> <li><code>instance profile</code> : wrapper around an iam role</li> <li>allows permissions to get inside the instance</li> <li>attached to the instance</li> <li>temporary credentials delivered via the instance meta-data</li> <li><code>iam/security-credentials/role-name</code></li> <li>creating the role in the console also creates the profile, must create both separate with cloudformation</li> </ul>"},{"location":"AWS/EC2/#ssm-parameter-store","title":"SSM Parameter` Store","text":"<ul> <li>storage for <code>configuration</code> and <code>secrets</code></li> <li>license codes, database strings, full configs and passwords</li> <li>types: <code>String, StringList, SecureString</code></li> <li>supports hierarchy and versioning</li> <li>plaintext and <code>ciphertext</code> (with KMS)</li> <li>public parameters</li> <li>integrated with IAM to authorize access</li> <li>changes can create events</li> </ul>"},{"location":"AWS/EC2/#system-and-application-logging-on-ec2","title":"System and Application Logging on EC2","text":"<ul> <li>Monitoring inside the instance</li> <li>cloudwatch is for metrics and cloudwatch logs is for logging</li> <li>neither natively capture data inside an instance</li> <li><code>cloudwatch agent</code> is required with some <code>configuration</code> and <code>permissions</code></li> <li>software that runs inside the instance and captures os visible data and sends to cloudwatch</li> <li><code>configuration</code> tells the agent what to do (what data to capture)</li> <li><code>permissions</code> can be given by attaching a role to the instance</li> <li>one log group for each log to capture. and one log stream in each log group for each instance sending data</li> </ul>"},{"location":"AWS/EC2/#ec2-placement-groups","title":"Ec2 Placement Groups","text":"<p>Allows you to control where ec2 instances are placed.</p> <ul> <li><code>Cluster</code> Placement Groups (<code>PERFORMANCE</code>)</li> <li>pack instances close together</li> <li>best practices : to let aws find a location with needed capacity<ul> <li>launch all instances at the same time</li> <li>use same tyupe of instances</li> </ul> </li> <li><code>single AZ</code>, same rack, sometimes same host, all members have direct connections to each other<ul> <li>10Gbps stream (vs 5 normally)</li> <li>lowest latency and max packets per second (PPS) possible in aws</li> <li>should use enhanced networking for best performance</li> </ul> </li> <li>can span VPC peers, but performance will be impacted</li> <li>requires a supported instance type</li> <li>use cases : performance, fast speeds, low latency</li> <li><code>Spread</code> Placement Groups (<code>Resilience</code>)</li> <li>keep instances separated, different hardware (racks)<ul> <li>each rack has its own network and power source</li> </ul> </li> <li>can span multiple AZ's, 7 instances per AZ limit</li> <li>not supported for dedicated instances or hosts</li> <li>use case :small number of critical instances that need to be kept separated from each other</li> <li><code>Partition</code> Placement Groups (<code>Topology Awareness</code>)</li> <li>groups of instances spread apart</li> <li>can span multiple AZ's</li> <li>divided into <code>paritions</code>, max 7 per AZ</li> <li>each partition has its own racks no sharing</li> <li>can launch as many instances as needed, spread across the partitions</li> <li>have awareness of which partition the instance is in</li> <li>use case : huge scale parallel systems</li> </ul>"},{"location":"AWS/EC2/#enhanced-networking-ebs-optimized-instances","title":"Enhanced Networking &amp; EBS Optimized Instances","text":"<p>Enhanced Networking - SR-IOV : Host has Network Interface Cards (NIC) that are virtualization aware   - higher I/O &amp; Lower Host CPU Usage   - More Bandwith (fasster network speed)   - higher packets per second (PPS) and consistent lower latency</p> <p>EBS Optimized Instances - dedicated capacity for EBS (doesn't affect data networking) - most instances support, and have it enabled by default - required for high performance EBS volume types</p>"},{"location":"AWS/ECS/","title":"Elastic Container Service (ECS)","text":""},{"location":"AWS/ECS/#introduction-to-containers","title":"Introduction to Containers","text":"<p>Virtual machines virtualize operating systems. - os consume a lot of the resources (bloat)</p> <p>Container engine runs as a process within host os. - containers run on top of container engine - <code>doesn't create full new os</code> for each instance - much <code>lighter</code> than virtual machines</p> <p><code>Docker Image</code> - created using a <code>docker file</code> - created from base image or scratch - images contain read only layers, <code>(file system layers)</code> - each set of changes adds another layer to the docker image   - changes are layered onto the image using a differential architecture - used to create a <code>docker container</code></p> <p><code>Docker Container</code> - running copy of a docker image - has an addition <code>read write file system layer</code>   - allows containers to run - multiple containers can be created from the same docker image   - the difference between containers created from the same docker image is only the final read write layer   - similar <code>layers are reused</code> across containers - portable, self contained, always run as expected - ports are <code>exposed</code> to the host and beyond - applications stacks can be multi-container</p> <p><code>Container Registry (dockerhub)</code> - registry of container images - where images are uploaded and downloaded from   - download centos base image from here for example</p>"},{"location":"AWS/ECS/#ecs-concepts","title":"ECS Concepts","text":"<p>Allows you o use containers running on infrastructure managed by aws</p> <ul> <li><code>ec2 mode</code>:  hosted on ec2 instances</li> <li><code>fargate</code> : serverless way of running docker containers</li> <li><code>container definition</code> : tells ecs which image to use and which ports are exposed</li> <li><code>task definition</code>: represents an <code>application</code> as a whole</li> <li>can be made of <code>one or more containers</code></li> <li>store the: <ul> <li><code>resources</code> used by task (cpu and memory)</li> <li><code>networking</code> mode that task use</li> <li><code>compatibility</code> (ex2 or fargate)</li> <li><code>task role</code> : iam role that a task can assume to gain temporary credentials to interact with aws</li> </ul> </li> <li><code>ecs service</code></li> <li>service definition : define how we want a task to <code>scale</code><ul> <li>used to create HA along with a load balancer</li> <li>how many copies, restarts</li> </ul> </li> <li><code>ecs cluster</code> : where containers run from</li> <li>upload task or service here</li> </ul>"},{"location":"AWS/ECS/#cluster-modes","title":"Cluster Modes","text":"<p>Main difference is Admin overhead involved and costs.</p> <ul> <li>ecs management components : handle scheduling, orchestration, cluster management, placement engine (where to run containers)</li> </ul> <p><code>EC2 Mode</code> - ecs cluster created within a vpc   - benefits from <code>multiple AZ</code> available in the vpc - instances run the containers - specify an initial size which controls the number of container instances   - handled by an autoscaling group (<code>ASG</code>) - tasks and services   - get images from registry   - uses task and service definition to deploy container images onto container hosts as containers - <code>user manages the cluster</code> - <code>pay for instance even if they aren't running containers</code></p> <p><code>Fargate Mode</code> - still define task an service definitions, cluster, and vpc - runs  on <code>shared AWS infrastructure</code> - each task or service gets injected into vpc   - given an elastic network interface ENI (ip address within the vpc) - only pay for the resources that running containers consume</p> <p>Scenarios and Choices <code>EC2</code> - large workload - price conscious <code>ECS(EC2 mode)</code> - if you use containers <code>Fargate</code> - large workload - overhead conscious - small or burst workloads - batch or periodic workloads</p>"},{"location":"AWS/ECS/#kubernetes","title":"Kubernetes","text":"<ul> <li>open-source system for automating deployment, scaling, and management of containerized applications.</li> <li><code>cluster</code></li> <li>deployment of kubernetes, management, orchestration</li> <li><code>cluster control plane</code></li> <li>manages the cluster, scheduling, applications, scaling and deploying</li> <li><code>kube-apiserver</code><ul> <li>front end for kubernetes control plane</li> <li>what nodes interact with</li> <li>can horizontally scale for HA</li> </ul> </li> <li><code>etcd</code>q<ul> <li>highly available key value storage within cluster</li> </ul> </li> <li><code>kube-scheduler</code><ul> <li>assigns node to pods based on needs and constraints</li> </ul> </li> <li><code>cloud-controller-manager</code><ul> <li>provide cloud specific control logic</li> </ul> </li> <li><code>kube-controller-manager</code><ul> <li><code>node controller</code> : monitoring and responding to node outages</li> <li><code>job controller</code> : run pods to execute jobs</li> <li><code>endpoint controller</code> : populates endpoints in cluster</li> <li><code>service account &amp; token</code> : accounts/api token creation</li> </ul> </li> <li><code>kube-proxy</code><ul> <li>runs on each node</li> <li>coordinates networking</li> </ul> </li> <li><code>cluster nodes</code></li> <li>vm or physical server on which pods are placed in</li> <li><code>containerd</code> or <code>Docker</code> software for handling container operations</li> <li><code>kubelet</code> agent to interact with cluster control plane</li> <li><code>pods</code></li> <li>1+ containers, often one container per pod</li> <li>smallest units of computing</li> <li>pods are non-permanent</li> <li><code>service</code></li> <li>abstraction, running on one or more pod</li> <li><code>ingress</code></li> <li>exposes way into a service</li> <li><code>ingress controller</code><ul> <li>used to provide ingress</li> </ul> </li> <li><code>persistent storage</code></li> <li>lifecycle lives beyond any 1 pod</li> </ul>"},{"location":"AWS/ECS/#elastic-kubernetes-service-eks","title":"Elastic Kubernetes Service (EKS)","text":"<ul> <li>aws managed kubernetes</li> <li>control plane scales and runs on multiple AZs</li> <li>integrates with AWS services</li> <li>ECR, RLB, IAM, VPC, EBS, EFS</li> <li>EKS cluster</li> <li>eks control plan &amp; eks nodes</li> <li>etcd distributed across multiple AZs</li> <li>nodes</li> <li>self managed on ec2</li> <li>managed node groups : eks handles provisioning and lifecycle management</li> <li>fargate</li> </ul>"},{"location":"AWS/EFS/","title":"Elastic File System (EFS)","text":""},{"location":"AWS/EFS/#efs-architecture","title":"EFS Architecture","text":"<ul> <li>AWS managed implementation of NFS which allows for the creation of shared <code>filesystems</code></li> <li>shared between many EC2 instances</li> <li>private service</li> <li>made available via <code>mount targets</code> inside a VPC</li> <li>mount targets : exist in subnets with an IP from it</li> <li>should put mount target in every AZ a VPC uses for high availability</li> <li>EC2 instances connect to the mount target to access EFS</li> <li>can be accessed from on-premises : VPN or DX</li> <li><code>Linux Only</code></li> <li><code>Performance modes</code></li> <li><code>General Purpose</code> : default for 99.9% of use cases</li> <li><code>Max I/O</code> : for anything that is highly parallel</li> <li><code>Throughput Modes</code></li> <li><code>Bursting</code><ul> <li>throughput scales with size of the file system</li> </ul> </li> <li><code>Provisioned</code><ul> <li>specify throughput requirements separate from the amount of data being stored</li> </ul> </li> <li><code>Storage classes</code></li> <li><code>Standard</code><ul> <li>default, frequently accessed data</li> </ul> </li> <li><code>Infrequent Access</code><ul> <li>to save money for anything not used a lot</li> </ul> </li> <li><code>Lifecycle policies</code> can be used with classes</li> </ul>"},{"location":"AWS/GlobalContentDelivery/","title":"Cloudfront","text":""},{"location":"AWS/GlobalContentDelivery/#architecture","title":"Architecture","text":"<ul> <li><code>content delivery network.</code> : Improve delivery of content from original location of data to viewers</li> <li>done by caching and an efficient global network</li> <li>integrates with ACM for HTTPS</li> <li>download caching only</li> <li><code>Origin</code> : Source location of your location</li> <li>S3 ORigin or Custom Origin</li> <li><code>Distribution</code> : configuration unit of cloudfront</li> <li>unique domain name is created for each distribution or can use your own domain name</li> <li>after configuration is complete : deploy distributution to cloudfront network<ul> <li>configuration sent to all chosen edge locations</li> </ul> </li> <li><code>Behavior</code><ul> <li>use origins as content source</li> <li>linked to distributions, which can have on or more behavior</li> <li>configured with path patterns</li> </ul> </li> <li><code>Edge Location</code> : local cache of your data</li> <li>more cache hits equals lower origin load</li> <li><code>Regional Edge Cache</code> : larger version of edge location</li> <li>for things accessed less frequently</li> <li> <p>only supported by custom origins</p> </li> <li> <p><code>Flow</code></p> </li> <li>customer searches, and goes to edge location</li> <li>go to regional edge cache if supported and edge location doesn't have content</li> <li>go to origin</li> </ul>"},{"location":"AWS/GlobalContentDelivery/#origins","title":"Origins","text":"<ul> <li><code>Origins</code></li> <li>if edge location gets a request for an object that isn't cached, it is retrieved from the origin</li> <li><code>origin groups</code> : collection of 2 or more origins that can be used by a behavior<ul> <li>provide resilience</li> </ul> </li> <li><code>S3 Buckets</code><ul> <li>origin domain name : point directly at s3 bucket</li> <li>origin path : use a particular path of bucket</li> <li><code>origin access identit</code>y : give cloudfront a virtual identity</li> <li>view protocol and origin protocol are the same</li> <li>origin custom headers</li> <li>if static website hosting is configured, it is view as a custom origin</li> </ul> </li> <li><code>media package channel endpoints</code></li> <li><code>media store container points</code></li> <li><code>custom origins</code><ul> <li>minimum origin SSL protocol</li> <li><code>origin protocol policy</code> : HTTP only, HTTPS only, match viewer</li> <li><code>HTTP/S port</code> : where origin listens on, used for connections between edge locations and origin</li> <li>custom headers : configure origin to accept connections only from cloudfront</li> </ul> </li> </ul>"},{"location":"AWS/GlobalContentDelivery/#distributions-behaviors","title":"Distributions &amp; Behaviors","text":"<ul> <li><code>Distribution settings:</code></li> <li><code>can have multiple behaviors</code> which have a precedence</li> <li>price class (which edge locations to use)</li> <li>WAF</li> <li>alternate domain name</li> <li>SSL certificate<ul> <li>default comes with default cname</li> <li>custom required for custom cname</li> <li>SNI or non SNI</li> <li>security policy (TLS version)</li> <li>supported HTTP versions</li> <li>logging</li> <li>enabled, disabled</li> </ul> </li> <li><code>Behavior settings :</code></li> <li>where caching controls and restricting viewer access are configured</li> <li>origin or origin group</li> <li><code>viewer protocol policy</code> (policy used between viewer and edge location)</li> <li>allowed HTTP methods</li> <li>field level encryption<ul> <li>encrypt from point data enters edge location</li> </ul> </li> <li> <p><code>cache directives</code></p> <ul> <li>methods that are cached</li> <li>cache based on header</li> <li>TTL</li> <li>restrict viewer access</li> <li>must specify trusted signers</li> <li>compress objects automatically</li> <li>lambda edge functions</li> </ul> </li> <li> <p><code>Time To Live (TTL)</code></p> </li> <li>default <code>24hrs</code> (behavior) (validity period)</li> <li>object in cache <code>expires</code> after TTL, but is <code>not deleted</code><ul> <li>next request will make cache forward request to origin to check object version</li> <li>if object is not modified (304) : object in cache is marked as current and returned to user</li> <li>if there is a new version (200) : origin returns new object</li> </ul> </li> <li><code>Object Specific TTL</code><ul> <li>Custom Origin or S3 (via object metadata)</li> <li>Origin Header : Cache-Control <code>max-age</code> (<code>seconds</code>)</li> <li>Origin Header : Cache-Control <code>s-maxage</code> (<code>seconds</code>)</li> <li>Origin Header : <code>Expires</code> (<code>Date and Time</code>)</li> <li>data and time an object should be viewed as expired</li> </ul> </li> <li> <p><code>minimum</code> and <code>maximum</code> TTL</p> <ul> <li><code>limit the object specific TTL</code> values</li> </ul> </li> <li> <p><code>Invalidation</code></p> </li> <li>performed on a distribution, applies to all edge locations : takes time</li> <li><code>immediately expires all objects that match a path pattern</code>, regardless of their TTL</li> <li> <p>use <code>versioned file names</code> : to avoid needing invalidation </p> <ul> <li>avoid cost of invalidation</li> <li>logging is more effective</li> <li>still keep all version of objects to switch between</li> </ul> </li> <li> <p><code>SSL</code></p> </li> <li>cloudfront distributions are created with default domain names (cname) that comes with SSL<ul> <li>SSL cert covers all distributions</li> </ul> </li> <li>alternate domain names can be used to access distributions</li> <li>verify ownership (optionally HTTPS) using SSL certificate</li> <li>generate certificate in <code>us-east-1</code> with ACM</li> <li>options : HTTP or HTTPS, HTTP =&gt; HTTPS, HTTPS only</li> <li> <p>Two SSL connections</p> <ul> <li>viewer - cloudfront</li> <li>cloudfront - origin</li> </ul> </li> <li> <p><code>SNI (Server Name Indication)</code></p> </li> <li>TLS extension allowing client to tell server which domain name to access</li> <li><code>resulting in many SSL certs/Hosts using a shared IP</code></li> <li>HTTPS encryption happens at TCP layer, lower than HTTP</li> <li>no extra cost</li> <li>not supported by old browsers<ul> <li>will require dedicated IPs at a cost</li> </ul> </li> </ul>"},{"location":"AWS/GlobalContentDelivery/#access","title":"Access","text":"<ul> <li><code>OAI (Origin Access Identities)</code></li> <li>for S3 Origins</li> <li>associated with cloudfront distribution</li> <li>can be used in s3 bucket policies<ul> <li>DENY all but one or more OAIs</li> </ul> </li> <li><code>Custom Headers</code></li> <li>for custom origins<ul> <li>make require custom header</li> </ul> </li> <li>configure cloudfront to send custom headers with original request</li> <li><code>IP Based FW Blocks</code></li> <li>For Custom Origins</li> <li>traditional firewall around origin, configured to accept only from edge location IPs</li> </ul>"},{"location":"AWS/GlobalContentDelivery/#lambdaedge","title":"Lambda@Edge","text":"<ul> <li><code>Lambda@Edge</code></li> <li>lightweight lambda at edge location</li> <li>adjust data between teh viewer and origin</li> <li>node.js or python</li> <li>run in aws public space</li> <li>layers are not supported</li> <li><code>different limits</code><ul> <li>viewer : 128MB, 5sec</li> <li>origin : 30sec</li> </ul> </li> <li>use cases :<ul> <li>A/B testing : viewer request</li> <li>change url viewer requested</li> <li>migrating between s3 origins : origin request</li> <li>gradually transfer traffic to different s3 origin</li> <li>different objects based on device : origin request</li> <li>content by country : origin request</li> </ul> </li> </ul>"},{"location":"AWS/GlobalContentDelivery/#aws-certificate-manager-acm","title":"AWS Certificate Manager (ACM)","text":"<ul> <li><code>regional</code></li> <li><code>us-east-1</code> for cloudfront use-case</li> <li>allows the creation, management and renewal of certificates</li> <li>allows deployment of certificates onto <code>ONLY</code> supported AWS services such as <code>CloudFront</code>, <code>API Gateway</code> and <code>ALB</code></li> <li> <p><code>not EC2</code> or on-premise</p> </li> <li> <p>HTTPS uses SSL/TLS to encrypt data in-transit</p> </li> <li>servers prove identity using digital certificates signed by trusted authorities</li> <li>deploying to cloudfront distribution</li> <li>all edge locations also get the certificate</li> </ul>"},{"location":"AWS/GlobalContentDelivery/#global-accelerator","title":"Global Accelerator","text":"<ul> <li>2 anycast IP addresses</li> <li>1.2.3.4 &amp; 4.3.2.1</li> <li>allow a single IP to be in multiple locations, routing moves traffic to closest edge location</li> <li>traffic initial uses public internet and enters a global accelerator edge location</li> <li> <p>edge locations transit data over aws global network</p> </li> <li> <p>network product</p> </li> <li>non http/s</li> <li>TCP, UDP</li> </ul>"},{"location":"AWS/HAandScaling/","title":"High Availability and Scaling","text":""},{"location":"AWS/HAandScaling/#regional-and-global-aws-architecture","title":"Regional and Global AWS Architecture","text":"<ul> <li><code>R53</code> (DNS Service)</li> <li><code>Global Service Location and Discovery</code><ul> <li>how does your machine discover where to point at</li> </ul> </li> <li><code>Global Health Checks and Failover</code><ul> <li>detecting if infrastructure is healthy or not in a one location and moving customers to another Country as required</li> </ul> </li> <li><code>CloudFront</code></li> <li><code>Content Delivery</code> (CDN) and optimization<ul> <li>how does content get to users globally : from distributed or central location</li> <li>cache content globally as close to end user as possible to improve performance</li> </ul> </li> </ul> <ul> <li><code>web tier</code></li> <li>provide entry point to customer</li> <li>abstracts internals away from customer</li> <li><code>compute tier</code></li> <li>provide functionality for the customer</li> <li><code>storage tier</code></li> <li>provide storage for compute infrastructure</li> <li><code>cache tier</code></li> <li>faster data access by caching data</li> <li>reduce db reads, to improve performance and reduce costs</li> <li><code>db tier</code></li> <li>data storage</li> <li><code>app service</code></li> <li> <p>provide functionality to applications like queues, data streaming etc.</p> </li> <li> <p><code>Regional Scaling and Resilience</code></p> </li> </ul>"},{"location":"AWS/HAandScaling/#elastic-load-balancer-evolution","title":"Elastic Load Balancer Evolution","text":"<ul> <li>3 types of load balancers (ELB) available</li> <li>split between between v1 (avoid) and v2(prefer)</li> <li>CLB (v1), lacking features, more expensive</li> <li>Application Load Balancer (ALB)</li> <li>HTTP/S/WebSocket</li> <li>Network Load Balancer (NLB)</li> <li>TCP/TLS/UDP</li> <li>email, ssh</li> <li>v2 : faster, cheaper, support target groups and rules</li> </ul>"},{"location":"AWS/HAandScaling/#elastic-load-balancer-architecture","title":"Elastic Load Balancer Architecture","text":"<ul> <li>accept connections from customers and distribute them across any registered backend compute</li> <li><code>abstracts user away from physical infrastructure</code></li> <li>means the amount of infrastructure can change without affecting customers</li> <li>infrastructure can fail and be repaired, hidden from customers</li> <li>pick if using ipv4 only or ipv4 and ipv6</li> <li>pick which AZs the load balancer will use</li> <li><code>2 or more AZs</code></li> <li><code>one subnet</code> per AZ</li> <li>one load balancer is made up of <code>many nodes</code></li> <li>product places one or more load balancer nodes into the subnets</li> <li>load balancer created with a <code>DNS record</code> </li> <li>A record</li> <li><code>points to all of the ELB nodes</code> created with the product</li> <li>incoming requests are <code>distributed equally across all the nodes</code></li> <li>nodes scale within the AZ<ul> <li><code>HA</code> : if one fails it is replaced</li> <li>if load increases, then addition nodes are provisioned</li> </ul> </li> <li><code>Internet facing</code>: have private and public IPs</li> <li><code>Internal</code> : have only privates IPs</li> <li>generally used to separate different tiers of applications<ul> <li>like web,app,db etc.</li> </ul> </li> <li><code>allows tiers to scale independently of each other</code></li> <li>Nodes are configured with <code>listeners</code></li> <li>controls what the load balancer is listening to</li> <li>accept traffic on a part and protocol</li> <li>communicate with targets on a port and protocol</li> <li>nodes can make connections with instances that are registered with the load balancer</li> <li><code>can connect to both public and private</code> instances</li> <li>need <code>8+ freeIPs</code> per subnet and a <code>/27</code> or larger subnet to allow for scale</li> <li><code>Cross-Zone Load Balancing</code></li> <li>allows node to distribute connections equally across <code>all registered instances across all AZs</code></li> <li>allows for more even load balancing : when different AZs have unequal compute infrastructure</li> </ul>"},{"location":"AWS/HAandScaling/#application-vs-network-load-balancers-alb-vs-nlb","title":"Application vs Network Load Balancers (ALB vs NLB)","text":"<p><code>Load Balancer Consolidation</code> - <code>Classic Load Balancer</code>   - clb has an attached ssl certificate and autoscaling group   - clb distributes incoming connections to instances   - <code>doesn't scale</code>; every unique https application name requires an individual clb because SNI isn't supported</p> <ul> <li><code>V2 Load Balancers</code></li> <li>can use one load balancer for multiple applications<ul> <li>allows consolidation</li> </ul> </li> <li><code>listener based rules</code><ul> <li>can hold multiple ssl certificates</li> </ul> </li> <li><code>host based rules</code><ul> <li>using SNI</li> <li>direct incoming connections at multiple target groups</li> </ul> </li> <li> <p><code>target groups</code> forward connections to multiple scaling autoscaling groups</p> </li> <li> <p><code>Application Load Balancer (ALB)</code></p> </li> <li>Layer 7 load balancer<ul> <li><code>listens on HTTP or HTTPS only</code></li> <li>must have <code>SSL certs if HTTPS is used</code></li> </ul> </li> <li>connection are terminated on the ALB<ul> <li><code>no unbroken SSL connection</code></li> <li>a new connection is made to the application</li> </ul> </li> <li><code>slower than NLB</code> : since more levels of network stack to process</li> <li><code>application aware health checks</code></li> <li> <p><code>rules</code> </p> <ul> <li>direct connections which arrive at a listener</li> <li>processed in priority order</li> <li><code>default rule = catchall</code></li> <li><code>rule conditions</code></li> <li>content type, cookies, custom headers, user location and app behavior</li> <li>host-header, http-header, http-request-method</li> <li>path pattern, query string</li> <li>source ip</li> <li><code>actions</code></li> <li>forward, redirect, fixed-response, authenticate-oidc, authenticate-cognito</li> </ul> </li> <li> <p><code>Network Load Balancer (NLB)</code></p> </li> <li><code>layer 4 : TCP, TLS, UDP, TCP_UDP</code><ul> <li><code>no  HTTP or HTTPS</code></li> </ul> </li> <li><code>really fast</code> (millions of rps, 25% of ALB latency)</li> <li><code>health checks are not application aware</code></li> <li>can have <code>static IP's</code> : useful for <code>whitelisting</code></li> <li>can forward TCP to instances<ul> <li><code>unbroken end to end encryption</code></li> </ul> </li> <li>used with private link to provide services to other VPCs</li> </ul> <p><code>Deciding on One</code> - NLB   - unbroken encryption    - static IP for whitelisting   - fastest performance   - protocols not HTTP or HTTPS   - private link - ALB   - anything else</p>"},{"location":"AWS/HAandScaling/#launch-configuration-and-templates","title":"Launch Configuration and Templates","text":"<ul> <li><code>define the configuration of an ec2 instance in advance</code></li> <li>AMI, Instance Type, Storage, Key Pair</li> <li>Network and Security Groups</li> <li>Userdata and IAM role</li> <li>not editable</li> <li>LT has versions</li> <li><code>LT has newer features</code></li> <li>recommended over LC</li> <li>Placement Groups</li> <li>Capacity Reservations</li> <li>Elastic Graphics</li> <li>T2/T3 Unlimited</li> <li>LC are used for autoscaling groups</li> <li>LT </li> <li>used for  autoscaling groups</li> <li>used to <code>launch ec2 instances directly</code></li> </ul>"},{"location":"AWS/HAandScaling/#auto-scaling-groups","title":"Auto Scaling Groups","text":"<ul> <li>configure ec2 to <code>scale automatically depending on demand</code></li> <li><code>When and Where</code> to launch to</li> <li><code>self healing</code></li> <li>uses ec2 health checks</li> <li>terminates bad ones, and provisions a new one in its place</li> <li>uses launch template/configuration to know what to launch</li> <li><code>minium, desired, max</code></li> <li>x:y:z</li> <li>keep running instances at the desired capacity byb provisioning or terminating instances</li> <li><code>scaling policies</code></li> <li>update desired based on metrics ie) cpu load</li> <li><code>manual scaling</code> : manually adjust the desired capacity</li> <li><code>scheduled scaling</code> : time based adjustment<ul> <li>for periods of low or high usage</li> </ul> </li> <li><code>dynamic scaling</code><ul> <li><code>simple</code></li> <li>rule based on metric to provision or remove instances</li> <li><code>stepped scaling</code></li> <li>bigger +/- based on difference</li> <li>allow you to act quicker to extreme changes</li> <li><code>preferred</code> over simple</li> <li><code>target tracking</code></li> <li>define a metric to maintain</li> </ul> </li> <li><code>run within a vpc</code></li> <li>subnets within the vpc are configured on the autoscaling groups</li> <li>configured subnets will be used to provision instances</li> <li>there will be an attempt to keep the number of instances in each subnet equal</li> <li><code>cooldown period</code></li> <li> <p>how long to wait after a scaling action before doing another one</p> </li> <li> <p><code>Integration with Load Balancers</code></p> </li> <li>used with ALB for <code>elasticity</code></li> <li>ASH instances can automatically be added to or removed from a load balancers target group</li> <li><code>can use load balancers health checks</code></li> <li><code>Scaling Processes</code></li> <li><code>launch and terminate</code><ul> <li>suspend and resume</li> </ul> </li> <li><code>AddToLoadBalancer</code><ul> <li>if instances is added to LB on launch</li> </ul> </li> <li><code>AlarmNotification</code><ul> <li>accept notifications from CW</li> </ul> </li> <li><code>AZRebalance</code><ul> <li>balance instances evenly across all of the AZs</li> </ul> </li> <li><code>HealthCheck</code><ul> <li>on/off</li> </ul> </li> <li><code>ReplaceUnhealthy</code></li> <li><code>ScheduledActions</code><ul> <li>on/off</li> </ul> </li> <li> <p><code>Standby</code></p> <ul> <li>protect instance from ASG, when doing maintenance</li> </ul> </li> <li> <p><code>Cost</code></p> </li> <li>ASH are Free</li> <li>only resources created are billed</li> <li>use cooldowns to avoid rapid scaling</li> <li>think about more smaller instances</li> </ul>"},{"location":"AWS/HAandScaling/#asg-scaling-policies","title":"ASG Scaling Policies","text":"<ul> <li><code>ASG can be created without scaling policies</code></li> <li>in this case min, max, desired capacity are <code>static</code></li> <li><code>manual scaling</code></li> <li>manual change scaling</li> <li>for test or urgent situations</li> <li><code>Dynamic scaling</code></li> <li>automatically scaling based on a criteria</li> <li><code>Simple scaling</code><ul> <li>define actions which occur when an alarm moves into an alarm state</li> <li>ex) cpu utilization less than 40%</li> <li>not flexible, or efficient</li> <li>same amount added or removed irrespective of size of load change</li> </ul> </li> <li><code>Step Scaling</code><ul> <li>more flexible, more conditions possible</li> <li>adjustments vary based on the size of alarm breach</li> <li>larger load changes can be configured to add or remove more than a lesser load change</li> </ul> </li> <li><code>Target Tracking</code><ul> <li>define an ideal value for a metric</li> <li>autoscaling groups makes adjustments to get close to the ideal metric value</li> </ul> </li> <li><code>Scaling based on SQS</code><ul> <li>ApproximateNumberOfMessagesVisible</li> <li>scale based on the number of messages in the queue</li> </ul> </li> </ul>"},{"location":"AWS/HAandScaling/#asg-lifecycle-hooks","title":"ASG Lifecycle Hooks","text":"<ul> <li><code>custom actions on instances during ASG actions</code></li> <li>instance launch or terminate transitions</li> <li>instances are paused</li> <li>until a timeout (then either continue or abandon)</li> <li>you complete the lifecycle action with<code>CompleteLifecycleAction</code></li> <li> <p>can be integrated with EventBridge or SNS Notifications</p> </li> <li> <p>Simple flow:</p> </li> <li><code>Scale out</code><ul> <li>Pending -&gt; Pending Wait -&gt; Pending : Proceed -&gt; inService</li> </ul> </li> <li><code>Scale in</code><ul> <li>Terminating -&gt; Terminating Wait -&gt; Terminating Proceed -&gt; Terminated</li> </ul> </li> <li>send messages to SNS or EventBridge</li> </ul>"},{"location":"AWS/HAandScaling/#asg-healthchecks","title":"ASG HealthChecks","text":"<ul> <li>ASG assess health of instances within a group using health checks</li> <li>if instance fails a health check, it is replaced</li> <li><code>EC2 Checks</code></li> <li>Stopping, Stopped, Terminated, Shutting Down or impaired (not 2/2 status) =&gt; <code>UNHEALTHY</code></li> <li><code>ELB Check</code></li> <li>running and passing ELB health check =&gt; <code>HEALTHY</code></li> <li>can be more <code>application aware</code></li> <li><code>Custom Check</code></li> <li>instances marked healthy or unhealthy by an external tool</li> <li><code>health check grace period (Default 300s)</code></li> <li>delay before starting checks</li> <li>allows system launch, bootstrapping and application start</li> </ul>"},{"location":"AWS/HAandScaling/#ssl-offload-session-stickiness","title":"SSL Offload &amp; Session Stickiness","text":"<ul> <li>three different ways ELB can handle SSL</li> <li><code>SSL Bridging</code><ul> <li>default</li> <li>listener is configured for HTTPS</li> <li>connection is terminated on the ELB, and needs a certificate for the domain name</li> <li>LB initiates a <code>new SSL connection to backend instances</code></li> <li>pros : ELB gets to see unencrypted HTTP an can take actions</li> <li>cons : ELB and instances require SSL certificates  and instance need compute required for cryptographic operations</li> </ul> </li> <li><code>SSL Pass Through</code><ul> <li><code>NLB passes clients connection directly to instances</code></li> <li>Listener listens on TCP</li> <li>pros : no certificate exposure to aws</li> <li>cons : no load balancing based on HTTP since it is never decrypted, instances still need SSL certs and compute for cryptography</li> </ul> </li> <li> <p><code>SSL Offloading</code></p> <ul> <li>Listener is configured for HTTPS, connections are terminated and then <code>backend connections use HTTP</code></li> <li>pros : certificate not required on instances</li> <li>cons : data is in plaintext format in aws network</li> </ul> </li> <li> <p><code>Session Stickiness</code></p> </li> <li>with no stickiness, connections are distributed equally across all in-service backend instances</li> <li>generates a cookie which licks the device toa  single backend instance for a duration<ul> <li><code>1s to 7 days</code></li> </ul> </li> <li>allow an application to function if the state of the user session is stored on an individual server</li> <li>can cause uneven load on servers, hurst load balancing</li> </ul>"},{"location":"AWS/HAandScaling/#gateway-load-balancer","title":"Gateway Load Balancer","text":"<ul> <li>network security at scale</li> <li>help you run and scale 3rd party appliances</li> <li>like firewalls, intrusion detection, and prevention systems</li> <li>inbound and outbound transparent traffic inspection and protection</li> <li>GWLB endpoints : where traffic enters or leaves from</li> <li>GLWB balances across multiple backend appliances</li> <li>traffic and metadata is tunneled using Geneve protocol</li> </ul>"},{"location":"AWS/HighAvailability_FaultTolerance_DisasterRecovery/","title":"High-Availability (HA)","text":"<ul> <li><code>ensure</code> an agreed level of operational performance, <code>usually uptime</code></li> <li>about <code>minimizing outages</code></li> <li>can be achieved with having standby servers that are automated to be used when issues on main servers appear</li> <li>not as good as <code>FT</code></li> <li></li> </ul>"},{"location":"AWS/HighAvailability_FaultTolerance_DisasterRecovery/#fault-tolerance-ft","title":"Fault-Tolerance (FT)","text":"<ul> <li>enable a system to <code>continue operating properly</code> in the event of the <code>failure of some</code> of its <code>components</code> </li> <li>about <code>operating through failure</code></li> <li>can be achieved with <code>redundancy</code> and ability to <code>route around failures</code></li> <li>harder and more expensive than  <code>HA</code></li> </ul>"},{"location":"AWS/HighAvailability_FaultTolerance_DisasterRecovery/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>enable the <code>recovery of vital technology</code> following a natural or human-induced <code>disaster</code></li> <li>involves <code>pre-planning</code> and the <code>DR Process</code></li> <li>taking regular offsite backups</li> <li>logins and keys and process should be saved</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/","title":"HybridEnvsAndMigration","text":""},{"location":"AWS/HybridEnvsAndMigration/#site-to-site-vpn","title":"Site-to-Site VPN","text":"<ul> <li>a logical connection between a VPC and on-premises network</li> <li><code>encrypted in transit using IPSec</code></li> <li>running over the public internet</li> <li>Partial <code>HA</code></li> <li>has different endpoints in different AZ</li> <li>single point of failure is customers end</li> <li>Full HA<ul> <li>use an additional connection using a separate router</li> </ul> </li> <li>Quick to provision (<code>less than an hour</code>)</li> <li><code>Virtual Private Gateway (VGW)</code></li> <li>associated with one vpc</li> <li>target of one or more route tables</li> <li><code>Customer Gateway (CGW)</code></li> <li>logical configuration in aws</li> <li>or physical device the logical configuration represents</li> <li><code>VPN Connection</code></li> <li> <p>between VGW and CGW</p> </li> <li> <p>Step 1</p> </li> <li>get ip address of the vpc</li> <li>get ip range of on premise network</li> <li>get address of customers physical router</li> <li>Step 2</li> <li>Create VGW and attach it to VPC</li> <li>Create CGW for customers physical router<ol> <li>define public ip address to mach physical router</li> </ol> </li> <li>Step 3</li> <li>Create a VPN connection<ol> <li>linked to VGW</li> </ol> </li> <li> <p>Specify CGW to use</p> <ol> <li>tunnels between each endpoint and the physical router will be created</li> </ol> </li> <li> <p>Static VPN</p> </li> <li>static routes, and network configs</li> <li>simple</li> <li>no load balancing or multi-connection failover</li> <li>Dynamic VPN</li> <li>uses Border Gateway Protocol (BGP)</li> <li>create a relationship between VGw and customer router<ul> <li>allows to communicate about networks, an state of links</li> </ul> </li> <li>multiple vpn connections<ul> <li>provide HA and traffic distribution</li> </ul> </li> <li> <p>route propagation</p> <ul> <li>routes are added RT's dynamically</li> <li>routes learned from VGW</li> </ul> </li> <li> <p>Consider the following</p> </li> <li><code>1.25Gbps</code> speed limit</li> <li>latency: inconsistent since its over the public internet</li> <li>hourly cost, GB out cost, data cap (on prem)</li> <li>fast setup (hours)</li> <li>can be used as a backup for or with direct connect</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#direct-connect-dx","title":"Direct Connect (DX)","text":"<ul> <li>a <code>1Gbps</code> or <code>10Gbps</code> network port into AWS</li> <li>at a DX location</li> <li>1000-Base-LX or 10GBASE-LR standards</li> <li>just a <code>port</code></li> <li>customer/partner router needs to be connected to the port inside the DX location</li> <li>you need to arrange for cable to be extended to physical premises</li> <li><code>No HA</code></li> <li><code>No Encryption</code></li> <li>takes weeks or months</li> <li>Customer Router</li> <li>requires <code>VLANS</code> AND <code>BPG</code></li> <li><code>Virtual Interfaces (VIFS)</code></li> <li>isolated virtual connection that run overtop the physical DX connection</li> <li>can have multiple over one DX</li> <li><code>private VIF</code><ul> <li>associated with a <code>VGW</code> </li> <li>connect to a single VPC</li> </ul> </li> <li><code>public vif</code><ul> <li>provide connectivity to aws public zone services</li> </ul> </li> <li> <p>one vif = one VLAN and on BGP session</p> </li> <li> <p>Consideration</p> </li> <li>takes much long vs vpn</li> <li>faster than vpn : 40Gbps with aggregation</li> <li>low latency : doesn't use business bandwidth, uses aws network</li> <li>no encryption</li> <li>run IPSEC VPN over public VIF</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#resilience-and-ha","title":"Resilience and HA","text":"<ul> <li>region, DX location, Customer premises</li> <li>AWS Regions have multiple DX locations</li> <li>dx locations are connected to regions with HA networking</li> <li>cross-connect : a single cord connection bw DX port and customer router in DX location</li> <li>DX  is extended to customer premises]</li> <li>single points of failure</li> <li>DC location, DX router, Cross-connect, Customer DX router, Extension,, Customer Premises, customer router</li> <li>Adding Resilience and HA</li> <li>create multiple connections in same DX location<ul> <li>use more than one DX routers connected to their own customer DX router and extension</li> </ul> </li> <li>Use more than one DX locations and customer premises</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#transit-gateway-tgw","title":"Transit Gateway (TGW)","text":"<ul> <li>Network Transit Hub to connect VPC's to each other and on premises networks</li> <li>uses site-to-site VPN's and direct connect</li> <li>reduces network complexity</li> <li>single network object : HA and Scalable</li> <li>Attachments to other network types</li> <li>VPC, Site-to-Site VPN, Direct Connect Gateway</li> <li>VPC</li> <li>configured with a subnet in each AZ where service is required</li> <li>acts as HA inter VPC router (VPC peering replacement)</li> <li>transitive routing</li> <li>Site-to-Site VPN</li> <li>each customer premises gateway only has to connect to single transit gateway</li> <li>gives VPC's connection to on premises environment using VPN attachment</li> <li>Direct Connect Gateway</li> <li>Peering TGW</li> <li>Cross-Region, Same/Cross-Account</li> <li>uses aws global network : benefits from better latency than public internet</li> <li> <p>Support multiple route tables allowing complex routing architectures</p> </li> <li> <p>Considerations</p> </li> <li>supports transitive routing</li> <li>can be used to create global networks</li> <li>share between accounts using AWS RAM</li> <li>peer with different regions, same or cross account</li> <li>less complexity routing complexity</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#storage-gateway","title":"Storage Gateway","text":"<ul> <li>hybrid storage virtual appliance</li> <li>virtual environment on premises or in a data center</li> <li><code>Use Cases</code></li> <li><code>Extension</code> of File &amp; Volume Storage into AWS</li> <li>Volume Storage or Tape <code>backups</code> into AWS</li> <li><code>Migration</code> of existing infrastructure into AWS</li> <li><code>Modes</code></li> <li><code>Tape Gateway</code> (VTL)<ul> <li>storage gateway pretends to be a iSCSI tape library, changer, and drive</li> <li>stores virtual tapes in S3/Glacier</li> <li>virtual tape 100GB - 5TiB, <code>1PB</code> total storage across 1500 virtual tapes</li> <li>unlimited VTS (archived glacier) storage</li> </ul> </li> <li><code>File</code> <ul> <li>SMB and NFS</li> <li>file storage backed by S3 objects</li> <li>lifecycle policies can automatically control storage classes</li> <li>integrates with <code>Active Directory</code> for file authorization</li> </ul> </li> <li><code>Volume</code><ul> <li><code>Stored</code></li> <li>16TB per volume, 31 volume limit, <code>512TB</code> total capacity</li> <li>primary data is stored on-premises</li> <li>volumes are made available via iSCSI for network based servers to access</li> <li>backup data is asynchronously replicated to AWS creating EBS snapshots</li> <li>Ideal fo <code>migration and disaster recovery</code></li> <li><code>Cached</code></li> <li>primary data is stored in AWS, and snapshots are stored as standard EBS snapshots</li> <li>data that is accessed frequently is cached locally</li> <li><code>ideal for extending storage into AWS</code></li> <li>32TB per volume, 32 volumes, <code>1PB</code> total capacity</li> </ul> </li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#snowball-snowball-edge-and-snowmobile","title":"Snowball, Snowball Edge and Snowmobile","text":"<ul> <li>move large amounts of data <code>IN and OUT</code> of AWS</li> <li><code>Snowball</code></li> <li>ordered from AWS, device delivered  (not instant)</li> <li>data is encrypted using KMS</li> <li>50TB or 80TB</li> <li>1Gbps or 10Gbps network cabal needed to connect to device</li> <li><code>10TB to 10PB economical range</code></li> <li><code>multiple devices to multiple premises</code></li> <li><code>only storage</code></li> <li><code>Snowball Edge</code></li> <li><code>Storage and Compute</code></li> <li>large capacity than snowball</li> <li>faster networking than snowball</li> <li><code>storage optimized</code>(with EC2)<ul> <li>80TB, 24vCPU, 32Gib RAM</li> <li>1TB SSD if EC2 included</li> </ul> </li> <li><code>compute optimized</code><ul> <li>100TB + 7.68 NVME, 52vCPU, and 208GiB RAM</li> </ul> </li> <li><code>compute with GPU</code><ul> <li>same as compute optimized with a GPU</li> </ul> </li> <li>ideal for remote sites, or where data processing on ingestion is needed</li> <li><code>Snowmobile</code></li> <li>portable DC on a truck</li> <li>special order</li> <li>ideal for single location when 10PB+ is required</li> <li>up to 100PB per snowmobile</li> <li>note economical for multi-site or sub 10PB</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#directory-service","title":"Directory Service","text":"<ul> <li><code>provides managed directory service instances</code></li> <li>store of users, objects, and other configurations</li> <li>Directory</li> <li>store objects with a structure (domain/tree)<ul> <li>users, groups, computers, servers, file shares</li> </ul> </li> <li>multiple trees can be grouped into a forest</li> <li>commonly used in windows environments</li> <li>sign-in to multiple devices with same username/password, provides centralized management for assets</li> <li>runs within a vpc</li> <li>HA, deploys multiple AZs</li> <li> <p>some AWS services need a directory eg) Amazon Workspaces</p> </li> <li> <p><code>Modes</code></p> </li> <li><code>Simple AD</code><ul> <li>implementation of Samba 4 (compatibility with basics AD functions)</li> <li>500 users (small) or 5000 (large)</li> <li>isolated, not designed to integrate with existing on-premises directory</li> </ul> </li> <li><code>Managed Microsoft AD</code><ul> <li>primary running location is in AWS</li> <li>can create a trust relationship with existing on-premises directory</li> <li>over private networking (VPN or DX)</li> <li>resilient if the VPN fails : aws services will still be able to access the directory service</li> <li>directly supports applications that required actual Microsoft AD</li> </ul> </li> <li><code>AD Connector</code><ul> <li>need to establish private network connectivity with on-premises</li> <li>proxies requests back to an on-premises directory</li> <li>primary directory still on-premises</li> <li>proxy won't work if private connection fails</li> <li>ideal when on-premises directory already exists, and you don't want to create another on</li> </ul> </li> <li><code>Picking a Mode</code></li> <li><code>simple AD</code> : default</li> <li><code>Microsoft AD</code> : when MS AD is required by application, or you need to trust AD DS</li> <li><code>AD Connector</code> : use aws services which need a directory</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#datasync","title":"DataSync","text":"<ul> <li><code>Data transfer service TO and FROM AWS</code></li> <li>Migrations, Data processing Transfers, Archival/Cost Effective Storage, or DR/Business Continuity</li> <li>keeps metadata eg)permissions</li> <li>built in data validation</li> <li>Scalable : 10Gbps per agent (~100TB per day)</li> <li>Bandwidth Limiters : avoid the saturation of internet links</li> <li>Incremental and scheduled transfer options</li> <li>Compression and encryption in transit</li> <li>automatic recovery</li> <li><code>AWS Service integration : S3, EFS, FSx</code></li> <li>pay as you use : per GB</li> <li>agent runs on-premises and connects with AWS DataSync endpoint</li> <li>read data from NFS or SMB</li> <li> <p>transfers data into AWS</p> </li> <li> <p><code>Task</code> : A job within DataSync</p> </li> <li>define what is being synced, how quickly, from and where to</li> <li><code>Agent</code> : Software used tot read or write to on-premises data stores using NFS or SMB</li> <li><code>Location</code></li> <li>FROM and TO</li> <li>NFS, SMB, EFS, FSx, S3</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#fsx","title":"FSx","text":""},{"location":"AWS/HybridEnvsAndMigration/#fsx-for-windows-servers","title":"FSx for Windows Servers","text":"<ul> <li>fully <code>managed</code> native windows file servers/shares</li> <li><code>designed for integration with windows environments</code></li> <li><code>integrates with directory service or self-managed AD</code></li> <li>single or multi-AZ within a VPC</li> <li>on-demand and scheduled backups</li> <li>accessible using, VPC, Peering, VPN, DX</li> <li>KMS at rest encryption and in transit</li> <li>shares accessed via <code>SMB</code> protocol</li> <li>supports volume shadow copies : file level versioning</li> <li><code>highly performant</code> : 8MB/s -&gt; 2GB/s, 100ks IOPS, &lt;1ms latency&gt;</li> <li><code>VSS</code> : user driven restore of files</li> <li><code>windows permission model</code></li> <li>supports <code>DFS</code> : scale out file share structure (distributed file system)</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#fsx-for-lustre","title":"FSx for Lustre","text":"<ul> <li><code>managed</code> Lustre</li> <li>designed for <code>HPC</code> - Linux Clients</li> <li>machine learning, big data, financial modelling</li> <li>supports <code>POSIX</code></li> <li>100's GB/s throughput and sub millisecond latency</li> <li><code>Deployment Types</code></li> <li><code>Scratch</code><ul> <li>highly optimized for short term</li> <li>no replication</li> <li>no HA</li> <li>fast</li> <li>200MB/s per TiB</li> </ul> </li> <li><code>Persistent</code><ul> <li>longer term</li> <li>HA in one AZ</li> <li>self-healing</li> <li>50/100/200MB/s per TiB</li> </ul> </li> <li>burst : 1300MB/s per TiB (credit system)</li> <li>Accessible within VPC, over VPN, or Direct Connect</li> <li>Can be associated with a repository</li> <li>which is an S3 bucket</li> <li>objects in bucket are visible in file system</li> <li>data is lazily loaded from S3 into file system when first accessed</li> <li>data can be exported back to S3 using <code>hsm_archive</code></li> <li>Architecture</li> <li>metadata store on metadata targets (MDTs)</li> <li>objects are stored on called object storage targets (OSTs)<ul> <li>1.17TiN in size</li> </ul> </li> <li>storage servers handle requests placed against the file system, and provide cache for frequently accessed data</li> <li> <p><code>baseline performance based on size</code></p> <ul> <li>size min 1.2TiB</li> </ul> </li> <li> <p>Considerations</p> </li> <li>larger file systems mean more servers, more disks and more chances of failure</li> </ul>"},{"location":"AWS/HybridEnvsAndMigration/#transfer-family","title":"Transfer Family","text":"<ul> <li>managed file transfer service</li> <li><code>supports transferring TO and FROM s3 and EFS</code></li> <li>providers managed servers which support  protocols</li> <li>Protocols</li> <li>File Transfer Protocol (<code>FTP</code>) : unencrypted</li> <li>File Transfer Protocol Secure (FTPS) : TLS encryption</li> <li>Secure Shell (<code>SSH</code>) File Transfer Protocol (<code>SFTP</code>) : over SSH</li> <li>Applicability Statement 2 (AS2) : structured B2B data</li> <li>Identities</li> <li>Service managed</li> <li>Directory service</li> <li>Custom (lambda/APGW)</li> <li>Managed File Transfer Workflows (MFTW)</li> <li>serverless file workflow engine</li> <li>Endpoint Types</li> <li><code>Public</code><ul> <li>endpoint in aws public zone, accessible from public internet</li> <li>only SFTP</li> <li>dynamic IP</li> </ul> </li> <li>VPC<ul> <li>VPC with Internet</li> <li>SFTP, FTPS, AS2</li> <li>VPC Internal</li> <li>SFTP, FTP, FTPS, AS2</li> <li>accessible via VPN/DX</li> <li>static IP, can use SG or NACL</li> </ul> </li> <li>Multi-AZ, resilient and scalable</li> <li>billed server per hour and data transferred</li> <li>FTP and FTPS : Directory Service or Custom IDP only</li> <li>FTP : VPC internal only</li> <li>AS2 : VPC only</li> <li>if you need to access S3/EFS but with existing protocols</li> <li>integrating with existing workflows</li> </ul>"},{"location":"AWS/IAM/","title":"IAM","text":""},{"location":"AWS/IAM/#aws-accounts","title":"AWS Accounts","text":"<p>An AWS Account container for identities (users) and resources</p> <p>The <code>ROOT USER</code> is created when the account is created. - has full control over the account and any resources within it - can't be restricted or deleted</p> <p>AWS <code>Multi-Factor Authentication (MFA)</code> is a simple best practice that adds an extra layer of protection on top of your user name and password.</p>"},{"location":"AWS/IAM/#tips-when-making-a-new-account","title":"Tips when making a new account","text":"<ul> <li>use gmail, <code>+</code> to create multiple accounts with different emails all pointing to the same email address.</li> <li>enable MFA for root user</li> <li>create a budget</li> <li>enable iam user and role access to billing</li> </ul>"},{"location":"AWS/IAM/#identity-and-access-management-iam","title":"Identity and Access Management (IAM)","text":"<ul> <li>globally resilient service</li> <li>can create other identities</li> <li><code>users</code> : humans or apps that need access to the account<ul> <li>a certain number of entities</li> </ul> </li> <li><code>groups</code> : collections of related users</li> <li><code>roles</code> : used by <code>aws services</code>, or <code>external access</code> to  your account<ul> <li>for an uncertain number of entities</li> </ul> </li> <li><code>policies</code> : allow or deny access to aws services when attached to a user, group, or rol</li> </ul> <p>Three main jobs... 1. <code>Manages identities</code> : an ID provider (<code>IDP</code>) 2. <code>Authenticate</code> : prove you are who you claim to be 3. <code>Authorize</code> : allow or deny access to resources</p> <ul> <li>No cost</li> <li>Global service</li> <li>no direct control on external accounts or users</li> <li>identity federation</li> </ul>"},{"location":"AWS/IAM/#iam-access-keys","title":"IAM Access Keys","text":"<p>Long term credentials... -&gt; don't change regularly - can have two at a time - can be created, deleted, made inactive or active</p> <p>Has two parts : <code>Access Key ID + Secret Access Key</code></p>"},{"location":"AWS/IAM/#iam-policies","title":"IAM Policies","text":"<p>Attached to AWS identities and either ALLOW or DENY access to AWS resources.</p> <p>Comprised of one or more statements that define what actions to allow and deny a resource.</p> <ul> <li><code>Sid</code>: (statement id), used to help reader what a statement does</li> <li><code>Effect</code> : what happens when the <code>action</code> and <code>resource</code> match the policy</li> <li><code>Action</code> : One or more actions the policy is concerned with</li> <li><code>Resource</code> : One or more resources the policy is concerned with</li> </ul> <p>Statements with <code>DENY</code> effect are given priority over <code>ALLOW</code>. The default effect is <code>DENY</code>.</p> <p>Inline vs Managed Policy - inline policies are created by adding a policy to an individual identity or resource   - used for specific or exceptional allows or denies - managed policies are created on their own and then attached to one or more entities   - reusable, low overhead   - should be used for the normal default operational rights ina business</p>"},{"location":"AWS/IAM/#iam-users","title":"IAM Users","text":"<p>An identity used for <code>long term aws access</code> - humans, applications, or service accounts</p> <p><code>Principle</code> : An entity trying to access an aws account - person or application, needs to authenticate against an identity within an IAM - authentication is done with either <code>username: password</code> or <code>access keys</code> - once an identity is authenticated, then aws knows which policies apply to it to authorize it</p> <p><code>authentication</code> : proving identity <code>authorization</code> : checking which statements apply to the identity</p> <p>Limits - max <code>5,000</code> users per account - user can be a part of a max <code>10</code> groups</p>"},{"location":"AWS/IAM/#amazon-resource-name","title":"Amazon Resource Name","text":"<p>Used to uniquely identify resources within any aws accounts. - used in policies to define the resources a statement is about</p> <pre><code>arn:aws:s3:::bucket     # refers to the bucket\narn:aws:s3:::bucket/*   # refers to every object within the bucket, but not the bucket\n</code></pre>"},{"location":"AWS/IAM/#iam-groups","title":"IAM Groups","text":"<p>Containers for IAM Users.  - used to organize large sets of IAM Users. - not real identities,    - no credentials    - can't be used to log in or in resource policies   - can't be referenced as a principal in a policy - can have both inline and managed policies attached - no nesting of groups - <code>300</code> groups per account</p>"},{"location":"AWS/IAM/#iam-roles","title":"IAM Roles","text":"<ul> <li>used when an identity is going to be assumed by multiple principles</li> <li>assuming a role gives you temporary security credentials</li> <li><code>trust policy</code> : which identities can assume a role</li> <li><code>permissions policy</code> : policy that allows or denies the permissions</li> </ul> <p><code>use cases :</code> 1. common use case is when an aws service needs permissions 2. When a user needs an emergency increase of permissions 3. when integrating aws with an existing identity provider (identity federation)</p> <p><code>benefits:</code> - now aws credentials on app, (temporarily created by role) - can be used with existing customer logins (facebook, active directory) - scale to millions of users to get around the iam user limit</p> <p><code>service linked roles</code> - role that is linked directly to an AWS service - predefined by a service - provide permissions that a service needs to interact with other aws services - service might create or delete the role - <code>iam:PassRole</code> action allows an identity to pass an existing role to another service (role separation)</p>"},{"location":"AWS/IAM/#aws-organizations","title":"AWS Organizations","text":"<p>Manage many accounts.</p> <p><code>Steps:</code> 1. Create an aws organization with a standard account    - account that creates the organization is now the <code>management account</code>    - <code>standard accounts</code> are those not belonging to the organization 2. Invite other standard accounts into the organization    - the standard accounts who accept the invite are now <code>member accounts</code></p> <p><code>Composition:</code> 1. <code>Organization Root</code> root container for aws organization   - contains either member accounts or the management account   - can also contain other containers called <code>organizational units (OU)</code>     - OU's can contain accounts or other OU's</p> <p><code>Features:</code> 1. Consolidated Billing    - accounts that join the organization lose their own billing method    - member accounts pass their billing through aws <code>management account</code> or <code>payer account</code>    - consolidation of <code>reservations</code> and <code>volume discounts</code> 2. Can create new accounts directly within an organization</p> <p><code>Common Pattern: Identity federation</code> - single aws account that contains all the identities that are logged into - use a feature called <code>role switch</code>, to role switch into other member accounts of the organization   - behind the scenes, this is done by <code>assuming roles</code> in the other aws accounts</p>"},{"location":"AWS/IAM/#service-control-policies","title":"Service Control Policies","text":"<p>A policy document, attached to either: - root container of an organization (impact all accounts in organization) - or on or more organizational units (impact all accounts in the OU) - or individual aws accounts</p> <ul> <li><code>Member</code> accounts can be effected, the <code>management</code> account cannot.</li> <li>SCP's are <code>account permission boundaries</code></li> <li>limit what account (including account root user) can do<ul> <li><code>limiting account effectively limits the root user</code></li> </ul> </li> <li><code>don't grant permission</code><ul> <li>control what permissions an account <code>CAN and CANNOT grant</code> via <code>identity policies</code></li> </ul> </li> <li>use a deny list architecture</li> <li>implicitly allow all permissions, then add policy to deny services you want to restrict</li> </ul>"},{"location":"AWS/IAM/#control-tower","title":"Control Tower","text":"<ul> <li>quick and easy setups of multi-account environment</li> <li>orchestrates other aws services to provide its functionality</li> <li><code>landing zone</code></li> <li>multi-account environment</li> <li><code>SSO/ID Federation</code></li> <li>default ou's:<ul> <li>Security : log archive &amp; Audit Accounts (CloudTrail &amp; Config Logs)</li> <li>Sandbox : test/less rigid security</li> </ul> </li> <li>monitoring and notifications : cloudwatch and sns</li> <li><code>guard rails</code></li> <li>detect/mandate rules/standards across all accounts</li> <li>mandator, strongly recommended, elective</li> <li><code>preventive</code> : stops you doing things using scp</li> <li><code>detective</code> : compliance checks (config rules)</li> <li><code>account factory</code></li> <li>automate and standardize new account creation</li> <li>guard rails automatically added</li> <li>account and network standard configuration</li> <li><code>dashboard</code></li> <li>single page oversight of entire environment</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/","title":"InfrastrucureAsCode","text":""},{"location":"AWS/InfrastrucureAsCode/#cloudformation-physical-logical-resources","title":"CloudFormation Physical &amp; Logical Resources","text":"<ul> <li>cloudformation template : YAML or JSON</li> <li>used to create stacks</li> <li>logical resource : what to create<ul> <li>can be reference / queried once create is in complete status</li> </ul> </li> <li>Stack</li> <li>create physical resources from the logical</li> <li>if stacks template changes, then physical resources are also changed</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#template-and-pseudo-parameters","title":"Template and Pseudo Parameters","text":"<ul> <li><code>template parameters</code></li> <li>accept input via console/cli/api</li> <li>can be referenced from within logical resources<ul> <li>influence the physical resources</li> </ul> </li> <li>Options<ul> <li>Defaults, AllowedValues, Min/Max length, AllowedPatterns, NoEcho, Type</li> </ul> </li> </ul> <pre><code>Parameters:\n  InstanceType:\n  Type: String\n  Default: t3.micro\n  AllowedValues:\n    - t3.micro\n    - t3.medium\n    - t3.large\n  Description: Pick a supported instance type\n</code></pre> <ul> <li><code>psuedo parameter</code></li> <li>made available by aws, can be referenced without being defined</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#intrinsic-functions","title":"Intrinsic Functions","text":"<ul> <li><code>Ref</code></li> <li>reference parameter value or logical resource physical ID </li> <li><code>Fn::GetAtt</code></li> <li>get attribute associated with a resource<ul> <li>like publicIP</li> </ul> </li> <li><code>Fn::Join</code> &amp; <code>Fn::Split</code></li> <li>join and split strings</li> <li><code>Fn::GetAzs</code></li> <li>get list of Azs where default vpc has subnets in</li> <li><code>Fn::Select</code></li> <li>select one element from a list</li> <li><code>Conditions (F::)</code></li> <li>IF, AND, EQUALS, NOT, OR</li> <li><code>Fn::Base64</code></li> <li>accepts non encoded text, and outputs encoded text</li> <li><code>Fn::Sub</code></li> <li>substitute things with text base on runtime information</li> <li><code>Fn::Cidr</code></li> <li>generate a list of smaller cidr ranges for subnets from a larger vpc</li> <li>Fn::ImportValue, Fn::Transform</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#mappings","title":"Mappings","text":"<ul> <li>optional</li> <li>contain mappings of keys to values</li> <li>allow lookups</li> <li>can have a key, or Top &amp; Second Level</li> <li>Fn::FindInMap intrinsic function to retrieve values in a map.</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#outputs","title":"Outputs","text":"<ul> <li>optional</li> <li>values can be declared in this section</li> <li>accessible from parent stack when using nesting</li> <li>can be exported, allowing cross stack references</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#conditions","title":"Conditions","text":"<ul> <li>created in optional Conditions section of template</li> <li>evaluated to true or false</li> <li>before resources are created</li> <li>use intrinsic functions AND, EQUALS, IF, NOT, OR</li> <li>associated with logical resources to control if they are created or nota</li> <li>can be nested</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#dependson","title":"DependsOn","text":"<ul> <li>establish formal dependencies between resources</li> <li>cloudformation tries to automatically understand dependencies using references or functions used in the template</li> <li>can specify list of resources to define multiple dependencies</li> <li>example : make elastic ip depend on internet gateway, since an igw is needed to create an eip</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#wait-conditions-creation-policy-cfn-signal","title":"Wait Conditions, Creation Policy &amp; cfn-signal","text":"<ul> <li><code>cnf-signal</code></li> <li> <p>utility running on the instance that sends message back to cloudformation</p> </li> <li> <p>configure cloudformation to wait for certain number of success signals</p> </li> <li>timeout H:M:S for those signals (12hr max)</li> <li>failure : failure signal or timeout before getting all signals </li> <li><code>creation policy</code></li> <li>generally used for ec2 instances or autoscaling groups</li> <li> <p>create endpoint that can be signaled to indicate when instance is really in created state </p> </li> <li> <p><code>wait condition</code></p> </li> <li>for general progress gate</li> <li>its own logical resource, not defined in an existing one</li> <li>can depend on other resources, and vice versa</li> <li><code>WaitHandle</code></li> <li><code>generate presigned url</code> for resource signals</li> <li><code>can get response</code><ul> <li>response can be accessed elsewhere in the template</li> </ul> </li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#nested-stacks","title":"Nested Stacks","text":"<ul> <li><code>root / parent</code> stack</li> <li>has nested stacks </li> <li>can have a <code>cloudformation as a resource</code></li> <li>require a url to the template</li> <li>must pass parameters</li> <li>root stack <code>can reference outputs of nested stacks</code></li> <li>STACKNAME.Outputs.xxx</li> <li>break up stack into modular templates allows you to <code>reuse templates</code></li> <li><code>lifecycle linked</code></li> <li><code>overcome 500 resource limit of one stack</code></li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#cross-stack-references","title":"Cross-Stack References","text":"<ul> <li>cloudformation stack are designed to be isolated and self-contained</li> <li>outputs can be exported, making them visible to other stacks</li> <li>exports must have a unique name in the region</li> <li>Fn::ImportValue used to reference export values</li> <li>when you want to reuse resources</li> <li>different life-cycles</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#stacksets","title":"StackSets","text":"<ul> <li>deploy cfn stacks across many accounts &amp; regions</li> <li>StackSets</li> <li>containers in an admin account</li> <li>contain stack instances which reference a stack</li> <li>Stack instances and stacks</li> <li>in target accounts</li> <li>each stack = 1 region 1 account</li> <li>workflow</li> <li>permission granted via self-managed or service-managed roles  AWS ORG</li> <li>StackSets gain access to target accounts and create stack instances followed by the stacks themselves</li> <li>Concurrent Accounts</li> <li>how many accounts can be deployed into</li> <li>more concurrent accounts the faster resources are deployed</li> <li>Failure Tolerance</li> <li>amount of individual deployments can fail, before the stack set is considered a fail</li> <li>Retain Stacks</li> <li>remove stack instances but retain the stacks</li> <li>Scenario:</li> <li>Enable AWS Config across many accounts</li> <li>Create AWS Config Rules : MFA, EIPS, EBS Encryption</li> <li>Create IAM Roles for cross-account access at scale</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#deletion-policy","title":"Deletion Policy","text":"<ul> <li>if you delete a logical resource from a template</li> <li>by default the physical resource is deleted</li> <li>this can cause data loss</li> <li>deletion policy can be defined on each resource</li> <li>Delete : Default</li> <li>Retain : physical resource will not be deleted if logical resource is</li> <li>Snapshots : snapshot of resource before it is deleted</li> <li>supports : EBS Volume, ElastiCache, Neptune, RDS, Redshift</li> <li>snapshot live past stack lifetime</li> <li>Only apply to delete, not replace</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#stack-roles","title":"Stack Roles","text":"<ul> <li>by default cfn uses the permissions of the logged in identity</li> <li>mean you need the permissions for aws</li> <li>cfn can assume a role to gain the permissions</li> <li>allows for role separation</li> <li>identity creating the stack doesn't need resource permissions, only needs PassRole to cfn</li> <li>identity also needs the permissions to create, update, delete stacks</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#cfn-init","title":"CFN-INIT","text":"<ul> <li>simple configuration management system</li> <li>runs during bootstrapping</li> <li>configuration directives stored in template</li> <li>AWS::CloudFormation::Init part of logical resource</li> <li>config key : containers of configuration directives</li> <li>configSets</li> <li>group of config keys</li> <li>define which config keys to use and in what order</li> <li>user data vs cfn-init</li> <li>User data : HOW: procedural</li> <li>cfn-init : WHAT : desired state<ul> <li><code>idempotent</code></li> </ul> </li> <li>cfn-init helper script</li> <li>installed on ec2 os</li> <li>workflow:</li> <li>template has ec2 resource with cloudformation init in metadata</li> <li>cfn-init helper tool works in userdata<ul> <li>gets data from stack</li> </ul> </li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#cfn-hup","title":"CFN-HUP","text":"<ul> <li>cfn-init only runs once</li> <li>cfn-hup helper is a daemon that is installed an runs on the instance</li> <li>chfn-hup detects changes in resource metadata</li> <li>run configurable actions when a change is detected</li> <li>re-runs cfn-init</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#changesets","title":"ChangeSets","text":"<ul> <li>usual flows:</li> <li>template -&gt; stack -&gt; create physical resources</li> <li>delete stack -&gt; delete physical resources</li> <li>update template -&gt; update stack -&gt; resources change<ul> <li>no interruption, some interruption, replacement</li> </ul> </li> <li>Change Sets let you preview changes</li> </ul>"},{"location":"AWS/InfrastrucureAsCode/#custom-resources","title":"Custom Resources","text":"<ul> <li>cloudformation doesn't support everything</li> <li>custom resources let cfn integrate with anything it doesn't yet</li> <li>passes data to something, gets data back to something</li> </ul>"},{"location":"AWS/KMS/","title":"Key Management Service","text":"<ul> <li>Regional and Public Service (AWS public zone)</li> <li>create, store and manage keys</li> <li>symmetric and asymmetric keys</li> <li>cryptographic operations (encrypt, decrypt, ...)</li> <li>keys never leave KMS</li> <li>complies with FIPS 140-2 L2</li> </ul> <p><code>Customer Master Key : CMK</code> - isolated to a region - AWS Managed CMK   - created automatically by AWS when a service that uses encryption is used   - rotates every 3 years - Customer Managed CMK   - created explicitly by customer   - configurable through key policies   - rotation is optional, 1 year cycles - alias : shortcut to a particular CMK, scoped per region - used by user and application in encryption operations - container for actual physical master keys   - logical : ID, date, policy, desc &amp; state   - backed by physical key material     - generated or imported   - used for up to 4KB of data</p> <p><code>Key Policies</code> - KMS has to be explicitly told that keys trust the aws account they are in - key trust accounts, which then allows account to manage it by applying iam permission policies to iam users in the account - can grant one group the ability to create keys, and others to use them, at different granularity's for role separation</p> <p><code>Data Encryption Keys : DEKs</code> - generated by KMS using CMK - used to encrypt data larger than 4kb - steps   - kms create two version of dek     - a plaintext and a cipher-text version      - cipher-text version is encrypted by a CMK that can eventually decrypt it   - plaintext version is used to encrypt data and is immediately discarded   - encrypted data and key are then stored side-by-side   - send encrypted key to KMS that decrypts it with the same CMK that created it   - then use the newly decrypted key to decrypt that data   - discard the data encrypted key</p>"},{"location":"AWS/KMS/#encryption-approaches","title":"Encryption Approaches","text":"<p>Encryption at Rest - protect data while it is being stored</p> <p>Encryption in Transit - protect data while it is being transferred - sender encrypts data before sending, receiver decrypts when it receives</p> <ul> <li>Plaintext : Un-encrypted data</li> <li>Algorithm : code that takes plain text and an encryption key to generate encrypted data</li> <li>Key : password</li> <li>Cipher text : encrypted data that is created by an algorithm taking a key and plaintext</li> </ul> <p>Symmetric Encryption - same key is used to encrypt and decrypt data</p> <p>Asymmetric Encryption - public and private key - public encrypts but cannot decrypt, private key decrypts - more computationally difficult than symmetric   - sometimes used to initially agree on a symmetric key, and then the symmetric key is used </p> <p>Signing - used to prove identity - sender signs message with his private key   - receiver can use sender public key to verify that the private key was used to sign the message</p> <p>Steganography - used to hide the fact that encryption was used - cipher text is embedded in another plaintext data that serves as a medium of transportation</p>"},{"location":"AWS/MachineLearning/","title":"MachineLearning","text":""},{"location":"AWS/MachineLearning/#comprehend","title":"Comprehend","text":"<ul> <li>natural language processing service</li> <li>input = document</li> <li>output = entities, key phrases, language, sentiment, PII ...</li> <li>pre-trained models or custom</li> <li><code>real-time</code> analysis for small workloads</li> <li><code>async</code> jobs for larger workloads</li> </ul>"},{"location":"AWS/MachineLearning/#forecast","title":"Forecast","text":"<ul> <li>forecasting for time-series data</li> <li>import historical and related data</li> <li>output = forecast and forecast expandability</li> </ul>"},{"location":"AWS/MachineLearning/#fraud-detector","title":"Fraud Detector","text":"<ul> <li>fully managed fraud detection service</li> <li>upload historical data and choose model type</li> <li>model type</li> <li><code>online fraud</code> : little historical data</li> <li><code>transaction fraud</code> : suspect payments</li> <li><code>account takeover</code> : phishing or other social based attack</li> </ul>"},{"location":"AWS/MachineLearning/#kendra","title":"Kendra","text":"<ul> <li><code>intelligent search service</code></li> <li>supports wide range of question types</li> <li>types of questions</li> <li><code>factoid</code> : who, what, where</li> <li><code>descriptive</code>: how</li> <li><code>keyword</code></li> <li><code>index</code> : searchable data organized in an efficient way</li> <li><code>data source</code> : where data lives</li> <li>kendra connects and indexes from this location</li> <li>synchronize with index on a schedule</li> <li>documents : <code>structured or unstructured</code></li> </ul>"},{"location":"AWS/MachineLearning/#lex","title":"Lex","text":"<ul> <li><code>text or voice conversational interfaces</code></li> <li>automatic <code>speech recognition</code> (ASR)</li> <li>natural <code>language understanding</code> (NLU)</li> <li><code>bots</code></li> <li>converse in 1+ languages</li> <li>utterances : different ways something can be said</li> <li>how to fulfil intent<ul> <li>usually lambda function</li> </ul> </li> <li>slots</li> <li>parameters for intent</li> </ul>"},{"location":"AWS/MachineLearning/#polly","title":"Polly","text":"<ul> <li>turns text into lifelike speech</li> <li>tts types</li> <li>standard tts</li> <li>neural tts (much better)</li> <li>output formats : mp3, ogg vorbis ...</li> <li>Speech Synthesis Markup Language (SSML)</li> <li>addition control on how polly generates speech</li> <li>emphasis, pronunciation, whispering ...</li> </ul>"},{"location":"AWS/MachineLearning/#rekognition","title":"Rekognition","text":"<ul> <li>deep learning based image and video analysis</li> <li>identify object, people, content moderation ...</li> <li>pay per image</li> </ul>"},{"location":"AWS/MachineLearning/#sagemaker","title":"SageMaker","text":"<ul> <li>fully managed machine learning service</li> <li>fetch, clean, prepare, train, evaluate, deploy, monitor</li> <li>sagemaker studio : IDE for ML lifecycle</li> <li>sagemaker domain : isolation/grouping for a project</li> <li>containers : docker containers deployed to ml ec2 instance</li> <li>hosting : deploy endpoints for your models</li> <li>no cost : but the resources it creates do</li> </ul>"},{"location":"AWS/MachineLearning/#textract","title":"Textract","text":"<ul> <li>detect and analyze text  contained in input documents</li> <li>input = JPEG, PNG, PDF, TIFF</li> <li>synchronous and asynchronous</li> <li>pay for usage</li> <li>detection of text</li> <li>relationships in text, metadata</li> <li>extracts fields from documents, receipts, IDs</li> </ul>"},{"location":"AWS/MachineLearning/#transcribe","title":"Transcribe","text":"<ul> <li>automatic speech recognition (ASR)</li> <li>input = audio</li> <li>output = text</li> <li>pay as you use</li> <li>options</li> <li>language customizable, filers for privacy, audience appropriate language, speaker identifier, custom vocabulary</li> <li>use case</li> <li>index audio to allow searching</li> <li>meeting notes</li> <li>subtitles/captions and transcripts</li> <li>call analytics</li> </ul>"},{"location":"AWS/MachineLearning/#translate","title":"Translate","text":"<ul> <li>text translation service</li> <li>translates text from one language to another</li> <li>ensures meaning is translated</li> <li>autodetect source text language</li> <li>use case</li> <li>multi-language user experience</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/","title":"NoSQLandDynamoDB","text":""},{"location":"AWS/NoSQLandDynamoDB/#dynamodb","title":"DynamoDB","text":""},{"location":"AWS/NoSQLandDynamoDB/#architecture","title":"Architecture","text":"<ul> <li>nosql : <code>key/value &amp; document</code></li> <li><code>Public</code> database-as-a-service</li> <li><code>fully managed</code></li> <li>scaling options</li> <li><code>manual</code></li> <li><code>automatic</code></li> <li><code>on-demand</code><ul> <li>cost per interaction</li> </ul> </li> <li><code>capacity</code><ul> <li>write capacity unites (<code>WCU</code>)<ul> <li>1 WCU -&gt; 1KB per second</li> </ul> </li> <li>read capacity unites (<code>RCU</code>)<ul> <li>1RCU -&gt; 4KB per second</li> </ul> </li> </ul> </li> <li><code>highly resilient</code></li> <li>across <code>AZs</code></li> <li>optionally : <code>global</code></li> <li><code>Really fast</code>, single-digit milliseconds (SSD Based)</li> <li> <p>supports backups, point in time recovery, encryption at rest</p> </li> <li> <p><code>Tables</code></p> </li> <li>grouping of items with the same primary key</li> <li><code>primary key</code> :<ul> <li><code>simple</code> : partition key</li> <li><code>composite</code> : partition key + sort key</li> </ul> </li> <li>item max <code>400kb</code></li> <li> <p>item can have one, all, mixture, or different attributes</p> </li> <li> <p><code>Backups</code></p> </li> <li>on-demand : full copy of table retained until removed<ul> <li>migrate data</li> <li>restore with or without indexes</li> <li>adjust encryption settings</li> </ul> </li> <li> <p><code>point-in-time-recovery (PITR)</code></p> <ul> <li>continuous record of changes</li> <li>35 day recovery window</li> <li>1 second granularity</li> </ul> </li> <li> <p>considerations :</p> </li> <li>nosql or key/value : preference for ddb</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#operations-consistency-and-performance","title":"Operations, Consistency and Performance","text":"<ul> <li><code>Capacity Provisioning</code></li> <li>1RCU -&gt; 4KB per second</li> <li>1 WCU -&gt; 1KB per second<ul> <li>can only write to leader node, scale less, so more expensive than RCU</li> </ul> </li> <li>every tables has a RCU and WCU burst pool<ul> <li>300 seconds</li> </ul> </li> <li><code>On-Demand</code><ul> <li>for unknown or unpredictable load</li> <li>low admin</li> <li>cost:</li> <li>charged per million R/W units</li> </ul> </li> <li> <p><code>Provisioned</code></p> <ul> <li>set RCU and WCU on a per table basis</li> <li>every operations consumes at least 1 RCU/WCU</li> <li>cost : cheaper than on-demand</li> </ul> </li> <li> <p><code>Operations</code></p> </li> <li><code>Query</code><ul> <li>can return 0 or more items</li> <li>capacity consumed is the size of all returned items</li> <li>filter discards attribute data , but still consumes it</li> <li>can only query on PK or PK + SK</li> </ul> </li> <li> <p><code>Scan</code></p> <ul> <li>least efficient, more flexible</li> <li>scans through entire table, consumes capacity for every item</li> <li>can scan for any attribute besides PK/SK</li> </ul> </li> <li> <p><code>Consistency</code></p> </li> <li>do transactions started in the future necessarily see the effects of other transactions committed in the past</li> <li>ddb data is replicated across storage nodes in different AZs</li> <li>writes are directed at the leader node<ul> <li>then data is replicated to other nodes</li> </ul> </li> <li><code>eventually consistent read</code><ul> <li>reads direct to one of the storage node</li> <li>not guaranteed to get latest data</li> <li>scales better since can use any node</li> <li>lower price</li> </ul> </li> <li> <p><code>strongly consistent read</code></p> <ul> <li>reads directed to only the leader node</li> </ul> </li> <li> <p>Calculation</p> </li> <li>WCU Calculation<ul> <li>roundup(item size / 1KB) * (number per second)</li> </ul> </li> <li>RCU Calculation<ul> <li>roundup(item size / 4KB) * (number per second)</li> </ul> </li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#local-and-global-secondary-indexes","title":"Local and Global Secondary Indexes","text":"<ul> <li>a way to improve efficiency of data retrieval</li> <li>provide an alternative view on table data</li> <li>attribute propagation : all, keys_only, or include others</li> <li>projecting uses capacity</li> <li> <p>indexes are sparse : only items that have values in new PK/PK+SK are added</p> </li> <li> <p><code>LSI</code></p> </li> <li>use for <code>strong consistency</code></li> <li>must be created with a table</li> <li>5 LSI's per base tables</li> <li>alternative <code>SK</code> on the table</li> <li> <p><code>share the capacity</code> (RUC/WCU) with table</p> </li> <li> <p><code>GSI</code></p> </li> <li>use as <code>default</code></li> <li>can be created at any time</li> <li>default limit of 20 per base table</li> <li>alternative <code>PK</code> and <code>SK</code></li> <li><code>have their own capacity</code> (RCU/WCU) allocations</li> <li><code>eventually consistent</code>,asynchronous replication</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#streams-lambda-triggers","title":"Streams &amp; Lambda Triggers","text":"<ul> <li><code>stream</code></li> <li>time ordered list of item changes in a table<ul> <li><code>inserts, updates, and deletes</code></li> </ul> </li> <li><code>24 hour</code> rolling window</li> <li>enabled on per table basis</li> <li> <p><code>view types</code></p> <ul> <li>keys only</li> <li>new image</li> <li>old image</li> <li>new and old image</li> </ul> </li> <li> <p><code>Lambda Triggers</code></p> </li> <li>Lambda can be integrated to provide trigger functionality </li> <li>invoking when new entries are added on the stream.</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#global-tables","title":"Global Tables","text":"<ul> <li>provide <code>multi-master cross-region replication</code></li> <li>can re<code>ad and write to any region and replica</code></li> <li>generally <code>sub-second replication</code> between regions</li> <li><code>steps</code></li> <li>create tables in multiple regions</li> <li>then add them all to the same global table </li> <li><code>last writer wins</code> is used for conflict resolution</li> <li>globally eventually consistent</li> <li>strongly consistent reads only in the same region as writes</li> <li>provides <code>global HA</code>, Global DR/BC</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#accelerator-dax","title":"Accelerator (DAX)","text":"<ul> <li><code>in-memory cache</code> that improves performance</li> <li><code>faster</code></li> <li><code>can reduce costs</code></li> <li>application uses dax sdk and makes  a single call for data</li> <li>dax either returns data from cache or from database</li> <li><code>directly integrated with ddb</code></li> <li><code>less complexity</code> and admin overhead</li> <li>runs in <code>VPC</code></li> <li><code>deploy nodes across AZ for HA</code></li> <li>data replicated from primary node across replica nodes</li> <li>endpoint load balances across cluster nodes</li> <li><code>caches</code></li> <li><code>item cache</code><ul> <li>holds items from (Batch)GetItem call</li> </ul> </li> <li><code>query cache</code><ul> <li>holds data based on query/scan parameters</li> </ul> </li> <li>can scale <code>up</code> and <code>out</code></li> <li>supports <code>write-through</code></li> <li>can write to ddb using dax sdk</li> <li>eventually consistent</li> <li>good for read heavy operation</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#athena","title":"Athena","text":"<ul> <li>serverless query service</li> <li>ad-hoc query on data stored in s3</li> <li> <p>pay only for data consumed</p> </li> <li> <p>flow</p> </li> <li>data stored in s3<ul> <li>unstructured, semi-structured, or structured</li> </ul> </li> <li>define schema<ul> <li>define how to get original source data to a table structure</li> </ul> </li> <li>data is projected through the schema when read</li> <li>allows sql-like queries</li> <li>output can be sent to other services</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#elasticache","title":"ElastiCache","text":"<ul> <li>in-memory databases</li> <li>high performance</li> <li>not persistent</li> <li>engines</li> <li><code>Redis</code><ul> <li>advanced structures</li> <li>multi AZ</li> <li>back up and restore</li> <li>transactions : improve consistency</li> </ul> </li> <li><code>Memcached</code><ul> <li>simple data structures</li> <li>no replicated</li> <li>no backups</li> <li>multi-threaded</li> </ul> </li> <li><code>uses case</code></li> <li>used to cache data</li> <li>for <code>read heavy workloads</code> with low latency requirements</li> <li><code>reduces database workloads and costs</code></li> <li><code>store session data</code> for stateless servers</li> <li>requires application code changes</li> </ul>"},{"location":"AWS/NoSQLandDynamoDB/#redshift","title":"Redshift","text":"<ul> <li><code>petabyte-scale data warehouse</code></li> <li>design for reporting and analytics</li> <li><code>OLAP</code> (<code>column</code> based)</li> <li>pay as you use</li> <li>redshift spectrum : direct query s3</li> <li>federated query : direct query other DBs</li> <li>sql-like interface (<code>JDBC/ODBC</code>) connections</li> <li>server based (not serverless)</li> <li>takes time to provision, so no for adhoc queries like athena</li> <li><code>integration</code></li> <li>copy data from <code>dynamodb</code></li> <li><code>DMS</code> migrate from db</li> <li>stream using <code>kinesis firehose</code></li> <li><code>One AZ</code> in a <code>VPC</code> : <code>not HA</code></li> <li>data is replicated to 1 additional node</li> <li><code>automatic snapshot</code> to s3 (8hrs/5GB)<ul> <li>1-35 day retention</li> </ul> </li> <li><code>manual snapshots</code> to s3</li> <li><code>snapshots can be migrated across regions</code></li> <li>leader node : query input, planning and aggregation</li> <li>manages distributing data to node slices</li> <li>compute node : perform queries of data</li> <li><code>enhanced vpc routing</code></li> <li>advanced networking control</li> <li>by default public routes are used for external sources like S3</li> <li>allows traffic routing using vpc configuration</li> </ul>"},{"location":"AWS/Notes/","title":"Notes","text":""},{"location":"AWS/Notes/#rds","title":"RDS","text":"<ul> <li>can invoke a lambda function from an amazon <code>aurora</code> MySQL compatible edition db cluster with a <code>native function</code> or <code>stored procedure</code></li> <li>capture data changes</li> <li>rds events provide operation events such as</li> <li>db instance events, db parameter group events, db security group events, and db snapshot events</li> <li>aurora serverless is an on-demand, auto-scaling configuration for amazon aurora</li> <li>infrequent, intermittent, sporadic, or unpredictable workloads</li> <li>multi-az</li> <li>synchronous replication</li> <li>enhanced monitoring</li> <li>can authenticate to db using IAM database authentication</li> <li>MySQL and PostgreSQL</li> </ul>"},{"location":"AWS/Notes/#cloudfront","title":"CloudFront","text":"<ul> <li>cf <code>signed-url</code> and <code>signed-cookies</code> allow you to control who can access your content</li> <li>signed-urls<ul> <li>RTMP, restrict access to individual files, using a client that doesn't support cookies</li> </ul> </li> <li>signed cookies<ul> <li>provide access to multiple restricted files, don't wan to change urls</li> </ul> </li> </ul>"},{"location":"AWS/Notes/#organization","title":"Organization","text":"<ul> <li>account management service</li> </ul>"},{"location":"AWS/Notes/#resourceaccessmanager-ram","title":"ResourceAccessManager RAM","text":"<ul> <li>enables you to easily and securely share aws resources with any aws account or within your organization</li> </ul>"},{"location":"AWS/Notes/#asg","title":"ASG","text":"<ul> <li>default termination policy</li> <li>if multiple AZ, choose AZ with most instances</li> <li>oldest launch configuration instance in AZ</li> <li>closest to next billing hour</li> <li>random</li> </ul>"},{"location":"AWS/Notes/#ec2","title":"EC2","text":"<ul> <li><code>cloudwatch agent</code></li> <li>install on ec2</li> <li>memory, disk swap, disk space, page file utilization and log collection</li> <li>view custom metrics in the cloudwatch console</li> <li>communicate across subnets</li> <li>check network acl</li> <li>check security group</li> <li>increase network availability</li> <li>attach new eni to primary instance</li> <li>if primary instance fails, attach new eni to secondary instance</li> <li>billing</li> <li>charged when stopping to hibernate</li> <li>charged when reserved instance is terminated</li> <li>stop and start : underlying host might change</li> <li>vCPU-based On-Demand Instance limit per region</li> </ul>"},{"location":"AWS/Notes/#r53","title":"R53","text":"<ul> <li>can route to EC2 using R53</li> <li>active-active</li> <li>include all available resources</li> <li>when you want maximum availability</li> <li>active-passive</li> <li>primary, secondary resources</li> </ul>"},{"location":"AWS/Notes/#elasticache","title":"ElastiCache","text":"<ul> <li>authenticate users using redis auth</li> <li>create new redis cluster with both <code>transit-encryption-enabled</code> and  <code>--auth-token</code> parameters enabled</li> </ul>"},{"location":"AWS/Notes/#lambda","title":"Lambda","text":"<ul> <li>secure env variables</li> <li>create new kms key and use it to enable encryption helpers</li> </ul>"},{"location":"AWS/Notes/#s3","title":"S3","text":"<ul> <li>versioning</li> <li>prevent accidental deletes and overwriting</li> <li>object lock</li> <li>write once read many</li> <li>access point</li> <li>control who can access bucket</li> <li>multi-region access point<ul> <li>use global accelerator</li> </ul> </li> <li>Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required</li> <li>Provisioned capacity ensures that your retrieval capacity for expedited retrievals is available when you need it</li> </ul>"},{"location":"AWS/Notes/#apigw","title":"APIGW","text":"<ul> <li>use throttling limits to protect backend systems and applications from traffic spikes</li> <li>can cache frequently accessed data</li> </ul>"},{"location":"AWS/Notes/#ecs","title":"ECS","text":"<ul> <li>doesn't support resource based policy</li> </ul>"},{"location":"AWS/Notes/#iam","title":"IAM","text":"<ul> <li>can assign IAM role to users or groups from you AD once its integrated with your VPC via the aws directory service ad connector</li> <li>policy attached to roles/groups</li> </ul>"},{"location":"AWS/Notes/#lb","title":"LB","text":"<ul> <li>path-based routing, host-based routing, and bi-directional streaming using Remote Procedure Call (gRPC)</li> <li>configure ALB with gRPC as protocol</li> <li>SNI</li> <li>Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).</li> <li>host multiple TLS-secured applications, each with its own TLS certificate, behind a single load balancer.</li> </ul>"},{"location":"AWS/Notes/#config","title":"Config","text":"<ul> <li>track all configuration changes</li> </ul>"},{"location":"AWS/Notes/#dms","title":"DMS","text":"<ul> <li>migrate databases, warehouses, nosql</li> <li>can replicate on going changes (change data capture CDC)</li> <li>supports SSL for security</li> </ul>"},{"location":"AWS/Notes/#proton","title":"Proton","text":"<ul> <li>deploy serverless or container-bases applications with increased efficiency</li> <li>can define infrastructure standards and cd pipelines</li> <li>can use components dev can add supplementary resources to applications before the standard template</li> </ul>"},{"location":"AWS/Notes/#sns","title":"SNS","text":"<ul> <li>use over SES for monitoring</li> </ul>"},{"location":"AWS/Notes/#asg_1","title":"ASG","text":"<ul> <li>default cooldown is 300sec</li> </ul>"},{"location":"AWS/Notes/#guardduty","title":"GuardDuty","text":"<ul> <li>used to find suspicious access patterns</li> </ul>"},{"location":"AWS/Notes/#datasync","title":"DataSync","text":"<ul> <li>move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server</li> </ul>"},{"location":"AWS/Notes/#kms","title":"KMS","text":"<ul> <li>Unless the key policy explicitly allows it, you cannot use IAM policies to allow access to a KMS key</li> </ul>"},{"location":"AWS/Notes/#aws-backup","title":"AWS Backup","text":"<ul> <li>centralized backup service that makes it easy and cost-effective for you to backup your application data across AWS services</li> </ul>"},{"location":"AWS/Notes/#fargate","title":"Fargate","text":"<ul> <li>5GB ephemeral storage</li> </ul>"},{"location":"AWS/Notes/#vpc","title":"VPC","text":"<ul> <li>ACL outbound connection use ephemeral port</li> </ul>"},{"location":"AWS/RDS/","title":"Relation Database Service (RDS)","text":""},{"location":"AWS/RDS/#database-refresher-and-models","title":"Database Refresher and  Models","text":"<ul> <li><code>Relational</code></li> <li>use Structured Query Language (SQL)</li> <li><code>rigid schema</code> : structure in and between tables of data<ul> <li>defined in advance</li> <li>difficult to use for data that has rapidly changing relationships</li> </ul> </li> <li>table of rows, where each row has a unique primary key</li> <li> <p>relationship between tables</p> <ul> <li><code>composite key</code> : links to two tables which indirectly connects them</li> </ul> </li> <li> <p><code>NoSQL</code></p> </li> <li>not one single thing, many models</li> <li>generally more relaxed schema</li> <li><code>Key-Value</code><ul> <li>list of key value pairs</li> <li>no real schema or tables</li> <li>scalable since sections of the list can be split to different servers</li> <li>really fast</li> <li>in memory caching</li> </ul> </li> <li><code>Wide Column Store (DynamoDB)</code><ul> <li>two keys, partition and other key</li> <li>can have multiple attributes, different items don't need to have the same attributes</li> </ul> </li> <li><code>Document</code><ul> <li>key : value, where the values are documents</li> <li>documents usually json or xml format</li> <li>ideal for interacting with whole documents or deep attribute interactions</li> </ul> </li> <li><code>Column (Redshift)</code><ul> <li>store data based on columns</li> <li>really good at querying single columns for reporting and analytics</li> </ul> </li> <li><code>Graph</code><ul> <li>nodes and edges representing relationships</li> <li>both can have key value pairs of data</li> </ul> </li> </ul>"},{"location":"AWS/RDS/#acid-v-base","title":"ACID v BASE","text":"<ul> <li>database transaction models</li> <li><code>cap theorem: (choose 2)</code></li> <li><code>consistency</code> : every read to a database will get the most recent write or get an error</li> <li><code>availability</code> : every request will receive a non-error response, without the guarantee that you get the most recent write</li> <li><code>partition tolerant</code> : system can be made of multiple network partitions</li> <li><code>acid : consistency</code></li> <li>transactions are : <ul> <li><code>atomic</code> : all or no components of a transaction succeeds or fails ex) banking transfers</li> <li><code>consistent</code> : transaction move database from one valid state to another, no in-between</li> <li><code>isolated</code> : concurrent transactions don't interfere with each other</li> <li><code>durable</code> : once committed, transactions are durable, stored on non-volatile memory</li> </ul> </li> <li><code>RDS</code></li> <li><code>limits scaling</code></li> <li><code>base : availability</code></li> <li>transactions are : <ul> <li><code>basically available</code> : read and write operations are available as much as possible without consistency guarantees</li> <li><code>soft state</code> : database doesn't enforce consistency, offloaded to application/user</li> <li><code>eventually</code> : if we wait long enough, reads from the system will be consistent</li> </ul> </li> <li><code>highly scalable</code></li> <li><code>nosql</code></li> </ul>"},{"location":"AWS/RDS/#databases-on-ec2","title":"Databases on EC2","text":"<ul> <li><code>usually a bad idea</code></li> <li><code>admin overhead</code> : managing ec2 and dbhost</li> <li>backup / DR management / replication</li> <li>ec2 runs in a single AZ</li> <li>missing features aws db products have, performance</li> <li> <p>on or off, not serverless, no easy scaling</p> </li> <li> <p><code>putting database in an ec2 introduces dependency</code></p> </li> <li>need reliable communication between application and database ec2</li> <li>cost for data transferring across different AZ</li> </ul> <p><code>justifications</code> - access to the db instance OS - advanced db option tuning - db or db version aws doesn't support - specific os/db combination aws doesn't provide - architecture aws doesn't provide</p>"},{"location":"AWS/RDS/#relational-database-service-rds","title":"Relational Database Service (RDS)","text":"<ul> <li>Database-as-a-service (DBaas)</li> <li>managed database instance (1 or more databases)</li> <li>multiple engines MySQL, MAriaDB, PostgreSQL, Oracle, Microsoft SQL Server</li> <li>Amazon Aurora</li> </ul> <p>Architecture - RDS Database Instance   - use same tooling as engine to access   - has one attached block storage in the same AZ as the instance     - io1, gp2, magetic   - billed for the instance (cpu/memory) and for the storage GB/month   - access to to rds instance is controlled by its associated security group</p> <ul> <li>create a subnet group</li> <li>inform which subnets to place rds instances into</li> </ul>"},{"location":"AWS/RDS/#rds-multi-az","title":"RDS Multi AZ","text":"<ul> <li>secondary hardware is allocated in another AZ (standby replica)</li> <li>access database using cname, which points to primary rds</li> <li><code>Synchronous Replication</code></li> <li>database writes directed at primary database cname<ul> <li>which writes changes to it's storage</li> </ul> </li> <li>changes immediately replicated across to standby replica<ul> <li>standby writes changes to it's storage</li> </ul> </li> <li>almost zero lag, as replication occurs as data is being written to primary database</li> <li>if error occurs with primary database</li> <li> <p>rds changes the database endpoint cname to the standby replica</p> </li> <li> <p>gives <code>high availability</code>, but not fault tolerance</p> </li> <li>not available for free tier</li> <li>standby can't be directly used, doesn't increase performance</li> <li><code>60-120</code> seconds failover</li> <li><code>same region</code> only, work with other AZs in the VPC</li> <li><code>backups are taken from standby</code>, (removes performance impact)</li> <li>failure reasons : az outage, primary failure, manual failure, instance type change, software patching</li> </ul>"},{"location":"AWS/RDS/#rds-backups-and-restores","title":"RDS Backups and Restores","text":"<p><code>Recovery Point Objective (RPO)</code> - time between last backup and the point a failure occurred   - maximum data loss possible - influences technical solution and cost :    - lower rpo -&gt; HIGHER COST</p> <p><code>Recovery Time Objective (RTO)</code> - time between a failure and when system is return to service - influenced by process, staff, tech, and documentation - generally lower values cost more</p> <p>RDS is capable of performing Manual Snapshots and Automatic backups - both use aws managed s3 buckets that aren't visible to user - gives region resilience - first snapshot is full, then onwards incremental - can affect performance of single instance, but if using multi-az used on standby - <code>Manual Snapshots</code> : live forever - <code>Automatic Backups</code> : configure how often it occurs   - <code>every five minutes, database transactions logs are written to s3</code>     - the actual data that changes in the database     - database can be restored back in time within a 5 minute granularity       - <code>RPO = 5 minutes</code>     - database snapshot is restored, then the transaction logs are replayed over the top of the snapshot   - are not retained indefinitely, automatically cleaned up     - 0-35 day retention period   - can retain automated backups after deleting rds instance, but they will still expire     - must take manual snapshot</p> <p><code>RDS Restores</code> - creates a <code>new rds instance with new address</code>   - will have to update application that use the database endpoint address - restoring using snapshots restores to single point in time</p>"},{"location":"AWS/RDS/#rds-read-replicas","title":"RDS READ-Replicas","text":"<ul> <li>RDS Read Replicas can be added to an RDS Instance - <code>5 direct per primary instance</code>.</li> <li>each providing an additional instance of read performance<ul> <li>ie) make applications used replicas for read operations</li> </ul> </li> <li>offers <code>low RTO recovery</code> for any instance failure issues</li> <li>replicas can be promoted to a primary quickly</li> <li>don't help with data corruption as the corruption will be replicated to the RR.</li> <li>like another RDS instance, has its <code>own address</code></li> <li>asynchronous replication form primary to read replica</li> <li>can be in the <code>same region, or cross-region replicas</code>.</li> <li><code>global</code> performance improvements</li> <li>read replicas can have read replicas, but lag start to be a problem</li> </ul>"},{"location":"AWS/RDS/#rds-data-security","title":"RDS Data Security","text":"<ul> <li>authentication : how users can log in to rds</li> <li>can configure RDS to allow iam authentication<ul> <li>create RDS <code>local DB account</code> configured to use <code>aws authentication token</code></li> <li>policy attached to users or roles maps that iam identity onto the local RDS user</li> <li><code>generate-db-auth-token</code> for <code>15 mins</code></li> </ul> </li> <li>authorization : how access is controlled</li> <li>encryption in transit : between client and rds</li> <li>SSL/TLS, can be <code>mandatory</code></li> <li>encryption at rest : how data is protect when it is written to disk</li> <li>default : EBS volume encryption with KMS<ul> <li><code>handled by RDS HOST/EBS</code></li> <li>CMK generates data keys used for encryption operations</li> <li><code>storage, logs, snapshots, and replicas are encrypted</code></li> <li><code>can't be removed</code> once added</li> </ul> </li> <li>MSSQL and ORACLE support Transparent Data Encryption (<code>TDE</code>)<ul> <li>encryption handled within the DB engine, <code>ie encrypted before leaving the instance</code></li> <li>ORACLE supports integration with CloudHSM</li> <li>stronger key controls</li> </ul> </li> </ul>"},{"location":"AWS/RDS/#aurora-architecture","title":"Aurora Architecture","text":"<ul> <li>uses a <code>cluster</code></li> <li>made of a single <code>primary</code> instance and <code>0 or more replicas</code></li> <li>replicas can be used for reads during normal operation<ul> <li>provide availability and read scalability</li> </ul> </li> <li>no local storage : uses <code>cluster volume</code></li> <li>shared and available to all compute instances within a cluster <ul> <li>faster provision, improved availability and performance</li> </ul> </li> <li>data written to primary disk, <code>aurora synchronously replicates</code> the data across 6 replicas in 3 AZs</li> <li><code>aurora automatically detects failures</code> in disk volumes of shared storage<ul> <li>when segment fails, aurora <code>immediately repairs using data inside other storage nodes</code></li> <li>avoids data loss and reduces any need for snapshot restores</li> </ul> </li> <li><code>Up to 15 replicas</code>, that can be assigned to failover operations<ul> <li>instances can be added without requiring storage provisioning</li> </ul> </li> <li><code>All SSD Based - high IOPS, low latency</code></li> <li>Don't specify storage, <code>billed on what's used</code>, for the most used<ul> <li>create new cluster and migrate data to reduce costs</li> </ul> </li> <li><code>Cluster Endpoint</code></li> <li>points at primary instance</li> <li>can be used for read and write operations</li> <li><code>Reader Endpoint</code></li> <li>load balances across all available replicas</li> <li>used for read operations, easier reader scaling</li> <li><code>Custom Endpoint</code></li> <li>can create custom endpoints</li> <li><code>individual endpoints</code></li> <li>each instance (primary or replica) has its own endpoint</li> <li><code>Costs</code></li> <li>no free-tier option<ul> <li>doesn't support micro instances</li> </ul> </li> <li>beyond rds single az, aurora offers better value</li> <li>compute : hourly charge per second, 10 minute min</li> <li>storage : GB-Month consumed, IO costs per request</li> <li>100% DB Size in backups are included</li> <li><code>Backups</code></li> <li>work in aurora in the same ay as RDS</li> <li>restores create a new cluster</li> <li><code>backtrack</code> can be used which allow <code>in-place rewinds</code> to previous point in time</li> <li><code>fast clone</code> : makes a new databases <code>MUCH</code> faster than copying all the data<ul> <li>only stores differences of data that changed in the clone or the original</li> </ul> </li> </ul>"},{"location":"AWS/RDS/#aurora-serverless","title":"Aurora Serverless","text":"<ul> <li>removes the admin overhead of managing individual instances</li> <li>scalable <code>Aurora Capacity UnitsACU</code></li> <li>allocated from a shared pool managed by aws</li> <li>represent an amount of compute and memory</li> <li>can set <code>min and max</code> acu values</li> <li>can go to <code>0 and be paused</code></li> <li>billed per-second basis</li> <li>same resilience as aurora, 6 replicas across AZs</li> <li>requests go to a hidden layer <code>proxy fleet</code> managed by aws</li> <li>which broker connection with acu</li> <li>allows for seamless scaling</li> <li><code>Use cases</code></li> <li>infrequently used application</li> <li>new applications</li> <li>variable workloads</li> <li>unpredictable workloads</li> <li>development and test databases</li> <li>multi-tenant applications  (scaling is aligned between infrastructure size and revenue )</li> </ul>"},{"location":"AWS/RDS/#aurora-global-database","title":"Aurora Global Database","text":"<ul> <li>Create global level replication using aurora from a main region to up to <code>5 secondary regions</code></li> <li>entire secondary cluster is read only</li> <li>can have up to <code>16 replicas</code>, which can be promoted to R/W</li> <li>replication occurs at the storage layer</li> <li><code>1 second</code> replication from main to all regions</li> <li>no impact on db performance</li> <li>Use Case:</li> <li><code>cross-region</code> disaster recovery and Business continuity</li> <li><code>global read scaling</code> : low latency performance improvements</li> </ul>"},{"location":"AWS/RDS/#aurora-multi-master","title":"Aurora Multi-Master","text":"<ul> <li>default aurora mode has one <code>r/w</code> and <code>0+</code> replicas</li> <li><code>not fault tolerant</code></li> <li>failover takes time as a replica is promoted</li> <li>in <code>multi-master</code> mode all instances are <code>r/w</code></li> <li>no cluster endpoint to use</li> <li><code>no load balancing</code>, must be handled by application</li> <li><code>application can connect with the instances directly</code></li> <li>when writer gets a request, it tries to get others to agree to add the data to all data nodes</li> <li>changes is replicated to other nodes in memory cache<ul> <li>so their reads are consistent with newly updated data</li> </ul> </li> <li>application can send request to another instance if a request fails<ul> <li>foundation of being able to build a <code>fault tolerant</code> application</li> <li>faster availability</li> </ul> </li> </ul>"},{"location":"AWS/RDS/#database-migration-service-dms","title":"Database Migration Service (DMS)","text":"<ul> <li>no downtime</li> <li>runs using a replication instance</li> <li>an ec2 instance with migration software that can communicate with the DMS service</li> <li>replication tasks : define the options relating to the migration</li> <li><code>source and destination endpoints</code> pointing at source and target databases</li> <li>one endpoint must be aws</li> <li><code>full load</code></li> <li>migrating existing all data</li> <li><code>full load + Change Data Capture (CDC)</code></li> <li>migrates existing data and replicates any on going changes occurring on the source</li> <li><code>CDC Only</code></li> <li>if you want to use an alternative method to transfer the bulk of the data</li> <li>migrate only the changes</li> <li>Schema Conversion Tool (<code>SCT</code>)</li> <li>can assist with schema conversion</li> <li>used when converting one database engine to another</li> <li>not used when migrating between compatible engines</li> <li>works with OLTP and OLAP databases</li> <li><code>DMS &amp; Snowball</code></li> <li>larger migrations in multi TB size</li> <li>use schema conversion tool to extract data locally and move to a snowball device</li> <li>ship the device to aws, the load it onto aan s3 bucket</li> <li>DMS migrates from s3 into the target store</li> <li>CDC can capture changes and which can be written to target database via s3</li> </ul>"},{"location":"AWS/Route53/","title":"Route53","text":"<ul> <li>global service</li> <li>globally resilient (replicated between regions)</li> </ul> <p>Services</p> <ol> <li>Register Domains</li> </ol> <pre><code>    sequenceDiagram\n        Route53 -&gt;&gt; top level domain register : checks if domain is available\n        Route53 --&gt;&gt; nameservers : creates zonefile and stores it in namer servers\n        Route53 --&gt; specific domain registery ex .org : adds nameservers records into the zonefile to specific domain registery</code></pre> <ol> <li>Host Zonefiles on managed nameservers</li> </ol>"},{"location":"AWS/Route53/#r53-public-hosted-zones","title":"R53 Public Hosted Zones","text":"<ul> <li><code>Hosted Zone</code> : a DNS DataBase (zone file) for a domain</li> <li>what the DNS System references </li> <li>created with domain registration iva R53 or separately</li> <li>accessible from the public internet and VPCs</li> <li>Hosted on <code>4</code> R53 name servers specific for the zone</li> <li>use ns records to point at these ns</li> <li>resource records (<code>RR</code>) created within the hosted zone (items of data that dns uses)</li> <li>externally registered domains can point at R53 public zone</li> </ul>"},{"location":"AWS/Route53/#r53-private-hosted-zone","title":"R53 Private Hosted Zone","text":"<ul> <li><code>associated with VPC's</code>, and only accessible in those VPCs</li> <li>using different account using cli/api</li> <li><code>split-view</code> : overlapping <code>public</code> and <code>private</code> </li> <li>for <code>public</code> and <code>internal</code> use with the same zone name</li> </ul>"},{"location":"AWS/Route53/#cname-vs-r53-alias","title":"CNAME vs R53 Alias","text":"<ul> <li><code>cname</code> maps a name to another name</li> <li>ex) www.cat.op =&gt; cat.io</li> <li>invalid for naked/apex (cat.io)</li> <li>many aws services use a dns name (ELBs)</li> <li>with cname, cat =&gt; elb would be invalid</li> <li><code>alias</code> : map a name to an aws resource</li> <li>can be used for naked/apex and normal records</li> <li>for non apex/record functions like cname</li> <li>no charge for alias requests pointing at aws resources</li> <li>multiple types</li> <li>should be the same type as what the record is pointing at</li> </ul>"},{"location":"AWS/Route53/#simple-routing","title":"Simple Routing","text":"<ul> <li><code>1</code> record per name (www)</li> <li>each record can have <code>multiple values</code></li> <li>all values are returned to client, then client chooses one</li> <li><code>use case</code> : when you want to route requests towards one service such as a web server</li> <li><code>doesn't support health checks</code></li> </ul>"},{"location":"AWS/Route53/#health-checks","title":"Health Checks","text":"<ul> <li>separate from, but are used by records</li> <li><code>health checkers</code> located globally</li> <li>health checks check every 30s (every 10s costs extra)</li> <li>TCP, HTTP/HTTPS,  HTTP/HTTPS with string matching</li> <li>where a health checker makes a TCP connection and expects an response within a time limit</li> <li>states : <code>healthy</code> or <code>unhealthy</code></li> <li>type:</li> <li><code>endpoint</code> : assess the health of an endpoint that you specify</li> <li><code>cloudwatch</code> : react to cloudwatch alarm</li> <li><code>checks of checks (calculated)</code> : includes multiple checks</li> </ul>"},{"location":"AWS/Route53/#failover-routing","title":"Failover Routing","text":"<ul> <li>can add multiple records of the same name</li> <li>a <code>primary</code> and <code>secondary</code></li> <li>if the primary record is healthy</li> <li>any queries return the primary record</li> <li>if the primary record is not healthy</li> <li>any queries return the secondary record</li> <li><code>use case</code> : when you want to configure active passive failover</li> <li>route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy</li> </ul>"},{"location":"AWS/Route53/#multi-value-routing","title":"Multi Value Routing","text":"<ul> <li>like a mixture of simple and failover routing</li> <li>can create many records with the same name</li> <li>each mapping to an ip address</li> <li>each can have an associated health check</li> <li>when queried up to 8 healthy records are returned<ul> <li>if you have more than 8, then 8 are selected random</li> </ul> </li> <li>client picks which to use</li> <li>improves the availability, but not a replacement for load balancing</li> </ul>"},{"location":"AWS/Route53/#weighted-routing","title":"Weighted Routing","text":"<ul> <li>simple form of load balancing or testing new software versions</li> <li>can create multiple records with the same name</li> <li>each record has a record weight associated with it</li> <li>each record is returned based on the ratio of its weight to the total record weights</li> <li>can be associated with a health check</li> <li>if a chosen record is unhealthy, then the process of selection is repeated until  a healthy one is chosen</li> </ul>"},{"location":"AWS/Route53/#latency-based-routing","title":"Latency-Based Routing","text":"<ul> <li>used when optimizing for performance and user experience</li> <li>can create multiple records with the same name</li> <li>each record can have a record region associated with it</li> <li>latency based routing supports one record with the same name in each aws region</li> <li>aws maintains a database of latency between the users general location and the regions tagged in records</li> <li>the record returned is the one which offers the lowest estimated latency and is healthy</li> <li>if a record is unhealthy, then the next lowest latency is returned</li> </ul>"},{"location":"AWS/Route53/#geolocation-routing","title":"Geolocation Routing","text":"<ul> <li>records are tagged with location</li> <li>US state, country, continent, default</li> <li>R53 checks for records from the smallest to largest scope with default being the last check</li> <li>not about the closest record, about controlling for relevant locations</li> <li>use : restrict content based on users location</li> </ul>"},{"location":"AWS/Route53/#geoproximity-routing","title":"Geoproximity Routing","text":"<ul> <li>records can be tagged with an aws region or lat &amp; long coordinates</li> <li>routing is distance based, but also includes am optional bias</li> <li>a bias expands or shrinks the size of the geographic region in the decision making process</li> </ul>"},{"location":"AWS/Route53/#interoperability","title":"Interoperability","text":"<ul> <li>R53 has two roles</li> <li><code>registrar</code> role and <code>domain hosting</code> role</li> <li>R53 (doing both)</li> <li>allocates 4 name servers (domain hosting)</li> <li>creates a zone file (domain hosting) on the ns'</li> <li>communicated with the registry TLD (Domain Registrar)<ul> <li>sets the ns records for the domain to point to the 4 ns'</li> </ul> </li> <li>R53 doing only registrar (not common)</li> <li>ns are hosted somewhere else, but their information is passed to R53</li> <li>R53 for hosting only</li> <li>incase a domain has been registered with another service earlier</li> </ul>"},{"location":"AWS/S3/","title":"Simple Storage Service S3","text":"<ul> <li>Regionally resilient</li> <li>bucket name + key (filename)</li> <li>zero bytes to 5TB per object</li> <li>versionId, MetaData</li> <li>Access Control</li> <li>Subresources</li> <li>great for large scale data storage, distribution or upload</li> <li>can be used as input or output to many aws products not a file or block storage</li> </ul>"},{"location":"AWS/S3/#bucket","title":"Bucket","text":"<ul> <li>containers for data, created within a region</li> <li>name is globally unique</li> <li>unlimited objects</li> <li>flat structure, no true folders, folders are prefixes</li> <li>100 soft limit, 1000 hard limit</li> </ul>"},{"location":"AWS/S3/#object","title":"Object","text":"<ul> <li>key : value</li> </ul>"},{"location":"AWS/S3/#s3-security","title":"S3 Security","text":"<p>Private by default. Only root account can access it, and everything else must be explicitly given permission.</p>"},{"location":"AWS/S3/#bucket-policy","title":"Bucket Policy","text":"<p>A type of resource policy.</p> <ul> <li>allow access from the same or different account</li> <li>policy is attached to resource, which can then reference any identity either within or outside the account</li> <li>allow or deny anonymous principals</li> <li>by reference all principals</li> <li>bucket can only have one policy, but that policy can have multiple statements</li> </ul> <p><code>black public access</code> - option to override any policy or ACL that allow public access - applies on public access, and not on aws identities (anon principals) - disabling this doesn't give access permissions, instead it grants the ability to grant permission</p>"},{"location":"AWS/S3/#static-website-hosting","title":"Static Website Hosting","text":"<p>Allow access via HTTP.</p> <p>must set: - <code>index</code> : default page, or entry point to website - <code>Error</code> : page shown when there is an error</p> <p>Website url will be automatically generated from region and bucket name. If you want to use your own custom name, then the bucket name must match the domain.</p> <p>Use cases: 1. Host blogs 2. Offloading     - offload static media to s3, since it is cheaper than compute     - then html that is sent to clients browser can point to the buckets for the static media 3. Out-of-band pages     - if compute service is down, point customers to static website to give information</p>"},{"location":"AWS/S3/#object-versioning-and-mfa-delete","title":"Object Versioning and MFA Delete","text":""},{"location":"AWS/S3/#object-versioning","title":"Object Versioning","text":"<p>Lets you store multiple versions of objects within a bucket.</p> <ul> <li>Operations which would modify objects generate a new version.</li> <li>versions are identified by an <code>id</code></li> <li>when accessing an object, if an id is not specified, then the latest version of the object is used</li> <li>controlled at bucket level</li> <li><code>disabled</code> by default, once enabled it cannot be disabled again</li> <li>can be <code>suspended</code>, then re-enabled</li> <li>billed for storage of all versions of objects</li> <li>when deleting a versioned object without specifying an id, a new latest version called a <code>delete marker</code> will be created</li> <li>this hides the other versions and makes it looks like the object is deleted</li> <li>the delete marker can be deleted, thereby making the other versions visible again (un-delete)</li> <li>when deleting an object with specifying an id</li> <li>the object of that version will be really deleted</li> <li>objects versions will be moved up</li> </ul>"},{"location":"AWS/S3/#mfa-delete","title":"MFA Delete","text":"<ul> <li>Enabled in versioning configuration</li> <li>mfa is required to change bucket versioning state (enabled, suspended )</li> <li>mfa is required to delete versions</li> </ul>"},{"location":"AWS/S3/#performance","title":"Performance","text":"<p>Single PUT Upload - default way objects are uploaded to s3 - single data stream to s3 - if a stream fails, upload fails, requires full restart - limited to 5GB of data</p> <p>Multipart Upload - data is broken up,  - minimum size is 100MB - 10,000 max parts of size 5MB - 5GB   - last part can be smaller than 5MB - parts can fail and be restarted in isolation - improves transfer rate, by avoiding single stream inefficiencies or limitations</p> <p>Accelerated Transfer - uses edge locations to use aws global network which is purpose built to connect regions to each other - improve the reliability and speed of transferring data across regions over using the public network - bucket can't have period</p>"},{"location":"AWS/S3/#object-encryption","title":"Object Encryption","text":"<ul> <li>buckets aren't encrypted, objects are</li> <li>encryption is defined at object level, with a possibility of different objects using different methods</li> </ul>"},{"location":"AWS/S3/#at-rest","title":"At rest","text":"<p>Client-Side Encryption : object is encrypted on client side, before it gets uploaded to s3 - client is responsible for managing keys and encryption process</p> <p>Server-Side Encryption : object reaches s3 in plaintext, and then gets encrypted by s3 - Server-Side Encryption with Customer-Provided Keys (SSE-C)   - most control   - Customer manages keys, s3 manages encryption   - provide s3 they key along with object being upload   - object is stored with hash of key supplied   - hash verifies the same key is being used during decryption   - key is never stored in s3 - Server-Side Encryption with Amazon s3-Managed Keys (SSE-S3)   - default   - s3 managed both encryption and keys (AES 256)   - s3 key creates a unique key for every object uploaded   - a master key encrypts the unique key which is then stored with the encrypted data - Server-Side Encryption with Customer Master Keys (CMK's) stored in AWS Key Management Service (SSE-KMS)   - kms manages the keys, CMK creates DEK to encrypt object uploaded   - gives fine grain control over key being used   - allows role separation by a limiting permissions to CMK used to encrypt objects   - allow control to key rotation</p> <p>note in both cases the data is encrypted in transit as https is used to upload</p> <p>Bucket Default Encryption - applied when encryption is not specified at object level</p>"},{"location":"AWS/S3/#storage-classes","title":"Storage Classes","text":"<p>S3 Standard - default - replicated on at least 3 AZ's - charged GB/month fee for storage, $ per GB for transfer out, price per 1000 requests - no retrieval fee - no minimum duration - no minimum size - used for frequently accessed data</p> <p>S3 Standard-IA (Infrequent Access) - similar architecture to S3 Standard - cheaper than S3 Standard to store data - per GB retrieval fee, cost increases with data increases - minimum billing duration of 30 days - minimum capacity billed at 128KB - used for long-lived data, that is important but infrequently accessed</p> <p>S3 One Zone-IA - cheaper than S3 Standard and S3 Standard-IA - still has retrieval fee, minimum duration and capacity billed - stored in one AZ, cheaper storage by more risky - used for long-lived data, which is non-critical or replaceable, where access is infrequent</p> <p>S3 Glacier - instant - like S3 Standard-IA but cheaper storage, more expensive retrieval, longer minimum - minimum billing duration of 90 days - still have instant access to objects</p> <p>S3 Glacier - Flexible - objects are retrieved to S3 Standard - IA temporarily   - expedited (1-5 mins), standard (3-5) hours, bulk (5-12 hours)   - faster retrieval results in more expensive costs - objects cannot be made publicly accessible - 90 days, 40KB minimums - archive data where frequent or fast access is not needed</p> <p>S3 Glacier Deep Archive - 180 days, 40KB minimums - standard (12 hours), bulk (48 hours) - archival data that is rarely if ever accessed</p> <p>S3 Intelligent Tiering - frequent access, infrequent access, archive instant access, archive access, deep archive - s4 standard, S3 Standard-IA, S3 Glacier - instant, S3 Glacier - Flexible, S3 Glacier Deep Archive - monitors usage of objects and automatically moves objects to appropriate tiers - monitoring and automation cost per 1000 objects - used for long lived data where usage is changing or unknown</p>"},{"location":"AWS/S3/#lifecycle-configuration","title":"Lifecycle Configuration","text":"<p>Automate deletion of object (or object versions) or change storage classes.</p> <ul> <li>a lifecycle configuration is a set of rules</li> <li>consisting of actions on a bucket or group of objects</li> <li>transition actions : change the storage class of object</li> <li>expiration actions : can delete object or object version</li> <li>objects can transition down a waterfall, never up</li> <li>Standard -&gt; Standard IA -&gt; Intelligent-Tiering -&gt; One Zone-IA -&gt; Clacier IR -&gt; Glacie FR -&gt; Glacier DA</li> <li>minimum of 30 days in standard before transition, if it starts in standard</li> <li>a single rule cannot transition to standard IA or One Zone IA and then to glacier within 30 days</li> <li>use two rules to get around 30 day limit</li> </ul>"},{"location":"AWS/S3/#replication","title":"Replication","text":"<p>Cross-Region Replication (CRR) - source bucket replicated to destination bucket in a different region'</p> <p>Same-Region REplication (SRR) - source and destination buckets in the same region</p> <p>Replication Configuration - bucket to use - iam role s3 will assume, to be able to read and replicate data in object - if destination is in a different account, add a bucket policy that trusts the role used by source</p> <p>Options - all object or subset - storage class objects in destination will use, (default same as source class) - ownership - replication time control (RTC), for quicker (15min) replication</p> <p>Considerations - not retroactive - versioning needs to be on - one way replication : source -&gt; destination - can handle un-encrypted, SSE-S3, and SSE-KMS - source bucket owner needs permission to objects - no system events, glacier, or glacier deep archive - delete aren't replicated</p> <p>Use case - SRR    - Log Aggregation   - sync different account (Prod Test)   - resilience and strict sovereignty - CRR   - global resilience improvements   - latency reduction</p>"},{"location":"AWS/S3/#presigned-urls","title":"Presigned URLs","text":"<ul> <li>temporary url that gives access to S3 objects</li> <li>created by identity that has access to S3</li> <li>used to give unauthenticated identities access to S3, while keeping the bucket private</li> <li>download (GEt) or upload (PUT) supported</li> </ul> <p>Considerations - when using url, you are assuming the same permission as the identity that generated the url   - identity can create url to object he doesn't have access to, but the url will result in an access denied page   - if permissions change after the url is created, the permissions of the url will also change - don't generate with a role   - since roles can expire faster than the url, the url can stop working abruptly</p>"},{"location":"AWS/S3/#select-and-glacier-select","title":"Select and Glacier Select","text":"<p>Allow to access to partial access using SQL-Like statements.</p> <ul> <li>reduce billing and increase speed by filtering before data is streamed out of s3</li> </ul>"},{"location":"AWS/S3/#events","title":"Events","text":"<ul> <li>when enabled, a notification is generated when events occur in a bucket</li> <li>can be used to deliver to SNS, SQS, and lambda functions as part of serverless application</li> <li>add resource policies allowing s4 principal access</li> </ul> <p>events - object created - object deletion - object restored from glacier - object replication statistics</p>"},{"location":"AWS/S3/#access-logs","title":"Access Logs","text":"<ul> <li>enabling logging on source bucket</li> <li>destination bucket acl allow the source s3 log delivery group</li> <li>detailed information about requests made on source bucket</li> <li>for auditing or research access patterns of customers</li> </ul>"},{"location":"AWS/S3/#requester-pays","title":"Requester Pays","text":"<ul> <li>requester pays the cost of transfer out requests</li> <li><code>bucket configuration</code> (cannot be set per object)</li> <li>doesn't work with static website hosting or bitTorrent</li> <li>requires authentication to allow access</li> <li>requesters must add <code>x-amz-request-payer</code> in requests to confirm payment responsibility</li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/","title":"SECURITY, DEPLOYMENT &amp; OPERATIONS","text":""},{"location":"AWS/SecurityDeploymentOperations/#secrets-manager","title":"Secrets Manager","text":"<ul> <li><code>designed for secrets</code></li> <li>password, API keys</li> <li>usable via console, cli, api, sdk</li> <li><code>supports automatic rotation</code></li> <li>using lambda</li> <li>directly integrates with some aws products</li> <li>RDS</li> <li><code>encrypted at rest</code>  with KMS</li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#waf-shield","title":"WAF &amp; Shield","text":""},{"location":"AWS/SecurityDeploymentOperations/#shield","title":"Shield","text":"<ul> <li><code>provides aws resources with DDoS protection</code></li> <li> <p>layer 3 and layer 4 DDoS protection</p> <ul> <li>packets, segments, ips, ports, sessions</li> </ul> </li> <li> <p><code>Standard</code></p> </li> <li>free with R53 and cloudfront</li> <li><code>Advanced</code></li> <li>$3000p/m</li> <li>EC2, ELB, CloudFront, Global Accelerator, R53</li> <li>DDoS Response team &amp; financial insurance for attacks</li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#web-application-firewall-waf","title":"Web Application Firewall (WAF)","text":"<ul> <li><code>layer 7 (HTTP/s) firewall</code></li> <li>can identify normal and abnormal requests : protocol specific attacks</li> <li>can see unencrypted HTTP plaintext data</li> <li>inspect and block, replace, or tag data at L7</li> <li>identify, block, adjust specific applications<ul> <li>facebook</li> </ul> </li> <li>SQL injections, cross-site scripting, geo blacks, rate awareness</li> <li><code>Web Access Control Lists (WEBACL)</code></li> <li>integrated with ALB, API Gateway, CloudFront</li> <li>default action : allow or blockt</li> <li><code>waf rules</code><ul> <li>added to WEBACL, and are evaluated when traffic arrives</li> <li>Type : how it works</li> <li>Regular : if something occurs</li> <li>Rate-Based</li> <li>Statement : whats matched or count</li> <li>criteria : origin country, IP, headers, cookies, http method, url path, query string, body (first <code>8192</code> bytes)</li> <li>single, AND, OR, NOT</li> <li>Action : What it does</li> <li>Allow, Block, Count, Captcha</li> <li>custom response<ul> <li>block : custom response/header</li> <li>allow/counts/captcha : custom header only</li> <li>customer-header</li> <li>x-amzn-waf</li> <li>means application itself can react to traffic that has been matched</li> </ul> </li> <li><code>labels</code><ul> <li>internal to waf</li> <li>can be referenced later in same WEBACL</li> <li>multi-stage flows</li> <li>used with count/captcha </li> <li>allow/block stop processing</li> </ul> </li> </ul> </li> <li><code>rule groups</code><ul> <li>contain rules</li> <li>don't have default actions</li> <li>managed by aws or marketplace, you, service (Shield &amp; Firewall Manager)</li> <li>can be referenced by multiple WEBACLs</li> </ul> </li> <li>resource type : cloudfront or regional service</li> <li>WEBACL Capacity Units (WCU) <ul> <li>indication of complexity of rules</li> <li>default max 1500</li> </ul> </li> <li>adjusting WEBACL takes less time than associating one</li> <li>resource can have on WEBACL which can be associated with many resources</li> <li>can use eventbridge scheduled rules + lambda ip list parser to block bad actors</li> <li><code>logs</code></li> <li>can be directed to S3 (5min), cloudwatch logs, or kinesis firehose</li> <li>lambda event driven processing of logs to get insights to automatically update rules</li> <li><code>pricing</code></li> <li>monthly per WEBACL</li> <li>monthly per rule/rule group on WEBACL</li> <li>monthly per million requests per WEBACL</li> <li>bot control monthly and per requests</li> <li>captcha per 1000s</li> <li>fraud control/account takeover per month and logins attempts</li> <li>marketplace rule groups</li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#cloudhsm","title":"CloudHSM","text":"<ul> <li>similar to KMS</li> <li>single tenant hardware security module (HSM)</li> <li>provisioned by aws</li> <li>aws has no access to parts where keys are stored</li> <li>fully customer managed</li> <li>Compliance (FIPS)</li> <li>HSM : FIPS 140-2 Level 3</li> <li>KMS : FIPS 140-2 Level 2</li> <li><code>Integration</code></li> <li>HMS : industry standard apis<ul> <li>PKCS#11, JCE, CryptoNG</li> </ul> </li> <li>KMS : aws standard apis/iam</li> <li>KMS can use CloudHSM as a custom key stor</li> <li>architecture</li> <li>deployed into an aws managed vpc</li> <li>runs in 1 AZ</li> <li>HA : create cluster with an HSM in each AZ being used</li> <li>HSM keep keys and policies in sync when nodes are added or removed</li> <li>ENI for each HSM in cluster is injected to customer VPC</li> <li> <p>CloudHSM client is required to communicate with the HSM</p> </li> <li> <p>consideration</p> </li> <li>no native aws integration eg) S3</li> <li>can be used to offload SSL/TLS processing for web servers</li> <li>enable transparent data encryption (TDE) for oracle databases</li> <li>protect private keys for an issuing certificate authority</li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#config","title":"Config","text":"<ul> <li><code>record configuration changes over time on resources</code></li> <li>auditing of changes, compliance with standards</li> <li>does not prevent changes happening</li> <li>regional service</li> <li>supports cross-region and cross account aggregation</li> <li>changes can generate SNS notifications and near-realtime events via EventBridge and Lambda</li> <li>stores data in s3 config bucket</li> <li>standard</li> <li>configuration of all supported resources are tracked</li> <li>configuration item is created when a change occurs</li> <li>all configuration items for a resource(known configuration history) are stored in s3</li> <li><code>config rules</code></li> <li><code>aws managed or custom</code></li> <li>evaluates resources against a defined standard<ul> <li>resources are compliant or non-compliant</li> </ul> </li> <li>custom rules use lambda for evaluation</li> <li>can integrate with ssm to fix configuration of instances</li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#macie","title":"Macie","text":"<ul> <li>data security and data privacy service</li> <li><code>discover, monitor, and protect data that is stored in s3 buckets</code></li> <li>automated discovery of data</li> <li>personally identifiable information (<code>PII</code>)</li> <li>personal health information (<code>PHI</code>)</li> <li>Finance</li> <li><code>managed data identifiers</code></li> <li>built in</li> <li>find almost all common sensitive data</li> <li><code>custom data identifiers</code></li> <li>proprietary</li> <li>regex</li> <li><code>keywords</code> : optional sequences that need to be in proximity to regex match</li> <li><code>maximum match distance</code></li> <li><code>ignore words</code> : if regex match contains ignore words, it's ignored</li> <li><code>integration</code></li> <li>pass finding events to EventBridge</li> <li><code>multi-account</code></li> <li>via aws orgs or macie account inviting</li> <li><code>findings</code></li> <li><code>policy findings</code><ul> <li>when policy or settings change for s3 bucket that reduces security </li> <li>after macie is enabled</li> </ul> </li> <li> <p><code>sensitive data</code></p> </li> <li> <p><code>architecture</code></p> </li> <li>1 or more s3 buckets</li> <li>create <code>discover job</code> in macie<ul> <li>which buckets to analyze</li> <li><code>schedule</code> : when and how frequently it runs</li> <li>use identifiers to look for matches (<code>findings</code>) </li> </ul> </li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#inspector","title":"Inspector","text":"<ul> <li><code>scans EC2 instance and the instance OS</code></li> <li>also containers</li> <li><code>checks for vulnerabilities and deviations against best practice</code></li> <li>provides a report of findings ordered by priority</li> <li><code>network assessment</code></li> <li>agentless</li> <li><code>network &amp; host assessment</code></li> <li>uses agent</li> <li><code>rules packages</code></li> <li>determine what is checked</li> <li><code>network reachability</code><ul> <li>checks end-to-end</li> <li>ec2, alb, dx etc.</li> <li>findings</li> <li>RecognizedPortWithListener</li> <li>RecognizedPortNoListener</li> <li>RecognizedPortNoAgent</li> <li>UnrecognizedPortWithListener</li> </ul> </li> <li><code>common vulnerabilities and exposures (CVE)</code></li> <li><code>center for internet security (CIS) benchmarks</code></li> <li><code>security best practices for amazon inspector</code></li> </ul>"},{"location":"AWS/SecurityDeploymentOperations/#guardduty","title":"Guardduty","text":"<ul> <li><code>continuous</code> security monitoring service</li> <li>identify unexpected and unauthorized activities</li> <li>can integrated with eventbridge to notify or start event driven protection/remediation</li> <li>support multiple accounts</li> <li>supports data sources</li> <li>dns logs, vpc flow logs, cloudtrail event logs, cloudtrail management events, cloudtrail s3 data events</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/","title":"ServerlessAndApplicationServices","text":""},{"location":"AWS/ServerlessAndApplicationServices/#architecture","title":"Architecture","text":""},{"location":"AWS/ServerlessAndApplicationServices/#monolith-architecture","title":"Monolith Architecture","text":"<ul> <li><code>all components of application are together</code></li> <li>ex) web, db, storage</li> <li><code>fail together</code> : one component failing can affect evert other component</li> <li><code>scale together</code> : since every component is in the same server</li> <li><code>bill together</code> : all components have allocated resources even if they aren't consuming them</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#tiered-architecture","title":"Tiered Architecture","text":"<ul> <li><code>different components in different tiers</code></li> <li><code>still coupled together</code> : since each tier connects to the endpoint of another tier</li> <li><code>tiers can be vertically scaled independently of other tiers</code></li> <li>ex) increase size of web tier without affecting the db tier</li> <li><code>can use load balancers in between tiers</code></li> <li>abstract away connections</li> <li><code>allow tiers to horizontally scale independently</code></li> <li>tier being load balanced will still need to exist, else tiers depending on it will fail</li> <li><code>can't be scaled to zero</code></li> <li>still requires synchronous communication</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#evolving-with-queues","title":"Evolving with Queues","text":"<ul> <li><code>worker fleet architecture</code></li> <li><code>Queues can be put in between tiers</code></li> <li>decouples tiers</li> <li><code>asynchronous communication</code></li> <li><code>AutoScaling Group</code></li> <li>min/desired/max (0,0,100)</li> <li>scaling policy based on queue length</li> <li><code>provision instances to handle messages in queue, and scale to zero when queue is empty</code></li> <li>no communication happens directly</li> <li>components are decoupled</li> <li><code>components can scale independently</code></li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#microservice-architecture","title":"Microservice Architecture","text":"<ul> <li>micro-services do individual things very well</li> <li>components in a micro-service can be <code>producers and/or consumers</code></li> <li>producers generate event when something happens</li> <li><code>event router</code></li> <li>central exchange point for events</li> <li>has an event bus : constant flow of information</li> <li><code>producers send information to event bus, and event router delivers them to event consumers</code></li> <li><code>no constant running of resources or waiting for things</code></li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#aws-lambda","title":"AWS Lambda","text":"<ul> <li><code>function as a server (FaaS)</code></li> <li><code>900s</code> or <code>15mins</code> function timeout</li> <li> <p>assume <code>stateless</code> : brand new environment is used each time lambda is invoked</p> </li> <li> <p><code>resources</code></p> </li> <li><code>direct memory (indirect CPU) allocation</code><ul> <li>128MB to 10240MB memory</li> <li>1vCPU per 1769MB scaling</li> </ul> </li> <li> <p>disk space</p> <ul> <li><code>512MB</code> storage available as <code>/tmp</code></li> </ul> </li> <li> <p><code>billed for the duration that a function runs</code></p> </li> <li> <p><code>execution role</code></p> </li> <li>iam role assumed by lambda</li> <li> <p>provides permission to interact with other aws products and services</p> </li> <li> <p><code>design patterns</code></p> </li> <li><code>serverless applications</code> : s3, api gateway, lambda</li> <li><code>file processing</code> : s3, s3 events, lambda</li> <li><code>database triggers</code> : dynamodb, streams, lambda</li> <li><code>serverless cron</code> : eventbridge/CWEvents + lambda</li> <li> <p><code>realtime stream data processing</code>: kinesis + lambda</p> </li> <li> <p><code>networking</code></p> </li> <li><code>public networking</code><ul> <li><code>default</code></li> <li>can access <code>public aws services</code> and the <code>public internet</code></li> <li>best performance</li> <li>can run on shared hardware and networking with nothing specific to a customer</li> <li><code>no access to VPC based services</code> by default</li> <li>unless public IP's are provided and security controls allow external access</li> </ul> </li> <li> <p><code>private lambda</code></p> <ul> <li><code>runs in a private subnet</code></li> <li><code>obey all VPC networking rules</code></li> <li>can access VPC resources</li> <li>can't access things outside of VPC, unless VPC network configuration allows it</li> <li><code>VPC endpoints</code> can provide access to public aws services</li> <li><code>natGW and internetGW</code> are required to access internet resources</li> </ul> </li> <li> <p><code>security</code></p> </li> <li><code>execution role</code><ul> <li>iam role assumed by lambda</li> <li>provides permission to interact with other aws products and services</li> </ul> </li> <li> <p><code>resource policy</code></p> <ul> <li>controls what services and accounts can invoke a lambda function</li> </ul> </li> <li> <p><code>logging</code></p> </li> <li><code>cloudwatchlogs</code><ul> <li>stores <code>logs</code> from lambda executions</li> </ul> </li> <li><code>cloudwatch</code><ul> <li>stores <code>metrics</code></li> <li>invocation success/failure, retries, latency</li> </ul> </li> <li> <p><code>X-Ray</code></p> <ul> <li>distributed tracing</li> </ul> </li> <li> <p><code>invocations</code></p> </li> <li><code>synchronous</code><ul> <li>cli/api invoke lambda and <code>wait for a response</code></li> <li>errors and retries have to be handled within the client</li> </ul> </li> <li><code>asynchronous</code><ul> <li><code>aws services invoke lambda function</code></li> <li>ex) s3 event triggers lambda and s3 doesn't wait for a response</li> <li>if event fails, can retry 0-2 times</li> <li>retry logic handled by lambda</li> <li>processing needs to be idempotent </li> <li>events can be sent to a <code>dead letter queue after repeated failed</code> processing</li> <li><code>destinations</code></li> <li>events processed by lambda can be delivered to another destination (SQS, SNS, lambda, EventBridge) depending on success or failure</li> </ul> </li> <li> <p><code>event source mappings</code></p> <ul> <li><code>polls streams or queues</code> for batches of data and sends them to lambda function</li> <li>can't have partially failed/successful batch</li> <li>everything fails or succeeds</li> <li><code>permissions from lambda execution role are used</code> by event source mapping to interact with event source</li> <li>failed batches can be sent to DLQ</li> </ul> </li> <li> <p><code>Versions</code></p> </li> <li>lambda function can have different versions</li> <li><code>version</code> : is the code and the configuration of the lambda function</li> <li><code>immutable</code> : never changes once published, has its own ARN</li> <li><code>$Latest</code> points at the latest version</li> <li> <p><code>Aliases</code> (DEV, Stage, PROD) point at a version - can be changed</p> </li> <li> <p><code>Latency</code></p> </li> <li><code>execution context</code> : environment a lambda function runs in</li> <li>c<code>old start : ~100ms</code><ul> <li>environment is created</li> <li>runtime is downloaded and installed</li> <li>deployment package downloaded and installed</li> </ul> </li> <li><code>warm start : ~1-2ms</code><ul> <li>if future invocation happens quickly, then same context can be used</li> <li><code>strategies</code></li> <li><code>save stuff to tmp</code> to save time when context is reused<ul> <li>lambda needs to cope if context is brand new</li> </ul> </li> <li><code>define stuff outside if lambda handler</code>, and it can be reused again in invocations that occur in same context</li> </ul> </li> <li><code>provisioned concurrency</code><ul> <li>aws creates and <code>keeps a number of contexts warm</code>, improving start speed</li> </ul> </li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#cloudwatch-events-and-eventbridge","title":"CloudWatch Events and EventBridge","text":"<ul> <li><code>CloudWatch Events</code> delivers a near realtime stream of system events</li> <li>describing changes in aws products and service</li> <li>events are JSON objects</li> <li><code>EventBridge</code> will replace cloudwatch events (a <code>superset</code>)</li> <li> <p><code>can handle events from third parties and custom applications</code></p> </li> <li> <p><code>there is  default EventBus for a single aws account</code></p> </li> <li>stream of events that occur from any supported service inside the aws account</li> <li><code>cloudwatch events has only one event bus</code></li> <li> <p><code>EventBridge can create additional event buses</code></p> </li> <li> <p><code>rules</code></p> </li> <li>rules are <code>linked to a specific event bus</code></li> <li>define <code>targets</code></li> <li><code>matches an event</code> and <code>routes to one or more targets</code></li> <li><code>event pattern rule</code> : tries to matches actual events</li> <li><code>scheduled rules</code><ul> <li>use a <code>cron expression</code> to schedule execution</li> </ul> </li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#serverless-architecture","title":"Serverless Architecture","text":"<ul> <li>low overhead : manage few, if any servers</li> <li>applications are a collection of small and specialized functions</li> <li>stateless and ephemeral environments : duration billin</li> <li>event driven : consumption only when being used</li> <li>FaaS is used where possible for compute functionality</li> <li>managed services are used when possible : ex) S3, ddb</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#simple-notification-service","title":"Simple Notification Service","text":"<ul> <li><code>public</code> aws service</li> <li>coordinates the sending and delivery of messages</li> <li>messages are &lt;= <code>256KB</code></li> <li>SNS <code>Topics</code> are the base entity of SNS</li> <li>permissions and configurations</li> <li><code>Publisher</code> : <code>send</code> messages to a TOPIC</li> <li><code>Subscribers</code> : <code>receive</code> messages from a TOPIC</li> <li>HTTP endpoints, EMAIL, SQS, SMS, Lambda, Mobile Push</li> <li>can <code>filter</code> subscribers for relevant messages</li> <li>can <code>fan out a single message to multiple</code> sqs queues to process related workloads</li> <li><code>Delivery Status</code> : can confirm for some resource like HTTP, lambda, SQS</li> <li><code>Delivery Retries</code></li> <li><code>HA and Scalable within a Region</code></li> <li>Supports Server Side Encryption <code>(SSE)</code></li> <li><code>Cross-Account</code> via Topic Policy</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#step-functions","title":"Step Functions","text":"<ul> <li>State Machine Service</li> <li>serverless workflow START -&gt; STATES -&gt; END</li> <li>states are things that occur during the workflow</li> <li>States</li> <li><code>SUCCEED</code> % FAIL</li> <li><code>WAIT</code> : pause workflow</li> <li><code>CHOICE</code> : helps take a different path based on input</li> <li><code>PARALLEL</code> : create parallel branches within the state machine</li> <li><code>MAP</code> : takes a list as input, and performs an action for each item in list</li> <li><code>TASK</code> : single unit of work performed by a state machine</li> <li><code>standard</code></li> <li>default</li> <li>1 year execution limit</li> <li><code>express</code></li> <li>high volume</li> <li>5 minutes execution</li> <li>Created with Amazon States Language (<code>ASL</code>)</li> <li>IAM Role used for permissions</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#api-gateway","title":"API Gateway","text":"<ul> <li>Create and manage APIs</li> <li>Endpoint/entrypoint for applications</li> <li>sits between applications and integrations (services)</li> <li>highly available, scalable, handles authorization, throttling, caching, CORS, transformations, OPENApi spec, direct integration with aws services</li> <li>Phases</li> <li>Request Phase:<ul> <li>authorize, validate, transform incoming data from clients into a form that integration can handle</li> </ul> </li> <li>Response Phase:<ul> <li>Takes output from integration and transforms, prepares, and returns it to the client</li> </ul> </li> <li><code>Public Service</code></li> <li><code>HTTP</code>, <code>REST</code>, <code>WebSocket</code> APIs</li> <li><code>Authentication</code>:</li> <li><code>cognito</code><ul> <li>authenticate with cognito and receive token</li> <li>pass token token with request to api gateway, which then validates the token with cognito</li> </ul> </li> <li><code>lambda authorizer</code><ul> <li>client passes bearer token with request</li> <li>gateway calls a lambda authorizer, which returns iam policy and principle identifier</li> </ul> </li> <li><code>Endpoint Types</code></li> <li><code>Edge-Optimized</code> : routed to nearest CloudFront point of presence (POP)</li> <li><code>Regional</code> : clients in the same region</li> <li><code>Private</code> : accessible only within a VPC</li> <li><code>Stages</code></li> <li>APIs are deployed to stages, to isolate prod and testing</li> <li><code>Canary</code> : deployments not made to stage but to canary (subsection of stage)</li> <li>configure certain percent of traffic to be sent to canary</li> <li>canary can be promoted to become new base stage</li> <li>Errors</li> <li><code>4XX</code> : Client Error, invalid request on client side</li> <li><code>5XX</code> : Server Error, invalid request on server side</li> <li><code>403</code> : Access Denied, authorization error</li> <li><code>429</code> : throttling</li> <li><code>502</code> : bad output return</li> <li><code>503</code> : service unavailable</li> <li><code>504</code> : integration failure (29s)</li> <li><code>Caching</code></li> <li>configured per stage</li> <li><code>0 t0 3600s TTL, default 300s</code></li> <li>can be <code>encrypted</code></li> <li>500MB to 237GB</li> <li>reduce load, cost, and improved performance</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#simple-queue-service-sqs","title":"Simple Queue Service (SQS)","text":"<ul> <li>Public, fully managed, highly available, message queue service</li> <li>used to decouple application components</li> <li><code>Standard</code></li> <li>at least once delivery</li> <li>very high scaling</li> <li><code>FIFO</code></li> <li>guarantee an order</li> <li>exactly once delivery</li> <li>scales less : 3000 messages per second with batching, 300 without</li> <li>Messages up to <code>256KB</code> in size</li> <li>link to larger data</li> <li>received messages are hidden (<code>VisibilityTimeout</code>)</li> <li>client has certain amount of time to process message and delete item from queue</li> <li>if client fails, it will reappear (for a retry)</li> <li><code>Dead-Letter Queues</code> can be used for problem messages</li> <li><code>ASGs can scale based on length of a queue, and lambdas can be invoked based on queue length</code></li> <li><code>Billed based on requests</code></li> <li>1 request = 1-10 messages up to 25kKB</li> <li><code>short polling</code> : immediate, can get 0 messages</li> <li><code>long polling</code> : immediate (waitTimeSeconds)<ul> <li>waits for messages, cheaper, preferred</li> </ul> </li> <li>Encryption at rest (<code>KMS</code>) and in transit</li> <li><code>Queue Policy</code> (resource policy): can allow access from external account</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#kinesis","title":"Kinesis","text":"<ul> <li>scalable streaming service</li> <li> <p>public service, high availability</p> </li> <li> <p>SQS vs Kinesis</p> </li> <li>Kineses<ul> <li>ingestion, analytics, monitoring, app clicks</li> <li>multiple consumers</li> <li>rolling window persistence</li> </ul> </li> <li> <p>SQS</p> <ul> <li>worker pools, decoupling, asynchronous communication</li> <li>no persistence of messages</li> <li>1 production group 1 consumption group</li> </ul> </li> <li> <p>Kinesis</p> </li> <li>Data Steams : large scale ingestion of data, consumption by consumers</li> <li>Firehose : near real time delivery services</li> <li>Data Analysis : real time SQL processing</li> <li> <p>Video Streams : live video ingestion and analytics</p> </li> <li> <p>Firehose vs Data Analytics</p> </li> <li>Firehose : near realtime, simple lambda transformations</li> <li>Data Analytics : realtime, complex SQL transformations</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#kinesis-data-streams","title":"Kinesis Data Streams","text":"<ul> <li>allow large scale ingestion of data into aws, and consumption of data by other compute services</li> <li><code>producers</code> send data into kinesis stream</li> <li><code>multiple consumers</code> can access data from that moving window</li> <li>real time</li> <li><code>Scaling</code></li> <li>streams can scale from low to near infinite data rates</li> <li>uses <code>shard</code> architecture</li> <li>starts with one shard, and addition shards are added as more performance is needed</li> <li><code>1 shard : 1MB ingestion, 2MB Consumption capacity</code></li> <li>more shards, more performance, more costly</li> <li>streams store a <code>24-hour</code> moving window of data by default<ul> <li>can increase window to <code>7 days</code> at an increased cost</li> <li>kinesis data record : <code>1MB</code></li> </ul> </li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#kinesis-data-firehose","title":"Kinesis Data Firehose","text":"<ul> <li> <p>fully managed service to load data for data lakes, data stores, and analytics services</p> </li> <li> <p>connects to kinesis stream and moves data to another service</p> </li> <li>HTTP, splunk, Redshift, Elasticsearch, S3</li> <li>Redshift uses S3 as an intermediate storage</li> <li> <p>producer can send data to firehose directly</p> </li> <li> <p>automatic scaling, fully serverless, resilient</p> </li> <li><code>near real time delivery</code> (60 seconds)</li> <li>delivery when buffer size fills (1MB) or buffer interval in seconds passes (60seconds)</li> <li>supports <code>transformation</code> of data on the fly</li> <li>with <code>lambda</code></li> <li>optionally store source data in s3 bucket</li> <li> <p><code>billing</code> : based on data volume</p> </li> <li> <p>Use cases</p> </li> <li>provide persistence</li> <li>store data in a different format</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#kinesis-data-analytics","title":"Kinesis Data Analytics","text":"<ul> <li>real time processing of data</li> <li><code>Sources</code></li> <li>in-application input stream : kinesis data streams or firehose</li> <li>reference data : from s3 used to enrich streaming data</li> <li>Kinesis Analytics Application</li> <li>Application code processes input, using SQL</li> <li>processed data added to in-application output streams that are mapped to destination streams</li> <li>errors can be sent to in-application error stream</li> <li><code>Destinations</code></li> <li>near real time<ul> <li>kinesis firehose (S3, Redshift, Elasticsearch, splunk)</li> </ul> </li> <li>real time<ul> <li>lambda</li> <li>kinesis data streams</li> </ul> </li> <li>pay for data processed, but not cheap</li> <li>use cases:</li> <li>streaming data needing real-time SQL processing<ul> <li>time-series analytics, dashboards, metrics</li> </ul> </li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#kinesis-video-streams","title":"Kinesis Video Streams","text":"<ul> <li>ingest live video data from producers</li> <li>consumers can access data frame-by-frame or as needed</li> <li>can persist and encrypt data</li> <li>can't access data directly via storage, only via APIs</li> <li>integrates with other AWS services</li> <li>rekognition and connect</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#cognito","title":"Cognito","text":"<ul> <li>authentication, authorization, and user management for web/mobile applications</li> <li><code>USER POOLS</code></li> <li><code>sign-in &amp; sign-up  experience, managing user identities</code></li> <li>users can also sign in through social identity providers like google, facebook etc.</li> <li>allows you to sign in and get a JSON Web Token (<code>JWT</code>)<ul> <li>API Gateway can  directly authenticate with JWT</li> </ul> </li> <li>supports MFA</li> <li><code>IDENTITY POOLS</code></li> <li>Unauthenticated Identities : Guest Users</li> <li><code>Federated Identities</code><ul> <li><code>SWAP external identity for temporary aws credentials</code>.</li> </ul> </li> <li> <p>assume iam role on behalf of identity and return credentials used to access aws resources</p> </li> <li> <p><code>User Pools and Identity Pools together</code></p> </li> <li>use cognito user pool social sign in to create JWT</li> <li>pass user pool token to Identity Pools</li> <li><code>benefit : user pool abstracts all identity providers into one group</code>, identity pool only configured for a single identity provider (the user pool)</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#glue","title":"Glue","text":"<ul> <li>Serverless ETL (Extract, Transform, Load)</li> <li>moves and transforms data between source and destination</li> <li><code>Data catalog</code></li> <li>persistent <code>metadata</code> about data sources in region</li> <li><code>crawlers</code> connect to data sources, determine schema and create metadata</li> <li><code>Glue Job</code></li> <li>ETL Jobs</li> <li>uses data catalog</li> <li>performs transformations using script</li> <li>can be initiated manually or via events</li> <li>glue allocates resources from warm pool when required</li> <li><code>Data Sources</code>:</li> <li><code>Stores</code> : S3, RDS, JDBC compatible, dynamodb</li> <li><code>Streams</code> : Kinesis data stream, apache kafka</li> <li> <p><code>Targets</code> : S3, RDS, JDBC databases</p> </li> <li> <p>historically the ETL has been done by datapipeline</p> </li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#mq","title":"MQ","text":"<ul> <li>AWS implementation of Apache ActiveMQ</li> <li>supports open standards such as JMS, AMQP, MQTT, OpenWire and STOMP</li> <li>provides <code>both queues and topics</code></li> <li>one-to-one or one-to-many</li> <li>Single Instance or HA Pair</li> <li><code>VPC</code> based : need private networking</li> <li> <p>No native aws integration</p> </li> <li> <p><code>Use case:</code></p> </li> <li>migrating from an existing system</li> <li>if certain protocols are needed</li> </ul>"},{"location":"AWS/ServerlessAndApplicationServices/#appflow","title":"AppFlow","text":"<ul> <li><code>fully managed integration service</code></li> <li>exchange data between applications (connectors) using flows</li> <li>architecture</li> <li><code>connections</code><ul> <li>store configuration and credentials to access applications</li> </ul> </li> <li>configure <code>source to destination field mapping</code></li> <li>optional data <code>transformation</code></li> <li>optional <code>filters and validation</code></li> <li>sync data across applications</li> <li>ex) sales force records =&gt; redshift</li> <li>aggregate data from different source</li> <li>public endpoints, but works with PrivateLink</li> <li>AppFlow Custom Connector SDK (for custom connector)</li> </ul>"},{"location":"AWS/TechFundamentals/","title":"TechFundamentals","text":""},{"location":"AWS/TechFundamentals/#hash-functions-hashing","title":"Hash Functions &amp; Hashing","text":"<ul> <li>hashing</li> <li>take data and create fixed length representation</li> <li>unique hash for different data</li> <li>can't turn hash into original data</li> <li>hash function : algorithm to convert data into string</li> <li>collision : if different data get the same hash value</li> </ul>"},{"location":"AWS/TechFundamentals/#digital-signatures","title":"Digital Signatures","text":"<ul> <li><code>public key cryptography</code></li> <li>public key used to encrypt data</li> <li>private key used to decrypt data</li> <li>private key can sign data, and public key can verify it</li> <li><code>digital signature</code></li> <li>verifies integrity and authenticity</li> <li>hash of data is taken, original data unchanged</li> <li>digital sign hash using private key (authenticates the hash)</li> <li>use public key to verify the hash signature<ul> <li>exposes original hash</li> </ul> </li> <li>use same algorithm to hash data and see if it matches with the hash exposed by public key</li> </ul>"},{"location":"AWS/VPC/","title":"AWS Virtual Private Cloud","text":"<ul> <li>a virtual private network inside of AWS</li> <li>regionally resilient</li> <li><code>divided into subnets</code>, each located in an AZ</li> <li>private and isolated by default</li> <li>nothing in or out without explicit configuration</li> </ul>"},{"location":"AWS/VPC/#default","title":"Default","text":"<ul> <li><code>one default VPC per region</code></li> <li>comes with  internet gateway (IGW), Security Group (SG) &amp; NACL</li> <li>subnets assign public IPv4 addresses</li> </ul> <p>best practice not to use default VPC</p>"},{"location":"AWS/VPC/#aws-public-vs-private-services","title":"AWS Public vs Private Services","text":"<ol> <li><code>AWS Public Zone:</code><ul> <li>network that is connected to the public internet</li> <li>where aws services with public endpoints operate from, ie S3</li> <li>internet is used as transit in with communication</li> </ul> </li> <li><code>AWS Private Zone: Virtual Private Clouds (VPC)</code></li> <li>isolated unless configured to otherwise, can't be reached by public internet unless explicitly allowed</li> <li>services can be hosted in these private networks</li> <li>can communicate with aws public zone without touching the internet</li> <li>can access the internet via <code>IGW</code></li> <li>can be access by on-prem using <code>VPN</code> or <code>Direct Connect</code></li> </ol>"},{"location":"AWS/VPC/#sizing-and-structure","title":"Sizing and Structure","text":"<p>Considerations - what size should the VPC be - structure tiers and resiliency (availability) zones - are there any networks we can't use</p> <ul> <li><code>minimum /28 (16IP)</code></li> <li><code>maximum /16 (65456)</code></li> <li>preference to start at 10.16.x.y</li> <li>reserve 2+ networks per region being used per account</li> </ul> <p>Considerations - how many subnets are needed - how many IP's in total - how many per subnet</p> <p>Subnets - services use subnets where IP addresses are allocated from   - VPC services run from within subnets - located in one availability zone</p> <p>How to pick number of subnets 1. how many availability zones to use? (4 to be safe) : split VPC into 4 subnets    1. 1 /16 into 4 /18 2. how many tiers (choose 4) : split subnets to 4 tiers (web, app, db, pare)    1. 4 /18 each into 4 /20 -&gt; 16 /20</p>"},{"location":"AWS/VPC/#custom-vpcs","title":"Custom VPC's","text":"<ul> <li>IPv4 private CIDR Blocks and public IP's</li> <li>1 primary private IPv4 CIDR Block</li> <li>min /28, max /16</li> <li>optional secondary IPv4 Blocks</li> <li>Optional single assigned IPv6 /56 CIDR Block</li> </ul> <p>DNS In a VPC - provided by R53 - VPC Base IP + 2 Address - <code>enableDnsHostnames</code> : whether instances with public ip addresses in a VPC get public  dns names - <code>enableDnsSupport</code> : enables DNS resolution in VPC</p>"},{"location":"AWS/VPC/#vpc-subnets","title":"VPC Subnets","text":"<ul> <li>AZ Resilient</li> <li>a sub-network of a VPC, within a particular AZ</li> <li>a subnet can be in only one AZ, but an AZ can have multiple subnets</li> <li>IPv4 CIDR is a subset of the VPC CIDR</li> <li>cannot overlap with other subnets</li> <li>can optionally have IPv5 CIDR if it is enabled</li> <li>subnets can communicate with other subnets in the VPC</li> </ul> <p><code>Reserved Ip's (5 in total)</code> - Network Address, first address of subnet - Network + 1, used by VPC router - Network + 2, reserved for DNS - Network + 3, reserved for future requirements - Broadcast Address, Last Ip in subnet</p> <p>DHCP Options Set - Dynamic host configuration protocol - how compute devices receive IP addresses automatically - can't be edited, but can create a new one and reallocate - Auto Assign Public IPv4 or IPv6: assign public in addition to private IP automatically</p>"},{"location":"AWS/VPC/#routing","title":"Routing","text":"<ul> <li>a vpc router is a highly available device, present in every vpc</li> <li>default routes traffic between subnets within a vpc</li> <li>route tables control what happens to data as it leaves a subnet</li> <li><code>destination ips with larger prefix are given priority</code> when router chooses which route to use</li> <li>destination is matched with a <code>target</code> which the router will forward data to</li> <li><code>route tables attached to zero or more subnets</code></li> <li><code>subnet has to have a route table</code> either main or custom one attached</li> <li>local routes always exist, and have first priority</li> </ul>"},{"location":"AWS/VPC/#internet-gateway","title":"Internet Gateway","text":"<ul> <li><code>Regionally resilient</code> attached to a VPC</li> <li>VPC can have 0 or 1 IGW</li> <li>IGW can be attached to 0 or 1 VPC</li> <li>runs from within the aws public zone</li> <li>traffic between the VPC and the internet or aws public zone</li> <li>managed service</li> </ul> <p>Steps: 1. Create IGW 2. attach IGW to VPC 3. Create custom route table 4. Associate RT with VPC 5. Default routes =&gt; IGW 6. configure subnet to allocate IPv4</p> <ul> <li>a record is created and is maintained by IGW</li> <li>links instances private ips to their public IPv4</li> <li>instance doesn't know its public address</li> <li>IPv6 can be publicly routed by default</li> <li>IGW does no tranlations</li> <li>instances do know their IPv6</li> </ul> <p>Bastion Host / Jumpbox - instance in a public subnet - incoming management connections - then access internal VPC resources - management point or entry point for private only VPC's</p>"},{"location":"AWS/VPC/#stateful-vs-stateless-firewalls","title":"Stateful vs Stateless Firewalls","text":"<p><code>Stateless firewall</code> - see <code>inbound</code> and <code>outbound</code> traffic as two <code>separate parts</code> - will require two rules, one rule for outbound traffic and another for inbound   - outbound rule must allow all ports - requests are bound for a well known ports - response is to a random ephemeral port used by the client to make a request</p> <p><code>Stateful firewall</code> - can identify the request and response components of a connection as being related   - using port numbers and IP's - allowing a request automatically allows the response, and vice versa   - don't have to allow all outbound ports, since port relations are understood</p>"},{"location":"AWS/VPC/#network-access-control-lists-nacl","title":"Network Access Control Lists (NACL)","text":"<p>Traditional firewall available within aws vpcs.  - (<code>stateless</code>) - see <code>inbound</code> and <code>outbound</code> traffic as two <code>separate parts</code></p> <ul> <li><code>associated with subnets</code></li> <li>every subnet has one nacl</li> <li>a nacl can be associated with many subnets</li> <li>filers traffic crossing the <code>subnet boundary</code></li> <li>doesn't impact connections within a subnet</li> <li>cannot be assigned to aws resources, only subnets</li> <li>rule match the DST IP/range, DST Port, protocol and Allow or Deny</li> <li>can explicitly deny and allow</li> <li>no logical resources</li> <li>rules are <code>processed in order</code></li> <li>lowest to highest rule numbers</li> <li>once a match occurs process stops</li> <li> <ul> <li>is an implicit deny if nothing else matches</li> </ul> </li> <li>app port &amp; ephemeral ports are needed on each NACL for each communication type which occurs</li> <li>within a VPC</li> <li>to a VPC</li> <li>from a VPC</li> <li>vpc's are created with a default nacl that has no effect</li> <li>has an allow all and implicit deny rule for both inbound and outbound rules</li> <li>custom nacls can be created for a specific vpc and are initially associated with no subnets</li> <li>default only has the implicit deny, so all traffic is denied</li> <li>used together with security groups to add explicit deny</li> </ul>"},{"location":"AWS/VPC/#security-groups-sg","title":"Security Groups (SG)","text":"<ul> <li>stateful - detect response traffic automatically</li> <li>allowed (in or out) request -&gt; allowed response</li> <li>no explicit deny</li> <li>only allow or not allow (implicit deny)</li> <li>can't block specific bad actors</li> <li>supports IP/CIDR and logical resources</li> <li>including other security groups and itself</li> <li>attached to elastic network interfaces (ENI)</li> <li>attaching to an instances -&gt; attach primary network interface of the instance</li> </ul> <p>Logical Reference - reference a security group in rules   - considers all instances within the security group being referenced - reference itself   - considers anything with the SG attached   - scales with adds and removes from the SG</p>"},{"location":"AWS/VPC/#network-address-translation-nat-and-nat-gateways","title":"Network Address Translation (NAT) and Nat Gateways","text":"<p>Set of processes for remapping SRC and DST IP's. - when you want to keep resources private but give them access to making requests to resources in the public zone</p> <ul> <li><code>IP masquerading</code> many private IP's to one public IP</li> <li>gives private CIDR range outgoing internet access<ul> <li>can make request and receive response</li> </ul> </li> <li>cannot initiate from public internet to these private resources as each public IP maps to many private</li> <li>runs from a <code>public subnet</code></li> <li>so it can be given a public IP address</li> <li>default route table points to an Internet gateway</li> <li><code>AZ resilient service</code></li> <li>deploy one in each AZ for regional resilience</li> <li>pricing</li> <li>hourly charge per hour</li> <li>data processing charge per GB of processed data</li> <li><code>isn't required and doesn't work for IPv6</code></li> <li>since all IPv6 are publicly routable, unless nacl or sg are used</li> </ul> <p>EC2 as a NAT instance vs Nat Gateway - instance   - disable source and destination checks   - can be cheaper, but more management overhead   - use as a bastion server   - support NACL or SG - GW   - HA, high scalability, fully managed   - GW cannot be a bastion server   - only support nacl, use sg on instances behind GW</p>"},{"location":"AWS/VPC/#ssh-agent-forwarding","title":"SSH Agent Forwarding","text":"<ul> <li>connecting using agent forwarding creates a separate ssh channel</li> <li>allowing agent running on the client to be used ont he bastion</li> <li>when user, in bastion, connects to another resource</li> <li>the bastion can use the client ssh-agent to prove identity</li> <li>private key remains on the client at all times</li> <li>authentication requests are forwarded</li> <li> <p>same identity is used to connect to bastion and then through to the other resource</p> </li> <li> <p><code>eval ssh-agent</code> to check if agent is running</p> </li> <li><code>ssh-add -K ./keyName.pem</code> add private key to ssh agent</li> <li><code>ssh -A -i \"A4L.pem\" ec2-user@ec2-34-229-86-153.compute-1.amazonaws.com</code> make an ssh connection with agent forwarding enabled</li> <li><code>ssh  ec2-user@10.16.109.185</code> ssh from bastion to other resource</li> </ul>"},{"location":"AWS/VPC/#vpc-flow-logs","title":"VPC Flow Logs","text":"<ul> <li><code>capture metadata (not content)</code></li> <li>src ip, dest ip</li> <li>src port, dest port</li> <li>protocol<ul> <li>ICMP=1, TCP=6, UDP=17</li> </ul> </li> <li>action : ACCEPT, REJECT</li> <li>can be attached to:</li> <li><code>VPC</code> : applies to all ENIs in the VPC</li> <li><code>Subnet</code> : applies to all ENIs in the Subnet</li> <li><code>ENI</code> directly</li> <li><code>not realtime</code></li> <li><code>destinations</code></li> <li>S3<ul> <li>Can query with Athena</li> </ul> </li> <li>CloudWatchLogs</li> <li> <p>can capture ACCEPTED, REJECTED, or ALL metadata</p> </li> <li> <p>not logged</p> </li> <li>ec2 metadata service : 169.254.169.254</li> <li>DHCP</li> <li>amazon DNS, windows license</li> </ul>"},{"location":"AWS/VPC/#egress-only-internet-gateway","title":"Egress-Only Internet gateway","text":"<ul> <li>alternative to internet gateway, but gives privacy like NAT to IPv6</li> <li>outbound-only for IPv6</li> <li>create eigw, and make it as target for ::/0 route in VPC RT</li> </ul>"},{"location":"AWS/VPC/#gateway-endpoint","title":"Gateway Endpoint","text":"<ul> <li>provide private access to S3 and DynamoDB (which are public services)</li> <li><code>regional</code></li> <li>HA</li> <li>can't access cross-region</li> <li>prefix list added to route table</li> <li>represents the services (S3, DDB)</li> <li>points to gateway endpoint</li> <li>automatically configured by aws</li> <li>associated with vpc</li> <li><code>can only be accessed from within the vpc</code></li> <li>set which subnets are going to be used with it</li> <li><code>Endpoint policy</code></li> <li> <p>control what it can access</p> </li> <li> <p>use-case:</p> </li> <li>allow vpc to access public resources</li> <li><code>prevent leaky buckets</code><ul> <li>s3 buckets can be set to private only by allowing access only from a gateway endpoint</li> </ul> </li> </ul>"},{"location":"AWS/VPC/#interface-endpoint","title":"Interface Endpoint","text":"<ul> <li>provide private access to aws public services</li> <li>not ddb</li> <li>alternative to gateway endpoint</li> <li>added to specific subnets</li> <li><code>not HA</code></li> <li>for HA, one endpoint per AZ</li> <li>network access controlled via security groups</li> <li>endpoint policies</li> <li><code>TCP</code> and <code>IPv4</code> only</li> <li>Uses <code>PrivateLink</code></li> <li>inject third part services into private VPC</li> <li>Uses <code>DNS</code> and is allocated a private IP from the subnet it is in</li> <li>endpoint provides new services DNS endpoint</li> <li>Regional DNS</li> <li>Zonal DNS : per interface</li> <li><code>Private DNS</code> : overrides the default DNS for services with interface endpoint<ul> <li>private instances will default to use interface</li> </ul> </li> </ul>"},{"location":"AWS/VPC/#peering","title":"Peering","text":"<ul> <li>create encrypted network link between <code>two VPCs</code></li> <li><code>same/cross-region</code></li> <li>same region SGs <code>can reference peer SGs</code></li> <li><code>same/cross-account</code></li> <li>optional : public host-names can resolve to private Ips</li> <li>ex) public -&gt; private ec2 instance dns host name</li> <li>use same dns names to access services whether their in peer vpc or not</li> <li><code>does Not support transitive peering</code></li> <li>a -&gt; b -&gt; c != a -&gt; c</li> <li>must create a peering connection with each pair</li> <li>routing configuration is needed, SGs and NACL can filter</li> <li>directing traffic to remote CIDR to peer gateway object</li> <li><code>cannot be created where there is overlap in VPC CIDRs</code></li> </ul>"},{"location":"AWS/cloudComputing/","title":"cloudComputing","text":""},{"location":"AWS/cloudComputing/#cloud-computing","title":"Cloud Computing","text":"<p>Five general ; characteristic of cloud systems</p> <ol> <li><code>On demand self service</code> : provision and terminate using UI/CLI without human interaction</li> <li><code>Broad Network Access</code> : available over any <code>network</code> and access through <code>standard mechanisms</code></li> <li><code>Resource Pooling</code> : economies of scale, cheaper services</li> <li><code>Rapid Elasticity</code> : can be elastically provisioned to <code>scale rapidly outward and inward with demand</code></li> <li><code>Measured Service</code> : resource usage is measured, pay for what you use</li> </ol>"},{"location":"AWS/cloudComputing/#public-vs-private-vs-hybrid-vs-multi-cloud","title":"Public vs Private vs Hybrid vs Multi Cloud","text":"<ul> <li><code>public cloud</code></li> <li>aws, azure, gcp</li> <li><code>multi-cloud</code></li> <li>using a combination of public cloud service providers</li> <li><code>private</code></li> <li>on-premises version of <code>real</code> cloud</li> <li><code>hybrid</code></li> <li>both public and private</li> </ul> <p><code>Hybrid cloud is not public cloud + legacy on-premises</code></p>"},{"location":"AWS/cloudComputing/#service-models","title":"Service Models","text":"<p>Different levels of service models, that differ on how much of the infrastructure stack is handled by a user.</p> <p></p> <ul> <li>Infrastructure as a Service (IAAS) </li> <li><code>facilities, infrastructure, servers, and virtualization</code> are abstracted away</li> <li>deal with everything else</li> <li>Platform as a Service (PaaS)</li> <li>OS, Containers are further abstracted away</li> <li>Software as a Service (SaaS)</li> <li>only interact with the application</li> </ul>"},{"location":"AWS/cloudFormation/","title":"CloudFormation","text":"<p>Create aws infrastructure by creating templates. - templates can be either YAML or JSON</p>"},{"location":"AWS/cloudFormation/#sections","title":"Sections","text":""},{"location":"AWS/cloudFormation/#aws-template-format-version","title":"AWS Template Format Version","text":"<ul> <li>used to allow extending the service when new versions come out</li> </ul>"},{"location":"AWS/cloudFormation/#description","title":"Description","text":"<ul> <li>describe you template</li> <li>if the template format version is in the templates, it must come before this section</li> </ul>"},{"location":"AWS/cloudFormation/#metadata","title":"Metadata","text":"<ul> <li>template metadata</li> </ul>"},{"location":"AWS/cloudFormation/#parameters","title":"Parameters","text":"<ul> <li>set of parameters</li> </ul>"},{"location":"AWS/cloudFormation/#mapping","title":"Mapping","text":"<ul> <li>set of mappings</li> <li>create look tables</li> </ul>"},{"location":"AWS/cloudFormation/#conditions","title":"Conditions","text":"<ul> <li>set of conditions</li> </ul>"},{"location":"AWS/cloudFormation/#transforms","title":"Transforms","text":"<ul> <li>set of transforms</li> </ul>"},{"location":"AWS/cloudFormation/#resources","title":"Resources","text":"<ul> <li>mandatory section of a cloudformation template</li> <li>list of logical resources</li> <li>generally have properties used to configure the resources</li> </ul>"},{"location":"AWS/cloudFormation/#outputs","title":"Outputs","text":"<ul> <li>what to return once a resource is created</li> </ul>"},{"location":"AWS/cloudFormation/#flow","title":"Flow","text":"<p><pre><code>  graph LR;\n      Template--&gt;CloudFormation;\n      CloudFormation--&gt;Stack;\n</code></pre> - a stack is a living and active representation of the template - contains the logical resources from the template - cloudFormation will also create the physical infrastructure of the the logical resources</p>"},{"location":"AWS/cloudWatch/","title":"Cloud Watch","text":"<p>Collects and manages operation data</p> <p>Performs three main jobs - <code>metrics</code> : collection, monitoring, and action data related to aws instances, like cpu utilizations - <code>logs</code> : collection, monitoring, and action based on logging data - <code>Events</code> : event hub   - generate actions based on an aws service or time</p>"},{"location":"AWS/cloudWatch/#namespace","title":"<code>Namespace</code>","text":"<ul> <li>container for monitoring data</li> <li>aws/service is reserved for aws</li> </ul>"},{"location":"AWS/cloudWatch/#metric","title":"<code>Metric</code>","text":"<ul> <li>collection of related data points in a time ordered structure</li> <li><code>data points</code> : one time point of a specific metric</li> <li><code>dimension</code> : name : value pairs used to separate data points for different things, ex) instance type and value</li> <li><code>alarm</code> : linked to a specific alarm, can take action based on alarms state (<code>OK</code>, <code>ALARM</code>) that is decided by some logic on the metric</li> </ul>"},{"location":"AWS/cloudWatch/#cloudwatch-logs","title":"CloudWatch Logs","text":"<ul> <li><code>public</code> service : useable from aws on-premise as long as you have network connectivity and permissions</li> <li><code>store</code>, <code>monitor</code>, and <code>access</code> logging data</li> <li>built in integration with aws services such as EC2 and Lambda</li> <li>for anything outside of aws, use the <code>unified cloudwatch agent</code></li> <li>can generate a metric based on logs with <code>metric filter</code></li> </ul>"},{"location":"AWS/cloudWatch/#architecture","title":"Architecture","text":"<ul> <li>a <code>regional</code> service</li> <li>a <code>log event</code> are messages from a source</li> <li>a <code>log stream</code> is a sequence of log events from the same source</li> <li>a <code>log group</code> is a container for multiple log streams for the same type of loggingq</li> <li><code>retention</code> and <code>permission</code> configurations are set here</li> <li><code>metric filters</code> are also defined here</li> </ul>"},{"location":"AWS/cloudWatch/#cloud-trail","title":"Cloud Trail","text":"<ul> <li>logs api calls that affect an aws account (almost everything) as <code>CloudTrail Event</code></li> <li><code>90</code> days stored by default in <code>Event History</code></li> <li>need to create a trail to customise this</li> <li><code>Management</code> or <code>Data</code> Events</li> <li><code>Management</code> : creating or terminating resources (logged by default)</li> <li><code>Data</code> : information about resource operations, like accessing s3 object (must be enabled)</li> <li><code>regional</code> service</li> <li><code>one</code> region</li> <li><code>all regions</code> : trail in every region is aggregated as if it is one trail</li> <li><code>global</code> services log to <code>us-east-1</code> (must enable global service event logging)</li> <li>can store in an s3 bucket indefinitely as compressed json files</li> <li>can be integrated with cloudwatch logs</li> <li>CloudTrail can put its logging data into cloudwatch logs</li> <li>can create an organization trail to log OU</li> <li><code>not realtime</code></li> </ul>"},{"location":"AWS/exam/","title":"Exam","text":""},{"location":"AWS/exam/#exam-technique","title":"Exam Technique","text":"<ul> <li>shared exam room, kiosk, or at home</li> <li>130 minutes</li> <li>65 questions (2mins/question)</li> <li>720/1000 pass mark</li> <li>domain weights</li> <li>design resilient architecture : 30%</li> <li>design high-performing architecture : 28%</li> <li>design secure applications and architectures : 24%</li> <li>design cost optimized architecture : 18%</li> <li>question type</li> <li>multi choice : pick 1 / many</li> <li>multi-select :  pick correct</li> <li><code>difficulty technique</code></li> <li>phase 1 : do all the easy ones first</li> <li>phase 2 :<ul> <li>mark hard questions for review</li> <li>focus on medium questions</li> </ul> </li> <li>phase 3 : do hard questions, guess if running out of time</li> </ul>"},{"location":"AWS/exam/#question-technique","title":"Question Technique","text":"<ul> <li>question structure</li> <li>1-2 lines of preamble (scenario)</li> <li>question itself</li> <li>4-5 answers to pick from</li> <li>generally 1 or 2 answers which can be excluded</li> <li>locate these first</li> <li>most questions have a criteria or restriction</li> <li>cost effective</li> <li>best practice</li> <li>highest performance</li> <li>timeframe</li> </ul>"},{"location":"AWS/globalInfrastructure/","title":"Global Infrastructure","text":""},{"location":"AWS/globalInfrastructure/#regions","title":"Regions","text":"<ul> <li>Geographic Separation - isolated Fault Domain</li> <li>Geopolitical Separation - Different governance</li> <li>Location Control - Performance</li> </ul>"},{"location":"AWS/globalInfrastructure/#availability-zone-az","title":"Availability Zone (AZ)","text":"<ul> <li>one or more isolated data centers infrastructure within a region</li> <li>connected with high speed low-latency</li> </ul>"},{"location":"AWS/globalInfrastructure/#service-resilience","title":"Service Resilience","text":"<p><code>Global resilient</code> - globally available, ie Iam</p> <p><code>Region Resilient</code> - run in a single region - generally replicate data across AZ</p> <p><code>AZ Resilient</code> - run in a single AZ</p>"},{"location":"AWS/globalInfrastructure/#local-zones","title":"Local Zones","text":"<ul> <li>infrastructure placed close to large population and industry centers</li> <li>used to reduce distance of datacenter for <code>low latency</code></li> <li>1 zones, <code>no built int resilience</code></li> <li>not all products supported</li> <li><code>support DX</code></li> <li>use for the highest performance</li> </ul>"},{"location":"AWS/json/","title":"JSON","text":"<p>An object is an unordered set of <code>key : value</code> pairs. (dictionary) An array is an ordered collection of values separated by commas and enclosed in square brackets. (list)</p> <pre><code>{\n    \"string\": \"cat\",\n    \"number\": 323,\n    \"boolean\" : true,\n    \"null\" : null,\n    \"array\" : [1,2,3],\n    \"object\" : {\n        \"key\" : \"value\"\n    } \n}\n</code></pre>"},{"location":"AWS/networking_01/","title":"Networking","text":""},{"location":"AWS/networking_01/#osi-7-layer-model-networking-stack","title":"OSI 7-Layer Model (networking Stack)","text":"<p> - <code>media layers</code> : dealing with how data moves from point a to point b - <code>host layers</code> : deal with how data is chopped up and reassembled for transport</p>"},{"location":"AWS/networking_01/#layer-1-physical","title":"Layer 1 : Physical","text":"<ul> <li>layer 1 (physical) specifications define the <code>transmission and reception</code> of raw bit streams between a device and a shared physical medium</li> <li>defines things like voltage levels, timing, rates ...</li> <li>devices with the same specification have a shared understanding of a physical medium that connects them, thereby allowing them to <code>communicate</code></li> <li>if multiple devices transmit at once, a <code>collision</code> occurs, no collision detection</li> <li><code>no access control</code> : no control which devices can transmit</li> <li><code>no uniquely identified devices</code></li> <li>one broadcast and one collision domain</li> </ul> <p><code>hub</code> - multi-port device connecting devices on layer 1 level - propogates collisions in one port to all ports`</p>"},{"location":"AWS/networking_01/#layer-2-data-link","title":"Layer 2 : Data Link","text":"<p>Introduces a unique hardware address for every device on a network (<code>MAC addresses</code>)</p> <p>Introduces <code>frames</code>: A format for sending data over an layer 2 network</p> <ol> <li><code>preamble and start frame delimiter</code> : allow devices to identify the start of the frame</li> <li><code>destination MAC address</code></li> <li><code>source MAC address</code></li> <li><code>Ether type</code> : specify which layer 3 protocol is putting dat in frame</li> <li><code>payload</code> : data the frame carries</li> <li><code>frame check sequence</code> : check for corruption</li> </ol> <p>1, 2 3 are considered the MAX HEADER</p> <p>layer 2 frames generated from source side -&gt; passed to layer 1 -&gt; raw data passed to physical medium -&gt; taken off physical medium at destination side -&gt; passed to layer 2 software to interpret the frame</p> <p><code>Carrier Sense Multiple Access (CSMA)</code> - used to look for a carrier signal (if anyone other device is sending a message on the network) - if no carrier, data is sent to layer 1 to be transmitted through the medium</p> <p><code>Collision Detection (CD)</code> - a jam signal is sent by devices that detect a collision, then a random back off occurs   - back off : random period of time where no device will send a message</p> <p><code>Switch</code> - multi-port device that connects multiple devices on a layer 2 level - only send valid frames so collisions are isolated on the port, x-ports -&gt; x-collision domains</p> <p>Decimal and Binary Conversion | position              | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | | --------------------- | --- | --- | --- | --- | --- | --- | --- | --- | | Binary Position Value | 128 | 64  | 32  | 16  | 8   | 4   | 2   | 1   |</p>"},{"location":"AWS/networking_01/#layer-3-network","title":"Layer 3 : Network","text":"<p>Internet Protocol (IP) is a Layer 3 protocol which adds cross-network IP addressing and routing to move data between local area networks.</p> <p><code>IP Packet</code> : Generally store data from layer 4.</p> <p><code>v4</code> - Protocol : layer 4 protocol to pass data into - source and destination IP addresses - time to live - data</p> <p><code>v6</code> - larger source and destination IP addresses - hop limit instead of time to live</p> <p>Packets move between intermediary layer 2 network. Each time, the packet will be encapsulated in a frame specific to the network.</p> <p><code>IP Address (v4)</code> - 4 dotted numbers   - first two represent the network part, and the last two are the host part   - if the network part of two ip addresses match, then they are on the same ip network   - assigned by machine (DHCP) or human</p> <p><code>default gateway</code>  is an IP address on a local network, which packets are forwarded to, if the intended IP location is not local</p> <p><code>Subnet Mask</code> : allow an IP device to know if an IP address it's trying to communicate with is on the same network or not</p> <ul> <li>if the IP address is not local, then it forwards the packets to a <code>default gateway</code></li> </ul> <p><code>routing</code> : the process where packets hop across the internet tp get to a destination from a source, through <code>router</code> devices</p> <p><code>route tables</code> : enables routing. - has destination IP's mapped to Traget IP's - routers compare destination IP and route table - routers then send the packet to the target IP that best matches against the destination IP</p> <p>local default route 0.0.0.0/0 sends all packets to isp</p> <p><code>Address Resolution Protocol (ARP)</code> - a process that runs between layer 3 and layer 2 - used to find the MAC address if devices in a different IP address</p>"},{"location":"AWS/networking_01/#layer-4-transport","title":"Layer 4 : Transport","text":"<ul> <li>TCP and UDP run on top of IP networks, TCP is more reliable while UDP is faster</li> </ul> <p><code>TCP</code> - container for data. - contained within packets</p> <p><code>Source and destination port</code> - gives the ability to have multiple streams of conversation at the same time between two devices</p> <p><code>sequence number</code> - unique - incremented with each segment that is sent - can be used for error correction, or to maintain order - is a way of uniquely identifying a particular segment of a particular connection so that both sides can make observations about it (using acknowledgments)</p> <p><code>acknowledgment</code> - how one side indicates it as received up to and including a certain sequence number - every segment transmitted needs to be acknowledged</p> <p><code>flags</code> - flags are used to close the connection or synchronize sequence numbers</p> <p><code>window</code> - flow control, lets receiver control the rate the sender sends data - defines the number of bytes a device is willing to receive between acknowledgements - once reached the send will pause until devices acknowledges that amount of data</p> <p><code>checksum</code> - error checking, and arrange for retransmission of data</p> <p><code>urgent pointer</code> - used to define the priority of traffic, ie specific data within a segment is urgent and should be prioritized</p> <p><code>3-way handshake</code> - client sends its sequences to server - server sends message acknowledging client sequence and its own server sequence - client acknowledges server sequence</p> <p><code>Sessions and State</code> - a stateless firewall (NACL on AWS) has two rules   - one outbound and one inbound rule - a stateful firewall sees on thing   - allowing an outbound implicitly allows the inbound response</p>"},{"location":"AWS/networking_01/#network-address-translation-nat","title":"Network Address Translation (NAT)","text":"<ul> <li>designed to overcome IPv4 shortages and some security benefit</li> <li>translates private IPv4 addresses to Public</li> </ul> <p>IPv4 addresses are either publicly routeable or fall within the private address space</p> <p><code>Static NAT</code> - 1 private to 1 fixed public address (<code>IGW on aws</code>) - used when private IP needs access to the internet and needs a consistent public IP - the router (NAT device) maintains a NAT table, that maps <code>privateIP:PublicIP</code></p> <p><code>Dynamic NAT</code> -  pool of public IP addresses to use, which are temporarily allocated as needed    -  external access can fail if if the pool is exhausted -  occurs when you have less public than private addresses</p> <p><code>Port Address Translation (PAT)</code> - many private address to 1 public address (<code>NATGW on aws</code>)   - commonly experience in home or office networks where private IPv4 addresses are translated to a single public address, allowing outgoing internet access. - uses ports to identify individual devices</p> <ol> <li>local device has a privateIP and a private source port</li> <li>sends a packet to a router</li> <li>router has a nat table that maps <code>privateIP : publicIP</code> and <code>privatePort : publicPort</code>, and routes the packet</li> <li>the response, reaches the routers, get translated using the NAT table, then get sent to the correct local device</li> </ol>"},{"location":"AWS/networking_01/#subnetting","title":"Subnetting","text":"<ul> <li>splitting a larger down to smaller networks by splitting the range of IP's available</li> </ul>"},{"location":"AWS/networking_01/#distributed-denial-of-service-ddos","title":"Distributed Denial of Service (DDos)","text":"<ul> <li> <p>attack designed to <code>overload</code> websites</p> </li> <li> <p><code>Application</code> layer - HTTP Flood</p> </li> <li>exploit that requests are cheap but responses are computationally expensive for servers </li> <li>many bots send request to the same endpoint</li> <li><code>Protocol</code> Attack - SYN Flood</li> <li>bots send syns (connection initiations) with spoof IPs, network will be hung trying to connect with the fake IP's</li> <li><code>Volumetric</code> - DNS Amplification</li> <li>exploit a response where a response is significantly larger than the request</li> </ul>"},{"location":"AWS/networking_01/#secure-sockets-layer-ssl-and-transport-layer-security-tls","title":"Secure Sockets Layer (SSL) and Transport Layer Security (TLS)","text":"<ul> <li>TLS is newer version of SSL</li> <li>privacy and data integrity between client and server</li> </ul> <p><code>privacy</code> - communications b/w client and server are encrypted - starts with <code>asymmetric</code> to <code>symmetric</code>encryption</p> <p><code>identity</code> - verify client and server</p> <p><code>reliable</code> - protect against data alteration</p> <ol> <li>Verify the identity of the server</li> <li>negotiate an encryption method to use</li> <li>exchange asymmetric for symmetric encryption keys</li> <li>initiate connection</li> </ol>"},{"location":"AWS/sharedResponsibiltyModel/","title":"Shared Responsibility Model","text":"<ul> <li><code>Customer</code> is responsible for security <code>in the cloud</code></li> <li>specific os, data integrity, networking, identity, and configurations</li> <li><code>AWS</code> is responsibility for security <code>of the cloud</code></li> <li>usually the physical infrastructure aws manages</li> </ul>"},{"location":"AWS/yaml/","title":"Yaml","text":""},{"location":"AWS/yaml/#yaml","title":"YAML","text":"<p>Unordered collection of <code>key : value</code> pairs</p> <p>Supported data types:</p> <pre><code>string : red\ninteger: 12\nfloat: 3.14\nboolean : true\nnothing : null\nlist1 : [\"red\", \"green\", \"blue\"]\nlist2:\n    - red\n    - blue\n    - red\n# dictionary\n- trainer :\n  name : ash\n  region : kanto\n  town : pallet\n</code></pre>"},{"location":"AWS/CCP/","title":"Notes For Certified Cloud Practitioner","text":""},{"location":"AWS/CCP/Notes/","title":"Notes","text":"<ul> <li>Cloud Concepts</li> <li>What is Cloud Computing</li> <li>Types of Hosting</li> <li>What Is A Cloud Service Provider (CSP)</li> <li>Evolution of Computing</li> <li>Types of Cloud Computing</li> <li>Cloud Computing Deployment Models</li> <li>The Benefits Of The Cloud</li> <li>Seven Advantages to Cloud</li> <li>AWS Global Infrastructure</li> <li>Regions</li> <li>Availability Zones</li> <li>Fault Tolerance</li> <li>AWS Global Network</li> <li>Points Of Presence (PoP)</li> <li>AWS Direct Connect</li> <li>Local Zones</li> <li>Wavelength Zones</li> <li>Data Residency</li> <li>Gov Cloud</li> <li>AWS China</li> <li>AWS Ground Station</li> <li>Cloud Architecture</li> <li>Cloud Architecture Terminologies</li> <li>High Availability</li> <li>High Scalability</li> <li>High Elasticity</li> <li>Fault Tolerance</li> <li>High Durability</li> <li>Business Continuity Plan (BCP)</li> <li>Disaster Recovery Options</li> <li>Management and Development Tools</li> <li>AWS API</li> <li>AWS Management Console</li> <li>AWS Tools for PowerShell</li> <li>Amazon Resource Name (ARN)</li> <li>AWS Command Line Interface (CLI)</li> <li>AWS Software Development Kit (SDK)</li> <li>AWS CLoudShell</li> <li>Infrastructure as Code (IaC)</li> <li>AWS CloudFormation</li> <li>AWS Cloud Development Kit</li> <li>AWS Toolkit for VSCode</li> <li>Access Keys</li> <li>Shared Responsibility Model</li> <li>AWS Shared Responsibility Model</li> <li>Compute</li> <li>VM's, Container, and Serverless</li> <li>High PErformance Computing (HPC)</li> <li>Edge and Hybrid Computing</li> <li>Cost and Capacity Management</li> <li>Storage</li> <li>Types of Storage</li> <li>S3 Storage  Classes</li> <li>AWS Snow Family</li> <li>Storage Services</li> <li>Networking</li> <li>Cloud-Native Networking Services</li> <li>Enterprise/Hybrid Networking Services</li> <li>Databases</li> <li>Data Warehouse</li> <li>NoSQL Database Services</li> <li>Relational Database Services</li> <li>Other Database Services</li> <li>Elastic Compute Cloud (EC2)</li> <li>Introduction to EC2</li> <li>Instance Families</li> <li>Instance Type</li> <li>Dedicated Host vs Dedicated Instance</li> <li>Elastic IP</li> <li>EC2 Pricing Model</li> <li>Reserved Instances</li> <li>Reserved Instances (RI)</li> <li>REgional and Zonal RI</li> <li>RI Limits</li> <li>Capacity Reservations</li> <li>Standard vs Convertible RI</li> <li>Savings Plan</li> <li>Identity</li> <li>Zero-Trust Model</li> <li>Zero-Trust Model on AWS</li> <li>Directory Service</li> <li>Identity Providers (IdP)</li> <li>Single Sign On</li> <li>Lightweight Directory Access Protocol (LDAP)</li> <li>Multi-Factor Authentication</li> <li>Security Keys</li> <li>AWS Identity and Access Management (IAM)</li> <li>Anatomy of an IAM Policy</li> <li>Principle of Least Privilege (PoLP)</li> <li>AWS Account Root User</li> <li>AWS SSO</li> <li>Application Integration</li> <li>Queueing and Simple Queue Service (SQS)</li> <li>Streaming and Kinesis</li> <li>Pub-Sub and SNS</li> <li>API Gateway</li> <li>State Machines</li> <li>Event Bus</li> <li>Other Application Integration Services</li> <li>Containers</li> <li>VM's vs Containers</li> <li>Micro-services</li> <li>Kubernetes (K8's)</li> <li>Docker</li> <li>Container Services</li> <li>Governance</li> <li>Organizations and Accounts</li> <li>AWS Control Tower</li> <li>AWS Config</li> <li>AWS Quick Starts</li> <li>Tagging</li> <li>Resource Groups</li> <li>Business Centric Services</li> <li>Provisioning</li> <li>Provisioning Services</li> <li>AWS Elastic Beanstalk</li> <li>Serverless Services</li> <li>Windows on AWS</li> <li>Logging</li> <li>Anatomy of an Alarm</li> <li>CloudWatch Logs \u2014 Log Streams</li> <li>Log Insights</li> <li>ML and AI and Big Data</li> <li>AI and ML Services</li> <li>Big Data and Analytics Services</li> <li>AWS Well-Architected Framework</li> <li>General Design Principles</li> <li>Anatomy of a Pillar</li> <li>Operational Excellence</li> <li>Security</li> <li>Reliability</li> <li>Performance Efficiency</li> <li>Cost Optimization</li> <li>TCO and Migration</li> <li>Total Cost of Ownership (TCO)</li> <li>Capital Expenditure (CAPEX) vs Operational Expenditure (OPEX)</li> <li>AWS Pricing Calculator</li> <li>Migration Evaluator</li> <li>VM Import / Export</li> <li>Database Migration Service (DMS)</li> <li>Billing, Pricing, and Support</li> <li>AWS Support Plans</li> <li>technical Account Manager</li> <li>AWS Market Marketplace</li> <li>Consolidated Billing</li> <li>AWS Trusted Advisor</li> <li>Service Level Agreements (SLA)</li> <li>Health Dashboard's</li> <li>AWS Abuse</li> <li>AWS Partner Network (APN)</li> <li>AWS Budgets</li> <li>AWS Cost and Usage Reports (CUR)</li> <li>Billing Alarms</li> <li>AWS Cost Explorer</li> <li>Security</li> <li>Defense in Depth</li> <li>Confidentiality, Integrity, Availability (CIA)</li> <li>Encryption</li> <li>Digital Signatures and Signing</li> <li>Pen Testing</li> <li>AWS Artifact</li> <li>AWS Inspector</li> <li>AWS Shield</li> <li>Amazon Guard Duty</li> <li>Amazon Macie</li> <li>AWS VPN</li> <li>AWS Web Application Firewall (WAF)</li> <li>AWS Key Management Service (KMS)</li> <li>CloudHSM</li> <li>Variation Study</li> <li>Know you initialisms</li> <li>AWS Config vs AWS AppConfig</li> <li>SNS vs  SQS</li> <li>SNS vs SES vs PinPoint vs WorkMail</li> <li>AWS Inspector vs AWS Trusted Advisor</li> <li>Connect Names Services</li> <li>AWS Artifact vs Amazon Inspector</li> <li>ELB Variants</li> </ul>"},{"location":"AWS/CCP/Notes/#cloud-concepts","title":"Cloud Concepts","text":""},{"location":"AWS/CCP/Notes/#what-is-cloud-computing","title":"What is Cloud Computing","text":"<ul> <li>The practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server.</li> <li>Cloud providers own the servers, and manages the physical upkeep. </li> <li>You are responsible for configuring only the services and code</li> </ul>"},{"location":"AWS/CCP/Notes/#types-of-hosting","title":"Types of Hosting","text":"<ul> <li><code>Dedicated servers</code>: One physical machine dedicated to a single application</li> <li><code>Virtual Private Server (VPS</code>: One physical machine, virtualized into submachine to run multiple applications.</li> <li><code>Shared Hosting</code>: One physical machine shared by hundred of businesses.</li> <li><code>Cloud Hosting</code>: Multiple physical machines shared by thousands of businesses and services.</li> </ul>"},{"location":"AWS/CCP/Notes/#what-is-a-cloud-service-provider-csp","title":"What Is A Cloud Service Provider (CSP)","text":"<ul> <li>Provides multiple cloud services which can be chained together to create cloud architectures</li> <li><code>compute</code>: a virtual computer to run applications</li> <li><code>networking</code>: to define internet connections or network isolation</li> <li><code>storage</code>: virtual hard-drive to store files</li> <li><code>databases</code>: virtual databases for storing and reporting data</li> </ul>"},{"location":"AWS/CCP/Notes/#evolution-of-computing","title":"Evolution of Computing","text":"<ul> <li><code>Dedicated</code>:</li> <li>a physical server wholly utilized by a single customer</li> <li>guaranteed full utility of underlying resources</li> <li>can't vertically scale, must migrate</li> <li>will overpay for underutilized server</li> <li>limited by host OS</li> <li>multiple apps running can lead to resource sharing conflicts</li> <li><code>Virtual Machine's</code>:</li> <li>can run multiple virtual machines on one machine</li> <li>physical server shared by multiple customers</li> <li>pay a for a fraction of server</li> <li>overpaying for underutilized server</li> <li>limited by host OS</li> <li>multiple apps running can lead to resource sharing conflicts</li> <li>easy to export or import images for migration</li> <li>easy to vertically or horizontally scale</li> <li><code>\u200bHypervisor\u200b</code> is the software layer that lets you run the VMs\u200b</li> <li><code>Containers</code>:</li> <li>Multiple applications can run side by side without OS conflicts</li> <li>More efficient than virtual machines</li> <li>\u200b<code>Docker Deamon\u200b</code> is the name of the software layer that lets you run multiple conta## iners.\u200b</li> <li><code>Functions</code>:</li> <li>managed VM's running managed containers</li> <li>known as serverless compute</li> <li>your only responsibility is code and data</li> <li>very cost effective, only pay for time code is running</li> <li>cold start side effect</li> </ul>"},{"location":"AWS/CCP/Notes/#types-of-cloud-computing","title":"Types of Cloud Computing","text":"<ul> <li><code>SaaS</code>: Software as a service</li> <li>Completed project managed by a service provider</li> <li>Gmail</li> <li><code>PaaS</code>: Platform as a service</li> <li>provide platform to develop, run, and manage applications without the complications of maintaining infrastructure</li> <li>AWS Elastic Beanstalk</li> <li><code>Iaas</code>: Infrastructure as a service</li> <li>Most basic building blocks of cloud IT infrastructure.</li> <li>most flexibility and management control</li> <li>AWS, Azure, GCP</li> </ul>"},{"location":"AWS/CCP/Notes/#cloud-computing-deployment-models","title":"Cloud Computing Deployment Models","text":"<ul> <li>Cloud: Everything built and runs on the CSP</li> <li>On Premise: Everything built on company's data centers</li> <li>Hybrid: Both on Cloud and On Premise being used</li> <li>Cross Cloud: Using more than one CSP</li> </ul>"},{"location":"AWS/CCP/Notes/#the-benefits-of-the-cloud","title":"The Benefits Of The Cloud","text":""},{"location":"AWS/CCP/Notes/#seven-advantages-to-cloud","title":"Seven Advantages to Cloud","text":"<ol> <li><code>Cost-Effective</code>: Pay as you go with thousands of customers sharing the cost of the resources</li> <li><code>Global</code>: Launch workloads anywhere in the world</li> <li><code>Secure</code>: Cloud provider takes care of physical security</li> <li><code>Reliable</code>: Data backup, disaster recovery, data replication, fault tolerance</li> <li><code>Scalable</code>: Increase or decrease resources based on demand</li> <li><code>Elastic</code>: Automate scaling during spikes and drop in demand</li> <li><code>Current</code>: Underlying hardware and software are managed without interruption to you</li> </ol>"},{"location":"AWS/CCP/Notes/#aws-global-infrastructure","title":"AWS Global Infrastructure","text":""},{"location":"AWS/CCP/Notes/#regions","title":"Regions","text":"<p>A geographically distinct location which has multiple data-centers (AZs).</p> <ul> <li>Each region has at least two availability zones</li> <li>not all services are available in all region</li> </ul>"},{"location":"AWS/CCP/Notes/#availability-zones","title":"Availability Zones","text":"<p>A physical location made up of one or more data centers.</p> <p>The use of AZ's give customers the ability to operate production applications and databases that are more: - Highly available - Fault tolerant - Scalable</p> <p>A subnet is associated with an availability zone. - Launch AWS resource into a specific subnet to launch the resource into a specific AZ</p>"},{"location":"AWS/CCP/Notes/#fault-tolerance","title":"Fault Tolerance","text":"<p><code>Fault Domain</code>: A section of a network that is vulnerable to damage if critical device or system fails. - if a failure occurs, it will not cascade outside that domain</p> <p><code>Fault Level</code>: A collection of fault domains</p> <p>If an application is partitioned across AZs, companies are better protected from issues such as power outages, storms, etc.</p>"},{"location":"AWS/CCP/Notes/#aws-global-network","title":"AWS Global Network","text":"<p>The AWS Global Network represents the interconnections between AWS Global Infrastructure. - <code>Edge Locations</code> can act as on and off ramps to the AGN   - <code>AWS Global Accelerator</code> and <code>AWS S3 Transfer Acceleration</code> use edge locations to quickly reach AWS resources in other regions by traversing the AGN   - Amazon CloudFront (CDN) uses edge locations to provide edge storage and compute near the end user   - <code>VPC Endpoints</code> ensure your resources stay within the AWS Network and do not traverse over the public cloud</p>"},{"location":"AWS/CCP/Notes/#points-of-presence-pop","title":"Points Of Presence (PoP)","text":"<p>Intermediate locations between AWS Region and the end user.   - For AWS this is a data center utilized for content delivery or faster upload   - <code>Edge locations</code> are data centers that hold caches of the most popular files, so that delivery of distance to the end users are reduced   - <code>Regional Edge Cache</code> are data centers that hold much larger caches of less-popular files to reduce full round trip and cost of transfer fees   - <code>AWS CloudFront</code> is a Content Delivery Network (CDN) service that:     - routes requests to the nearest edge location cache     - allows to choose an origin to be the source of cache     - caches the contents of what origin would be returned to various edge locations</p> <ul> <li><code>Tier 1 network</code>: is a network that can reach every other network on the internet without purchasing IP transit or paying for peering</li> <li>aws availability zones are all redundantly connected to multiple tier-1 transit providers</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-direct-connect","title":"AWS Direct Connect","text":"<p>AWS Direct Connect is a private/dedicated connection between an on-prem data center and AWS. - Two very-fast network connection options   - lower 50-500MBps,  higher 1-10GBps - Helps reduce network costs and increase bandwidth throughput - Direct Connect Locations are trusted third party data centers that establish the connection</p>"},{"location":"AWS/CCP/Notes/#local-zones","title":"Local Zones","text":"<p>Data centers located close to densely populated areas to provide single-digit millisecond low latency performance - used to support highly-demanding applications sensitive to latency</p>"},{"location":"AWS/CCP/Notes/#wavelength-zones","title":"Wavelength Zones","text":"<ul> <li>Allow for edge-computing on 5G networks, by partnering with telecoms</li> <li>ultra low latency, as close as possible to user</li> <li>create subnets tied to a wavelength zone, to be able to launch virtual machines to the edge of the targeted networks</li> </ul>"},{"location":"AWS/CCP/Notes/#data-residency","title":"Data Residency","text":"<p>The physical or geographical location of where an organization or cloud resources reside.</p> <ul> <li><code>Compliance Boundaries</code>: A regulatory compliance that describes where data and cloud resources area allowed to reside</li> <li><code>Data Sovereignty</code>: Legal authority that can be asserted over data because its physical location is within the jurisdictional boundaries</li> </ul> <p>For workloads that need to meet compliance boundaries strictly. - <code>AWS Config</code>:    - policy as code service   - create rules to continuously check aws resource configuration - <code>IAM Polices</code>:   - can be written to explicitly deny access to specific aws regions - <code>AWS Outposts</code>:   - physical rack of servers that you can put your data in</p>"},{"location":"AWS/CCP/Notes/#gov-cloud","title":"Gov Cloud","text":"<p>Regions that allow customers to host sensitive and regulated workloads - currently only in the US</p>"},{"location":"AWS/CCP/Notes/#aws-china","title":"AWS China","text":"<p>AWS cloud offering in mainland china. Intentionally isolated from AWS global.</p>"},{"location":"AWS/CCP/Notes/#aws-ground-station","title":"AWS Ground Station","text":"<p>Fully managed service that lets you control satellite communications</p>"},{"location":"AWS/CCP/Notes/#cloud-architecture","title":"Cloud Architecture","text":""},{"location":"AWS/CCP/Notes/#cloud-architecture-terminologies","title":"Cloud Architecture Terminologies","text":"<ul> <li><code>Availability</code>: Ability to ensure service remains available</li> <li><code>Scalability</code>: Ability to grow and unimpeded</li> <li><code>Elasticity</code>: Ability automatically to shrink and grow to meet the demand</li> <li><code>Fault Tolerance</code>: Ability to prevent a failure</li> <li><code>Disaster Recovery</code>: Ability to recover from a failure</li> </ul>"},{"location":"AWS/CCP/Notes/#high-availability","title":"High Availability","text":"<p>Ensure service remains available - no single point of failure</p> <p>Can be achieved by running workloads across multiple availability zones.</p> <p><code>Elastic Load Balancer</code>: - A load balancer allows you to evenly distribute traffic to multiple servers in one or more data center. - routes traffic towards health data centers and servers</p>"},{"location":"AWS/CCP/Notes/#high-scalability","title":"High Scalability","text":"<p>Ability to increase your capacity based on the increasing demand of traffic, memory and computing power</p> <ul> <li><code>Vertical Scaling</code> (Scaling Up): Upgrading to bigger server</li> <li><code>Horizontal Scaling</code> (Scaling Out): Add more servers of the same size</li> </ul>"},{"location":"AWS/CCP/Notes/#high-elasticity","title":"High Elasticity","text":"<p>Ability to <code>automatically</code> increase your capacity based on the increasing demand of traffic, memory and computing power</p> <ul> <li>relies on horizontal scaling</li> <li>Scaling out (add more servers of same size)</li> <li>Scaling In (remove servers of same size)</li> </ul> <p><code>Auto Scaling Groups</code> (ASG) - AWS feature that will automatically add or remove servers based on scaling rules you define</p>"},{"location":"AWS/CCP/Notes/#fault-tolerance_1","title":"Fault Tolerance","text":"<p>Ability for your service to ensure there is no single point of failure.</p> <ul> <li><code>Fail-over</code>: plans to shift traffic to a redundant system in case the primary system fails</li> </ul> <p><code>RDS Multi-AZ</code>: - run duplicate standby database in another availability zone in case the primary one fails</p>"},{"location":"AWS/CCP/Notes/#high-durability","title":"High Durability","text":"<p>Ability to recover from a disaster and to prevent data loss during recovery</p> <p><code>CloudEndure Disaster Recovery</code>: - Continuously replicates your machines into a low-cost staging area in your target aws account and preferred region enabling fast and reliable recovery.</p>"},{"location":"AWS/CCP/Notes/#business-continuity-plan-bcp","title":"Business Continuity Plan (BCP)","text":"<p>A document that outlines how a business will continue to operate during an unplanned disruption in services.</p> <ul> <li><code>Recover Point Objective</code> (RPO)</li> <li>maximum acceptable amount of data loss after an unplanned data-loss incident</li> <li>how much data are you willing to lose</li> <li><code>Recovery Time Objective</code> (RTO)</li> <li>maximum amount of time your business can tolerate without incurring a significant financial loss</li> <li>how much time are you willing to go down</li> </ul>"},{"location":"AWS/CCP/Notes/#disaster-recovery-options","title":"Disaster Recovery Options","text":"<p>Coldest to Warmest</p> <ul> <li><code>Backup &amp; Restore</code></li> <li>rpo/rto hours</li> <li>back up your data and restore it to new infrastructure</li> <li><code>Pilot Light</code></li> <li>rpo/rto 10 mins</li> <li>data is replicated to another region with minimal services running</li> <li><code>Warm Standby</code></li> <li>rpo/rto</li> <li>scaled down copy of your infrastructure running ready to scale up</li> <li><code>Multi-site Active</code></li> <li>rpo/rto</li> <li>scaled up copy of your infrastructure in another region</li> </ul>"},{"location":"AWS/CCP/Notes/#management-and-development-tools","title":"Management and Development Tools","text":""},{"location":"AWS/CCP/Notes/#aws-api","title":"AWS API","text":"<p>Each aws service has its own service endpoint which you can send a request to. - The api endpoints are usually accessed through the cli, sdk, or console</p>"},{"location":"AWS/CCP/Notes/#aws-management-console","title":"AWS Management Console","text":"<p>A web based unified console. - build, manage, and monitor everything - point and click (clickops)</p>"},{"location":"AWS/CCP/Notes/#aws-tools-for-powershell","title":"AWS Tools for PowerShell","text":"<p>Lets you interact with the aws api via powershell cmdlets - Powershell is A task automation and configuration management framework.</p>"},{"location":"AWS/CCP/Notes/#amazon-resource-name-arn","title":"Amazon Resource Name (ARN)","text":"<p>Uniquely identify aws resources.</p>"},{"location":"AWS/CCP/Notes/#aws-command-line-interface-cli","title":"AWS Command Line Interface (CLI)","text":"<p>Allows users to programmatically interact with the aws api via entering commands into a shell.</p>"},{"location":"AWS/CCP/Notes/#aws-software-development-kit-sdk","title":"AWS Software Development Kit (SDK)","text":"<p>Collection of software development tools in one installable package. - used to programmatically create, modify, delete, or interact ith aws resources.</p>"},{"location":"AWS/CCP/Notes/#aws-cloudshell","title":"AWS CLoudShell","text":"<p>Browser based shell built into the aws management console. - free, scoped per region, same credentials as logged in user</p>"},{"location":"AWS/CCP/Notes/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p>Configuration script to automate creating, updating, or destroying cloud infrastructure. - a blueprint of your cloud infrastructure</p> <p>AWS CloudFormation - JSON or YAML</p> <p>AWS Cloud Development Kit - Code</p>"},{"location":"AWS/CCP/Notes/#aws-cloudformation","title":"AWS CloudFormation","text":"<p>Allows you to write IaC as either JSON or YAML. - simple, but can lead to large files</p>"},{"location":"AWS/CCP/Notes/#aws-cloud-development-kit","title":"AWS Cloud Development Kit","text":"<p>Allows you to use a programming language to write IaC. - generate CloudFormation templates as an output - comes with its own cli - cdk pipelines to quickly set up ci/cd - has a testing framework</p> <p>AWS SDK looks similar, but the key difference is CDK ensures Idempotent of your Infrastructure. - cdk wont create new resources if it hasn't been updated</p>"},{"location":"AWS/CCP/Notes/#aws-toolkit-for-vscode","title":"AWS Toolkit for VSCode","text":"<p>Open source for VSCode to create, debug, deploy aws resources. - explore aws resources and stacks - helps create SAM applications</p>"},{"location":"AWS/CCP/Notes/#access-keys","title":"Access Keys","text":"<p>A key and secret required to have programmatic access to aws resources when interacting with the aws api outside of the management console.</p>"},{"location":"AWS/CCP/Notes/#shared-responsibility-model","title":"Shared Responsibility Model","text":"<p>A cloud security framework that defines the security obligations of the customer versus the cloud service provider (csp).</p>"},{"location":"AWS/CCP/Notes/#aws-shared-responsibility-model","title":"AWS Shared Responsibility Model","text":"<p>Customer is responsible for security in the cloud. - data - configuration</p> <p>AWS is responsible for security of the cloud. - hardware - operation of managed services - global infrastructure</p>"},{"location":"AWS/CCP/Notes/#compute","title":"Compute","text":""},{"location":"AWS/CCP/Notes/#vms-container-and-serverless","title":"VM's, Container, and Serverless","text":"<p>Virtual Machines - <code>Elastic Compute Cloud (EC2)</code>   - allows you to launch a virtual machine in the cloud.   - a virtual machine is an emulation of a physical computer using software   - <code>Amazon Machine Image (AMI)</code>     - a predefined configuration for a virtual machine - <code>Amazon LightSail</code>   - managed virtual server services   - friendlier version of EC2</p> <p>Containers - <code>Elastic Container Service (ECS)</code>   - container orchestration service   - when you need docker as a service - <code>Elastic Container Registry (ECR)</code>   - repository for container images - <code>ECS Fargate</code>   - serverless container orchestration service   - same as ECS, but aws manages the server, pay on demand - <code>Elastic Kubernetes Service (EKS)</code>   - fully managed kubernetes service</p> <p>Serverless - <code>AWS Lambda</code>   - serverless functions service   - fully managed by aws, customer only uploads code and configurations   - pay based on runtime</p>"},{"location":"AWS/CCP/Notes/#high-performance-computing-hpc","title":"High PErformance Computing (HPC)","text":"<p>USually for machine learning workloads</p> <p><code>AWS ParallelCluster</code> - cluster management tool - make it easier to deploy and manage high performance computing clusters on aws</p>"},{"location":"AWS/CCP/Notes/#edge-and-hybrid-computing","title":"Edge and Hybrid Computing","text":"<p><code>Edge Computing</code> - pushing your computing workloads outside of your network to run closer to destination location</p> <p><code>Hybrid Computing</code> - running workloads on both on-premise data center and aws virtual private cloud (VPC)</p> <p><code>AWS Outposts</code> - physical rack of servers that can be put in your data centers - allow to user aws api and service in your data center</p> <p><code>AWS Wavelength</code> - build and launch application in a telecom data center - ultra low latency - closest as possible to end user</p> <p><code>VMWare Cloud on AWS</code> - manage on-premise virtual machines using VMWare as EC2 instances</p> <p><code>AWS Local zones</code> - edge data centers located outside of an aws region - when you need faster computing, storage, and databases in populated areas outside of aws region</p>"},{"location":"AWS/CCP/Notes/#cost-and-capacity-management","title":"Cost and Capacity Management","text":"<p><code>EC2 Spot Instances, Reserved Instances, and Savings Plan</code> - save on computing by paying upfront, committing to yearly contract or by being flexible about availability and interruption to computing service</p> <p><code>AWS Batch</code> - plans, schedules, and executes batch computing workloads - can utilize spot instances to save money</p> <p><code>AWS Compute Optimizer</code> - uses machine learning to analyze usage - suggests how to reduce costs and improve performance</p> <p><code>EC2 Autoscaling Groups (ASG)</code> - automatically add or remove EC2 servers based on current traffic</p> <p><code>Elastic Load Balancer (ELB)</code> - distribute traffic to multiple machines - re-route traffic from unhealthy instances to healthy instances or different availability zones</p> <p><code>AWS Elastic Beanstalk (EB)</code> - for easily deploying web applications - PaaS</p>"},{"location":"AWS/CCP/Notes/#storage","title":"Storage","text":""},{"location":"AWS/CCP/Notes/#types-of-storage","title":"Types of Storage","text":"<p><code>Elastic Block Storage (EBS)</code> - when you need a virtual hard drive attached to a vm - only a single write volume</p> <p><code>Elastic File Storage (EFS)</code> - when you need a file-share where multiple vms need to access the same drive - multiple connections via network share</p> <p><code>Simple Storage Service (S3)</code> - multiple reads and writes - max object size 5TB, unlimited numbe rof objects</p>"},{"location":"AWS/CCP/Notes/#s3-storage-classes","title":"S3 Storage  Classes","text":"<p><code>S3 Standard</code> - fast - highly available and durable - replicated across 3 AZ's</p> <p><code>S3 Intelligent Tier</code> - use ML to analyze object usage and determine the appropriate storage class</p> <p><code>S3 Standard-IA (Infrequent Access)</code> - Cheaper if you access files less than once a month</p> <p><code>S3 One-Zone-IA</code> - objects only exist in one AZ</p> <p><code>S3 Glacier</code> - for long term cold storage - retrieval can take minutes to hours - very cheap</p> <p><code>S3 Glacier Deep Archive</code> - lowest cost storage class - data retrieval 12 hours</p>"},{"location":"AWS/CCP/Notes/#aws-snow-family","title":"AWS Snow Family","text":"<p>Storage and compute devices used to physically move data in or out the cloud when moving data over the internet is to slow, difficult, or costly.</p> <p><code>Snowcone</code> - comes in two sizes: 8TB, 14TB</p> <p><code>Snowball Edge</code> - comes in two types   - storage optimized     - 80TB   - compute optimized     - 39.5 TB</p> <p><code>Snowmobile</code> - 100PB</p>"},{"location":"AWS/CCP/Notes/#storage-services","title":"Storage Services","text":"<ul> <li><code>S3 (Simple Storage service)</code></li> <li>object storage service</li> <li>industry-leading scalability, data availability, security, and performance</li> <li><code>S3 Glacier</code></li> <li>low cost storage fir archiving and long-term backup</li> <li>longer data retrieval wait time</li> <li><code>Elastic Block Storage (EBS)</code></li> <li>virtual hard drive you attach to EC2 instances</li> <li>Elastic File Storage (EFS)</li> <li>file storage mountable ot multiple EC2 instances at the same time</li> <li><code>Storage Gateway</code></li> <li>hybrid cloud storage with local caching</li> <li>expand in-premise storage capacity into the cloud</li> <li><code>File Gateway</code></li> <li>extend local storage to S3</li> <li><code>Volume Gateway</code></li> <li>caches local drives to S3, so you have a continuous back up of local files in the cloud</li> <li><code>Tape Gateway</code></li> <li>stores files onto virtual tapes for backing up on very cost effective long term storage</li> <li><code>AWS Snowfamily</code></li> <li>storage devices used to physically migrate large amounts of data to the cloud</li> <li><code>AWS Backup</code></li> <li>fully managed backup service</li> <li><code>CloudEndure Disaster Recovery</code></li> <li>continuously replicates your machines into a low-cost staging area in your target aws account and preferred region</li> <li>enabling ast and reliable recovery in case of failures</li> </ul>"},{"location":"AWS/CCP/Notes/#networking","title":"Networking","text":""},{"location":"AWS/CCP/Notes/#cloud-native-networking-services","title":"Cloud-Native Networking Services","text":"<ul> <li><code>Region</code></li> <li>geographical location of your network</li> <li><code>Availability Zone (AZ)</code></li> <li>data center containing aws resources</li> <li><code>Virtual Private Cloud (VPC)</code></li> <li>isolated section of aws cloud where you can launch aws resources</li> <li><code>Internet Gateway</code></li> <li>enables access to the internet for you VPC</li> <li><code>Route Tables</code></li> <li>determines where network traffic from your subnets are directed</li> <li><code>Network Access Control List (NACL)</code></li> <li>act as a firewall at the subnet level</li> <li>create Allow and Deny rules.</li> <li><code>Security Groups</code></li> <li>act as a firewall at the instance level</li> <li>only Allow rules.</li> <li><code>Subnets</code></li> <li>partition of an IP network intro multiple, smaller network segments</li> <li>Public subnets are generally used for placing resources which are accessible on the internet</li> <li>Private subnets are used when you need resources to be more secured and only accessible through tightly filtered traffic into the subnet</li> </ul>"},{"location":"AWS/CCP/Notes/#enterprisehybrid-networking-services","title":"Enterprise/Hybrid Networking Services","text":"<ul> <li><code>Direct Connect</code></li> <li>dedicated gigabit network connection from you premises location to aws</li> <li><code>AWS Virtual Private Network (VPN)</code></li> <li>establish secure connection to aws network</li> <li>site-to-site: on-prem to aws</li> <li>client vpn: user laptop to aws</li> <li><code>PrivateLinks (VPC Interface Endpoints)</code></li> <li>keeps traffic within the aws network and not traverse the internet to keep traffic secure</li> </ul>"},{"location":"AWS/CCP/Notes/#databases","title":"Databases","text":""},{"location":"AWS/CCP/Notes/#data-warehouse","title":"Data Warehouse","text":"<p>A relational database designed for <code>analytic workloads</code>. - generally perform aggregation, such as grouping - optimized around columns since they need to aggregate column data - designed to be HOT, return results from queries very fast - infrequently accessed - consume data from a relational database on a regular basis</p>"},{"location":"AWS/CCP/Notes/#nosql-database-services","title":"NoSQL Database Services","text":"<ul> <li><code>DynamoDB</code></li> <li>serverless, NoSQL, key/value, and document database</li> <li>designed to scale to billions of records</li> <li>guaranteed consistent data returns within a second</li> <li><code>DocumentDB</code></li> <li>NosQL document database that is MongoDB compatible</li> <li><code>Amazon Keyspaces</code></li> <li>fully managed apache cassandra database</li> </ul>"},{"location":"AWS/CCP/Notes/#relational-database-services","title":"Relational Database Services","text":"<ul> <li><code>Relational Database Service (RDS)</code></li> <li>supports common sql engines<ul> <li>mysql, mariadb, postgres, oracle, microsoft sql server</li> </ul> </li> <li><code>Aurora</code></li> <li>fully managed, of either MySQL or PostgreSQL</li> <li><code>Aurora Serverless</code></li> <li>serverless on demand version of aurora</li> <li>when you want most of aurora functionality but can trade to have cold starts</li> <li><code>RDS on VMWare</code></li> <li>deploy rds managed databases to on premise data center</li> </ul>"},{"location":"AWS/CCP/Notes/#other-database-services","title":"Other Database Services","text":"<ul> <li><code>Redshift</code></li> <li>petabyte size data warehouse</li> <li><code>ElastiCache</code></li> <li>managed database of in memory and caching open-source</li> <li>when you need to improve the performance of applications by adding a caching layer in front of the web server or database</li> <li><code>Neptune</code></li> <li>graph database</li> <li>when you need to understand the connections between data</li> <li><code>Amazon Timestreams</code></li> <li>fully managed time series database</li> <li>when you need to measure how things change over time</li> <li><code>Amazon Quantum Ledger</code></li> <li>fully managed ledger database that provides transparent, immutable, and cryptographically variable transaction logs</li> <li><code>Database Migration Service (DMS)</code></li> <li>on-premise database to aws</li> <li>from two databases using different sql engines</li> <li>from sql to nosql</li> </ul>"},{"location":"AWS/CCP/Notes/#elastic-compute-cloud-ec2","title":"Elastic Compute Cloud (EC2)","text":""},{"location":"AWS/CCP/Notes/#introduction-to-ec2","title":"Introduction to EC2","text":"<p>A highly configurable virtual machine.</p> <p>Steps - choose os - choose instance type - add storage (EBS, EFS) - configure instance   - Security Groups, Key Pairs, UserData, IAM Roles, Placement Groups</p>"},{"location":"AWS/CCP/Notes/#instance-families","title":"Instance Families","text":"<p>Different combinations of cpu, memory, storage, and network capacity. - allows you to choose the appropriate combination of capacity to meet your applications unique requirements</p> <p>Family Names - <code>General Purpose</code>   - balance of compute, memory, and network resources   - web servers and code repositories - <code>Compute optimized</code>   - high performance processor   - scientific modeling, gaming servers - <code>Memory Optimized</code>   - fast performance for workloads that process large data sets in memory   - real time big data analytics, in-memory caches - <code>Accelerated Optimized</code>   - hardware accelerators or co-processors   - machine learning, computational finance - <code>Storage Optimized</code>   - high sequential read and write access to very large data sets on local storage   - NoSQL, data warehousing</p>"},{"location":"AWS/CCP/Notes/#instance-type","title":"Instance Type","text":"<p>A particular instance size and instance family</p> <ul> <li>EC2 Instance Sizes generally double in price and key attributes</li> </ul>"},{"location":"AWS/CCP/Notes/#dedicated-host-vs-dedicated-instance","title":"Dedicated Host vs Dedicated Instance","text":"<ul> <li><code>Dedicated Host</code></li> <li>physical server isolation</li> <li>socket, cores, host ID visibility</li> <li>more expensive</li> <li><code>Dedicated Instance</code></li> <li>instance isolation via virtualization</li> <li>virtual server always lives on the same part of the physical server</li> <li><code>Default</code></li> <li>vm instance lives in a specific part of physical server until a reboot</li> </ul>"},{"location":"AWS/CCP/Notes/#elastic-ip","title":"Elastic IP","text":"<p>A way to configure static IP address for virtual machines.</p>"},{"location":"AWS/CCP/Notes/#ec2-pricing-model","title":"EC2 Pricing Model","text":"<ul> <li><code>on-Demand</code></li> <li>least commitment</li> <li>low cost and flexible</li> <li>pay per hour</li> <li>short term, unpredictable workloads</li> <li>cannot be interrupted</li> <li><code>Reserved</code></li> <li>best long term saving, upt to 75%</li> <li>commit to 1 or 3 years</li> <li>can resell unused reserved instances</li> <li>the greater upfront the great the savings</li> <li><code>Spot</code></li> <li>biggest savings, up to 90%</li> <li>request spare computing capacity</li> <li>flexible, can handle interruptions</li> <li><code>Dedicated</code></li> <li>most costly</li> <li>can be on demand or reserved (up to 75%)</li> </ul>"},{"location":"AWS/CCP/Notes/#reserved-instances","title":"Reserved Instances","text":"<p>Designed for applications that have a steady-state, predictable usage, or require reserved capacity.</p> <p><code>Class</code> : the less flexible the greater the savings - Standard   - up 75% reduced pricing compared to on-demand   - can modify ri attributes - Convertible   - up to 54% reduced pricing compared to on-demand   - can exchange ri attributes</p> <p><code>Payment Options</code>:The greater upfront the greater the savings - All upfront - Partial upfront - No upfront</p>"},{"location":"AWS/CCP/Notes/#reserved-instances-ri","title":"Reserved Instances (RI)","text":"<p>Also known as instance attributes. </p> <ul> <li><code>instance type</code></li> <li>instance family + instance size</li> <li><code>region</code></li> <li><code>tenancy</code></li> <li>whether your instance runs on shared or single tenant hardware</li> <li><code>platform</code></li> <li>the operating system of the machine</li> </ul>"},{"location":"AWS/CCP/Notes/#regional-and-zonal-ri","title":"REgional and Zonal RI","text":"<p>When you purchase a RI, you determine the scope of the reserved instance. The scope does not affect price.</p> <p><code>Regional RI</code>: Purchase for a region - does no reserve capacity - discount applies to instances in any AZ in the region - discount can apply to instance within the instance family - can purchase regional ri</p> <p><code>Zonal RI</code>: Purchase fo an availability zone - reserves capacity in the specified availability zone - discount only applies to instance in the selected AZ - no instance size instance size flexibility - can't purchase zonal ri</p>"},{"location":"AWS/CCP/Notes/#ri-limits","title":"RI Limits","text":"<p>Per-month you can purchase - 20 regional reserved instances per region   - cannot be used to exceed on-demand instance limit (20) - 20 zonal reserved instances per AZ   - can exceed on-demand instance limit</p>"},{"location":"AWS/CCP/Notes/#capacity-reservations","title":"Capacity Reservations","text":"<p>Request a reservation of a specific EC2 instance type or family.</p>"},{"location":"AWS/CCP/Notes/#standard-vs-convertible-ri","title":"Standard vs Convertible RI","text":"<ul> <li><code>Standard</code> </li> <li>can be modified</li> <li>changed az within same region, scope from zonal to regional, instance size, network</li> <li><code>Convertible</code><ul> <li>can be exchanged for another convertible ri with new ri attributes including</li> <li>instance family, instance type, platform, scope, tenancy</li> </ul> </li> </ul>"},{"location":"AWS/CCP/Notes/#savings-plan","title":"Savings Plan","text":"<p>Offers similar discounts as reserved instances but simplifies the purchasing process. - two different terms ( 1 year or 3 years) - <code>payment options</code>   - all upfront   - partial upfront   - no upfront - <code>Types</code>   - <code>compute</code>     - most flexibility, reduce cost by up to 66%     - automatically applies to EC2, fargate, and lambda services regardless of instance family, size, AZ, region, OS, or tenancy.   - <code>EC2 Instances</code>     - lowest prices, 72% savings     - commitment to usage of instance families in a region     - automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy   - <code>SageMaker</code>     - helps reduce sagemaker costs by up to 64%</p>"},{"location":"AWS/CCP/Notes/#identity","title":"Identity","text":""},{"location":"AWS/CCP/Notes/#zero-trust-model","title":"Zero-Trust Model","text":"<p><code>trust no one, verify everything</code></p> <p>In the zero trust model, identity becomes the primary security perimeter. - first line of defense</p> <p>Network Centric - focused on VPN's and firewalls Identity Centric - persons scope of access limited by there identity</p>"},{"location":"AWS/CCP/Notes/#zero-trust-model-on-aws","title":"Zero-Trust Model on AWS","text":"<p>Identity and Access Management (IAM) - <code>IAM Polices</code>: set of permissions detailing a users access to services - <code>Permission Boundaries</code>: what permission someone should not have - <code>Service Control Polices</code>: Organization-wide permissions - <code>IAM Policy Conditions</code>: more fine grain control of policy</p> <p>AWS does not have ready-to-use identity controls that are intelligent, which is why AWS is considered to not have a true Zero Trust offering for customers, and third-party services need to be used.</p> <p>A collection of AWS Services can be setup to intelligent-ish detection of identity concerns but requires expert knowledge - <code>AWS CloudTrail</code>: track all API Calls - <code>Amazon GuardDuty</code>: detect suspicious activity based on CloudTrail and other logs - <code>Amazon Detective</code>: investigate and quickly identify security issues, an ingest data from GuardDuty</p> <p>Third Party Services: - Azure Active Directory (Azure AD) - Google BeyondCorp - JumpCloud</p>"},{"location":"AWS/CCP/Notes/#directory-service","title":"Directory Service","text":"<p>A directory service maps the names of network resources to their network addresses. - shared information infrastructure for locating, managing, administering, and organizing resources   - volumes, folders, files, users, groups...</p> <p>A directory server is a server that provides a directory service. - each resource on the network is considered an object by the directory server - information about a particular resource is stored as a collection of attributes associated with that resource or object</p>"},{"location":"AWS/CCP/Notes/#identity-providers-idp","title":"Identity Providers (IdP)","text":"<p>A system that creates, maintains, and manages information for principals and also provides authentication services to applications within a federated or distributed network. - A trusted provider of your user identity that lets you use to authenticate to access other services - Federated identity is a method of linking a user's identity across multiple separate identity management systems\u200b</p> <ul> <li><code>OpenID</code></li> <li>open standard and decentralized <code>authentication</code> protocol</li> <li>allows you to login into different social media platforms using a google or facebook account</li> <li>who you are</li> <li><code>OAuth2.0</code></li> <li>uses <code>authorization</code> tokens to prove an identity between consumers and a service provider</li> <li>grant access to functionality</li> <li><code>Security Assertion Markup Language (SAML)</code></li> <li>open standard for authentication and authorization between an identity provider and a service provider</li> <li>single-sign-on</li> </ul>"},{"location":"AWS/CCP/Notes/#single-sign-on","title":"Single Sign On","text":"<p>An authentication scheme that allows a user to log in with a single ID and password to different systems and software</p>"},{"location":"AWS/CCP/Notes/#lightweight-directory-access-protocol-ldap","title":"Lightweight Directory Access Protocol (LDAP)","text":"<p>An open, vender-neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an internet protocol (IP) - provide a central place to store usernames and passwords - enables same-sign-on   - allows the same userID and password to be used for multiple applications, but they have to enter it in every time they want to login.\u200b</p> <p>Why use LDAP when SSO is more convenient?\u200b - Most SSO systems are using LDAP.\u200b - LDAP was not designed natively to work with web applications.\u200b - Some systems only support integration with LDAP and not SSO\u200b</p>"},{"location":"AWS/CCP/Notes/#multi-factor-authentication","title":"Multi-Factor Authentication","text":"<p>A security control where after you fill in your username and password, you have to use a second device such as a phone to confirm that it's you logging in.</p>"},{"location":"AWS/CCP/Notes/#security-keys","title":"Security Keys","text":"<p>A secondary device that can be used during multi-factor authentication. Resembling a memory stick with a button, that when pressed will autofill a security token.</p>"},{"location":"AWS/CCP/Notes/#aws-identity-and-access-management-iam","title":"AWS Identity and Access Management (IAM)","text":"<p>Allows you to create and manages AWS users and groups, and use permissions to allow and deny their access to AWS resources.</p> <ul> <li><code>IAM policies</code></li> <li>document that grant permissions for a specific user, group, or role to access services</li> <li>attached to IAM identities</li> <li><code>IAM Permission</code></li> <li>the API actions that can or cannot be performed</li> <li>represented in the IAM policy document</li> <li><code>IAM Users</code></li> <li>End users who interact with aws resources</li> <li><code>IAM Groups</code></li> <li>Group of users that share same permissions</li> <li><code>IAM Roles</code></li> <li>grant aws resources permissions to specific aws API actions</li> <li>associate policies to a role and the assign it to an aws resource</li> </ul>"},{"location":"AWS/CCP/Notes/#anatomy-of-an-iam-policy","title":"Anatomy of an IAM Policy","text":"<p>IAM Policies are written in JSON, and contain the permissions which determine what API actions are allowed or denied.</p> <ul> <li><code>Version of policy language</code></li> <li>2012-10-17 is the latest version</li> <li><code>Statement Container</code></li> <li>for policy element, allowed to have multiple</li> <li><code>Sid</code></li> <li>a way to label your statements</li> <li><code>Effect</code></li> <li>whether the policy will Allow or Deny</li> <li><code>Action</code></li> <li>list of actions that the policy allows or denies</li> <li><code>Principal</code></li> <li>account, user, or role which would like to allow or deny access</li> <li><code>resource</code></li> <li>the resource to which the actions apply</li> <li><code>condition</code></li> <li>circumstances under which the policy grants permission</li> </ul>"},{"location":"AWS/CCP/Notes/#principle-of-least-privilege-polp","title":"Principle of Least Privilege (PoLP)","text":"<p>Computer security concept of providing a usr, role, or application the least amount of permissions to perform an operation or action</p> <ul> <li><code>Just-Enough-Access (JEA)</code></li> <li>permitting only the exact actions for the identity to perform its task</li> <li><code>Just-In-Time (JIT)</code></li> <li>permitting smallest length of duration an identity can  use permissions</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-account-root-user","title":"AWS Account Root User","text":"<ul> <li><code>AWS Account</code></li> <li>the account which holds all your aws resources</li> <li><code>Root User</code></li> <li>special account that cannot be deleted</li> <li><code>User</code></li> <li>a user for common tasks that are assigned permissions</li> </ul> <p>Root Users can: - change account settings - close aws account - change or cancel aws support plan</p>"},{"location":"AWS/CCP/Notes/#aws-sso","title":"AWS SSO","text":"<p>AWS Single Sign-On (AWS SSO) is where you create or connect, your workforce identities in AWS once and manage access centrally across your AWS organization.</p> <p>Choose your Identity Source - AWS SSO, Active Directory, SAML 2.0 IdP Managed User Permissions Centrally - AWS Account, AWS Applications, SAML Applications</p> <p>Uses get Single Click Access</p>"},{"location":"AWS/CCP/Notes/#application-integration","title":"Application Integration","text":"<p>The process of letting two independent applications to communicate and work with each other. - commonly facilitated by an intermediate system</p>"},{"location":"AWS/CCP/Notes/#queueing-and-simple-queue-service-sqs","title":"Queueing and Simple Queue Service (SQS)","text":"<ul> <li>Simple Queue Service (SQS)</li> <li>Used to provide asynchronous communication and decouple processes via messages/events.</li> <li>delete messages once they are consumed</li> <li>not real time</li> <li>have to pull</li> <li>simple communication</li> <li>fully managed queuing service that enables you to decouple and scale microservice's, distributed systems, and serverless applications</li> </ul>"},{"location":"AWS/CCP/Notes/#streaming-and-kinesis","title":"Streaming and Kinesis","text":"<ul> <li>Amazon Kinesis</li> <li>events live in the stream for long periods of time, so complex operations can be applied</li> <li>real time</li> <li>fully managed solution for collecting, processing, and analyzing streaming data in the cloud</li> </ul>"},{"location":"AWS/CCP/Notes/#pub-sub-and-sns","title":"Pub-Sub and SNS","text":"<p>Publish-Subscribe pattern commonly implemented in messaging systems.</p> <ul> <li>Simple Notification Service (SNS)</li> <li>Publishers send their messages to an event bus</li> <li>event bus categorizes the messages into groups</li> <li>receivers subscribe to the groups</li> <li>messages are sent to subscribers</li> </ul>"},{"location":"AWS/CCP/Notes/#api-gateway","title":"API Gateway","text":"<ul> <li>Amazon API Gateway is a solution for creating secure APIs in your cloud environment at any scale.</li> <li>A program that sits between a single-entry point and multiple backends.</li> <li>Allows for throttling, logging, routing logic or formatting of the request and responses</li> <li>Create APIs that act as a front door for applications to access data, business logic, or functionality from back-end services.</li> </ul>"},{"location":"AWS/CCP/Notes/#state-machines","title":"State Machines","text":"<p>An abstract model which decides how one state moves to another based on a series of conditions.</p> <ul> <li>AWS Step Functions</li> <li>coordinate multiple aws services into a serverless workflow</li> <li>a graphical console to visualize the components of your application as a series of steps</li> <li>automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected</li> <li>logs the state of each step, so when things go wrong you can diagnose and debug problem quickly</li> </ul>"},{"location":"AWS/CCP/Notes/#event-bus","title":"Event Bus","text":"<p>Receives events from a source and routes events to a target based on rules.</p> <ul> <li>EventBridge</li> <li>serverless event bus service that is used for application integration by streaming real-time data to you applications</li> <li>types<ul> <li>default event bus for every account</li> <li>custom event bus, scoped to multiple accounts or other aws accounts</li> <li>SaaS event bus, scoped to third party SaaS providers</li> </ul> </li> <li>producers: aws services that emit events</li> <li>partner sources: third party apps that can emit events</li> <li>rules: define what event to capture and pass to targets</li> <li>targets: aws services that consume events</li> <li>events: data emitted by services, JSON objects</li> </ul>"},{"location":"AWS/CCP/Notes/#other-application-integration-services","title":"Other Application Integration Services","text":"<ul> <li>Amazon MQ</li> <li>is a managed message broker service that uses Apache ActiveMQ</li> <li>Managed Kafka Service (MSK) </li> <li>fully managed Apache Kafka service</li> <li>for building real-time streaming data pipelines and applications. Similar to Kinesis but more robust</li> <li>AppSync </li> <li>fully managed GraphQL service</li> </ul>"},{"location":"AWS/CCP/Notes/#containers","title":"Containers","text":""},{"location":"AWS/CCP/Notes/#vms-vs-containers","title":"VM's vs Containers","text":"<p>VMS do not make the best use of space.  - apps are not isolated which could cause config conflicts, security problems, or resource hogging</p> <p>Containers allow you to run multiple apps which are virtually isolated from each other - configure os and dependencies per container</p>"},{"location":"AWS/CCP/Notes/#micro-services","title":"Micro-services","text":"<ul> <li><code>Monolithic architecture</code></li> <li>one app which is responsible for everything</li> <li>functionality is tightly coupled</li> <li><code>Micro-services architecture</code></li> <li>multiple apps, each responsible for one thing</li> <li>functionality is isolated and stateless</li> </ul>"},{"location":"AWS/CCP/Notes/#kubernetes-k8s","title":"Kubernetes (K8's)","text":"<p>Open source container orchestration system for automating deployment, scaling, and management of containers. - advantage of kubernetes over docker is the ability to run containers distributed across multiple VM's - ideally for micro-service architectures where a company has <code>tens to hundreds of services</code> they need to manage - a unique component of kuberneetes are <code>pods</code>   - a pod is a group of one or more containers with shared storage, network resources, and other shared settings</p>"},{"location":"AWS/CCP/Notes/#docker","title":"Docker","text":"<p>PaaS product that use OS-level virtualization to deliver software in packages called containers. - <code>dokcer cli</code>: cli commands to download, upload, build, run, and debug containers - <code>dockerfile</code>: a configuration file on how to provision a container - <code>docker compose</code>: tool and configuration file when working with multiple containers - <code>docker swarm</code>: orchestration tool for managing deployed multi-container architectures - <code>dockerhub</code>: public online repository for containers published by the community for download - <code>The Open Container Initiative (OCI)</code>:  an open governance structure for creating open industry standards around container formats and runtime.</p> <p>Podman, Buildah and Skopeo - <code>podman</code>: is a container engine that is OCI-compliant and is a deopin replacement for docker   - daemon-less, allows you to create pods like K8 - <code>buildah</code>: is a tool used to build OCI images - <code>Skopeo</code>: tool for moving container images between different types of container storages</p>"},{"location":"AWS/CCP/Notes/#container-services","title":"Container Services","text":"<p>Primary Services: - <code>Elastic Container Service (ECS)</code>   - no cold starts   - self managed EC2 - <code>AWS Fargate</code>   - more robust than lambda   - scale to zero cost   - aws managed EC2 - <code>Elastic Kubernetes Service (EKS)</code>   - open source, avoid vendor lock-in - <code>AWS Lambda</code>   - only think about code   - short running tasks   - can deploy custom containers</p> <p>Provisioning and Deployment - <code>Elastic Beanstalk (EB)</code>   - ECS on training wheels, PaaS - <code>App Runner</code>   - PaaS specifically for containers - <code>AWS Copilot CLI</code>   - build, release, and operate production ready containerized applications on aws app runner, ECS, and aws Fargate</p> <p>Supporting Services - <code>Elastic Container Registry (ECR)</code>   - repos for your docer images - <code>X-Ray</code>   - analyze and debug between micro-services - <code>Step Functions</code>   - stitch together lambdas and ECS tasks</p>"},{"location":"AWS/CCP/Notes/#governance","title":"Governance","text":""},{"location":"AWS/CCP/Notes/#organizations-and-accounts","title":"Organizations and Accounts","text":"<ul> <li><code>Organizations</code></li> <li>allow the creation of new aws accounts, centrally managed billing, control access, compliance, security, and shared resources across your aws accounts</li> <li><code>Root Account User</code></li> <li>a single sign-in identity that has complete access to all aws services and resources in an account</li> <li><code>Organization Units</code></li> <li>a group of aws accounts within an organization which can also contain other organization units, creating a hierarchy</li> <li><code>Service Control Policy</code></li> <li>control over the allowed permission for all accounts within an organization</li> </ul> <p>Key Points - more than one account managed through a single account using Organizations - setup consolidated billing where 1 account pays the AWS bill for all - create isolated AWS accounts for different teams under the payer account - and place them inside Organizational Units (OU)   - allows you to set customized permission boundaries on the accounts using Service Control Policies (SCPs)</p>"},{"location":"AWS/CCP/Notes/#aws-control-tower","title":"AWS Control Tower","text":"<p>Helps enterprises quickly set-up a secure, aws multi-account.</p> <ul> <li><code>Landing Zones</code></li> <li>baseline environment following best practice to quickly start production ready workloads</li> <li><code>Account Factory</code></li> <li>automates provisioning of new accounts in your organization</li> <li>standardize the provisioning of new accounts with pre-approved account configurations</li> <li><code>Guardrails</code></li> <li>pre-packaged governance rules for security, operations, and compliance that can be applied to enter-prise wide or to specific groups</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-config","title":"AWS Config","text":"<p>Compliance as Code (CaC) framework that allows us to manage change in aws accounts on a per region basis - change management is a formal process to   - monitor changes   - enforce changes   - remediate changes</p> <p><code>AWS Config</code> - ensure a resource is configured a certain way - keep track of configuration changes - list of all resources in a region</p>"},{"location":"AWS/CCP/Notes/#aws-quick-starts","title":"AWS Quick Starts","text":"<p>Prebuilt templates by aws and aws partners to help deploy stacks - reference architecture for deployment - aws cloudformation templates that automate and configure the deployment - a deployment and implementation guide</p>"},{"location":"AWS/CCP/Notes/#tagging","title":"Tagging","text":"<p>A key and value pair that you can assign to aws resources. Can be used to organize resources: - resource management - cost management - operations management - security - governance and regulatory compliance</p>"},{"location":"AWS/CCP/Notes/#resource-groups","title":"Resource Groups","text":"<p>A collection of resources that share one or more tags - Helps you organize and consolidate information based on your project and the resources that you use. - Resource Groups can display details about a group of resource based on   - Metrics   - Alarms    - Configuration Settings</p> <p>Resource Groups appears in the Global Console Header and Under Systems Manager</p>"},{"location":"AWS/CCP/Notes/#business-centric-services","title":"Business Centric Services","text":"<p>-<code>Amazon Connect</code>   - virtual call center - <code>WorkSpaces</code>   - virtual remote desktop service - <code>WorkDocs</code>   - shared collaboration service - <code>Chime</code>   - platform for online meetings - <code>WorkMail</code>    - - Managed business email, contacts, and calendar service with support for existing desktop and mobile email client applications - <code>Pinpoint</code>   - Marketing campaign management system you can use for sending targeted email, SMS, push notifications, and voice messages. - <code>Simple Email Service</code>   - email sending service designed fro marketers and application developers to send marketing, notifications, and emails. - <code>QuickSight</code>   -  A Business Intelligence (BI) service. Connect multiple datasource and quickly visualize data in the form of graphs</p>"},{"location":"AWS/CCP/Notes/#provisioning","title":"Provisioning","text":"<p>The allocation or creation of resources and services to a customer.</p>"},{"location":"AWS/CCP/Notes/#provisioning-services","title":"Provisioning Services","text":"<ul> <li><code>Elastic Beanstalk</code></li> <li>an easy-to-use service for deploying and scaling web applications</li> <li><code>OpWorks</code> </li> <li>configuration management service that provides managed instances of Chef and Puppet</li> <li><code>CloudFormation</code> </li> <li>lets you deploy your cloud resources using infrastructure-as-code with JSON and YAML templates</li> <li><code>AWS QuickStarts</code></li> <li>pre-made packages that can launch and configure your AWS compute, network, storage, and other services</li> <li><code>AWS Marketplace</code></li> <li>a digital catalogue containing thousands of software listings</li> <li><code>AWS Amplify</code></li> <li>is a mobile and web application framework, that will provision multiple AWS services as your backend.\u200b</li> <li><code>AWS App Runner</code></li> <li>a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs</li> <li><code>AWS Copilot</code> </li> <li>AWS Copilot is a command-line interface (CLI) that enables customers to quickly launch and easily manage containerized applications on AWS</li> <li><code>AWS CodeStar</code> </li> <li>provides a unified user interface, enabling you to easily manage your software development activities in one place</li> <li><code>AWS Cloud Development Kit (CDK)</code></li> <li>an Infrastructure as Code (IaC) tool.</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-elastic-beanstalk","title":"AWS Elastic Beanstalk","text":"<p>PaaS allowing customer to develop, run, and manage applications without the complexity of building and maintaining the infrastructure -  powered by a <code>CloudFormation</code> template and setups   - Elastic Load Balancer   - Autoscaling Groups   - RDS Database   - EC2 Instance preconfigured (or custom ) platforms   - Monitoring (CloudWatch, SNS)   - In-Place and Blue/Green deployment methodologies   - Security (Rotates passwords)   - Can run Dockerized environments</p>"},{"location":"AWS/CCP/Notes/#serverless-services","title":"Serverless Services","text":"<p>Serverless architecture generally describes fully managed cloud services.</p> <p>A serverless service could have all or most of the following characteristics: - Highly elastic and scalable - Highly available - Highly durable - Secure by default</p> <p>Abstracts away the underlying infrastructure and are billed based on the execution of your business task.</p> <p>Serverless can Scale-to-Zero meaning when not in use the serverless resources cost nothing.</p> <ul> <li><code>DynamoDB</code> </li> <li>is a serverless NoSQL key/value and document database</li> <li><code>Simple Storage Service (S3)</code></li> <li>is a serverless object storage service.</li> <li><code>ECS Fargate</code></li> <li>is serverless orchestration container service.</li> <li><code>AWS Lambda</code></li> <li>is a serverless functions service. </li> <li><code>Step Functions</code></li> <li>is a state machine service</li> <li><code>Aurora Serverless</code></li> <li>is the serverless on-demand version of Aurora.</li> </ul>"},{"location":"AWS/CCP/Notes/#windows-on-aws","title":"Windows on AWS","text":"<ul> <li><code>Windows Servers on EC2</code></li> <li><code>SQL Server on RDS</code></li> <li><code>AWS Directory Service</code></li> <li>lets you run Microsoft Active Directory (AD) as a managed service\u200b</li> <li><code>AWS License Manager</code></li> <li>makes it easier to manage your software licenses from software vendors such as Microsoft.\u200b</li> <li><code>Amazon FSx for Windows File Server</code></li> <li>is a fully managed scalable storage built for Windows.\u200b</li> <li><code>AWS Software Development Kit (SDK)</code></li> <li>has .NET support</li> <li><code>Amazon WorkSpaces</code></li> <li>allows you to run a Windows virtual desktop</li> <li><code>AWS Migration Acceleration Program (MAP)</code></li> <li>migration methodology from moving large enterprise</li> </ul>"},{"location":"AWS/CCP/Notes/#logging","title":"Logging","text":"<ul> <li><code>CloudTrail</code></li> <li>logs all api calls between various aws services</li> <li>used to answer questions of who created/launched/configured a <ul> <li><code>Where</code> : Source IP Address</li> <li><code>When</code> : EventTime</li> <li><code>Who</code> : User, UserAgent</li> <li><code>What</code> : Region, Resource, Actionresources</li> </ul> </li> <li>Trails are output to S3 and do not have GUI like Event History. To analyze a Trail you\u2019d have to use Amazon Athena.</li> <li><code>CloudWatch</code></li> <li>a collection of multiple resources</li> <li><code>logs</code>: performance data about aws services</li> <li><code>metrics</code>: time-ordered set of data points for monitoring purposes<ul> <li>It's a variable that is monitored over time.</li> </ul> </li> <li><code>events</code>: trigger an event based ona  condition</li> <li><code>alarms</code>: trigger notifications based on metrics<ul> <li>Metric Alarm States</li> <li><code>OK</code> The metric or expression is within the defined threshold</li> <li><code>ALARM</code> The metric or expression is outside of the defined threshold</li> <li><code>INSUFFICIENT_DATA</code></li> </ul> </li> <li><code>dashboard</code>: visualize metrics</li> <li><code>AWS X-Ray</code></li> <li>distributed tracing system</li> <li>see how data moves from one app to another, how long it takes to moves, and if failed to move forward</li> </ul>"},{"location":"AWS/CCP/Notes/#anatomy-of-an-alarm","title":"Anatomy of an Alarm","text":"<ul> <li><code>threshold condition</code> : defines when a data point is breached</li> <li><code>evaluation periods</code> : number of previous periods</li> <li><code>data point</code> : metrics measurement at a given time period</li> <li><code>metric</code> : data that is being measured</li> <li><code>period</code> : how often alarm is evaluated</li> <li><code>networkin</code> : volume of incoming network traffic</li> <li><code>datapoints</code> to alarm : \"1 data point is breached in an evaluation period going back 4 periods.\"</li> </ul>"},{"location":"AWS/CCP/Notes/#cloudwatch-logs-log-streams","title":"CloudWatch Logs \u2014 Log Streams","text":"<ul> <li> <p><code>log stream</code> :  represents a sequence of events from an application or instance being monitored.</p> </li> <li> <p><code>log Events</code> : a single event in a log file. Log events can be seen within a Log Stream.</p> </li> </ul>"},{"location":"AWS/CCP/Notes/#log-insights","title":"Log Insights","text":"<p>Enables you interactively search and analyze cloudwatch data - more robust filtering than log streams - less burdensome than having to export logs to s3 an analyze them with athena - has it's own language <code>Query Syntax</code></p>"},{"location":"AWS/CCP/Notes/#ml-and-ai-and-big-data","title":"ML and AI and Big Data","text":""},{"location":"AWS/CCP/Notes/#ai-and-ml-services","title":"AI and ML Services","text":"<ul> <li><code>amazon codeguru</code></li> <li>machine-learning code analysis services, performs code-reviews and will suggest changes to improve quality and performance</li> <li><code>amazon lex</code></li> <li>build text and voice chatbots</li> <li><code>amazon personalize</code></li> <li>real-time recommendations service</li> <li><code>amazon polly</code></li> <li>text-to-speach service</li> <li><code>amazon rekogition</code></li> <li>image and video recognition service, analyze images and videos to detect and label objects</li> <li><code>amazon transcribe</code></li> <li>speech to text service</li> <li><code>amazon textract</code></li> <li>extract data from scanned documents</li> <li><code>amazon translate</code></li> <li><code>amazon comprehend</code></li> <li>nlp service</li> <li><code>amazon forecast</code></li> <li>time series forecasting service</li> <li><code>aws deep learning ami's</code></li> <li>ec2 instances pre-installed with popular deep learning frameworks</li> <li><code>aws deep learning containers</code></li> <li>docker images pre-installed with popular deep learning frameworks</li> <li><code>amazon fraud detector</code></li> <li>fraud detection service</li> <li><code>amazon kendra</code></li> <li>search engine</li> <li><code>amazon sagemaker</code></li> <li>build ml solutions end to end</li> </ul>"},{"location":"AWS/CCP/Notes/#big-data-and-analytics-services","title":"Big Data and Analytics Services","text":"<ul> <li><code>athena</code></li> <li>serverless query service</li> <li>takes csv and json files and temporarily loads them into sql tables</li> <li><code>opensearch</code></li> <li>full-text search engine</li> <li><code>elastic map reduce (EMR)</code></li> <li>data processing and analysis</li> <li>transform unstructured data into structured data on the fly</li> <li><code>kinesis data streams</code></li> <li>ream time streaming data service</li> <li><code>kinesis firehose</code></li> <li>simpler version of data streams</li> <li><code>kinesis data analytics</code></li> <li>run queries against data that is flowing real time</li> <li><code>kinesis video streams</code></li> <li>analyze or process real-time video streams</li> <li><code>managed kafka service (MSK)</code></li> <li>real-time streaming data pipelines</li> <li>more robust version of kinesis</li> <li><code>redshift</code></li> <li>petabyte sized data warehouse</li> <li><code>quicksight</code></li> <li>bi tool to build dashboards</li> <li><code>data pipelines</code></li> <li>reliably move data between compute and storage services</li> <li><code>glue</code></li> <li>ETL service</li> <li><code>lake formation</code></li> <li>centralized, curated, and secured repository that stores all your data</li> <li><code>data exchange</code></li> <li>catalog of third-party datasets. You can download for free</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-well-architected-framework","title":"AWS Well-Architected Framework","text":"<p>A whitepaper created by awws to help customers build using best practices.</p> <p>The framework is divided into 5 6 sections called pillars - <code>Operational Excellence</code> : run and monitor systems - <code>Security</code> : protect data and systems, mitigate risk - <code>Reliability</code> : mitigate and recover from disruptions - <code>Performance Efficiency</code> : use compute resources effectively - <code>Cost Optimization</code> : get lowest price</p> <p>General definitions - <code>Component</code> : code, configuration, and aws resource against a requirement - <code>Workload</code> : a set of components that work together to deliver business value - <code>Milestones</code> : key changes of your architecture through product life cycle - <code>Architecture</code> : how components work together in a workload - <code>Technology Portfolio</code> : a collection of workloads required for the business to operate</p>"},{"location":"AWS/CCP/Notes/#general-design-principles","title":"General Design Principles","text":"<ul> <li><code>Stop guessing your capacity needs</code></li> <li>cloud computing lets you use as little or as much depending on demand</li> <li><code>Test systems at production scale</code></li> <li>clone production env to testing, teardown when testing not in use to save money</li> <li><code>Automate to make architectural experimentation easier</code></li> <li>cloudformation, changesets stackupdate, and drift detection</li> <li><code>Allow for evolutionary architectures</code></li> <li>ci/cd</li> <li><code>Drive architecture using data</code></li> <li>cloudwatch, cloud trail</li> <li><code>Improve through game days</code></li> <li>simulate traffic on production or purposely kill ec2 to test recovery</li> </ul>"},{"location":"AWS/CCP/Notes/#anatomy-of-a-pillar","title":"Anatomy of a Pillar","text":"<ul> <li><code>Design principles</code></li> <li>a list of principles that need to be considered during implementation</li> <li><code>Definition</code></li> <li>overview of the best practice categories</li> <li><code>Best Practices</code></li> <li>detailed information about each best practice with aws services</li> <li><code>Resources</code></li> <li>additional documentation, whitepapers, and videos to implement this pillar</li> </ul>"},{"location":"AWS/CCP/Notes/#operational-excellence","title":"Operational Excellence","text":"<ul> <li><code>Perform operations as code</code></li> <li>eg. infrastructure as code</li> <li><code>Make frequent, small, reversible changes</code></li> <li>design workloads to allow components to be updated regularly</li> <li>eg. ci/cd</li> <li><code>Refine operations procedures frequently</code></li> <li>look for continuous opportunities to improve your operations</li> <li>eg. simulate traffic or event failure on your production workloads</li> <li><code>Anticipate failure</code></li> <li>write test code, kill servers to rest recovery</li> <li><code>Learn from all operational failures</code></li> </ul>"},{"location":"AWS/CCP/Notes/#security","title":"Security","text":"<ul> <li><code>Implement a strong identity foundation</code></li> <li>Principle of least privilege</li> <li><code>Enable traceability</code></li> <li>monitor alert and audit actions, integrate log and metric collection, automate investigation and remediation</li> <li><code>Apply security at all layers</code></li> <li>eg. Edge Network, VPC, Load Balancing Instances, OS, Application Code</li> <li><code>Automate security best practices</code></li> <li><code>Protect data in transit and at rest</code></li> <li><code>Keep people away from data</code></li> <li><code>Prepare for security events</code></li> </ul>"},{"location":"AWS/CCP/Notes/#reliability","title":"Reliability","text":"<ul> <li><code>Automatically recovver from failure</code></li> <li>monitor kpi's and trigger automation when a threshold is breached</li> <li><code>Test recovery procedures</code></li> <li>test how workload fails, validate recovery procedures</li> <li><code>Scale horizontally to increase availability</code></li> <li>replace one large resource with multiple small resources to reduce impact of a single failure</li> <li><code>Stop guessing capacity</code></li> <li><code>Manage change in automation</code></li> <li>make changes via infrastructure as code</li> </ul>"},{"location":"AWS/CCP/Notes/#performance-efficiency","title":"Performance Efficiency","text":"<ul> <li><code>Democratize advanced technologies</code></li> <li>Take advantage of advanced technology specialized and optimized for your use-case with on-demand cloud services.</li> <li><code>Go global in minutes</code></li> <li><code>Use serverless architectures</code></li> <li>Removes the operational burden of managing physical servers, and can lower transactional costs</li> <li><code>Experiment more often</code></li> <li><code>Consider mechanical sympathy</code></li> <li>Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals</li> </ul>"},{"location":"AWS/CCP/Notes/#cost-optimization","title":"Cost Optimization","text":"<ul> <li><code>Implement cloud financial management</code></li> <li><code>Adopt a consumption model</code></li> <li>Pay only for the computing resources that you require</li> <li><code>Measure overall efficiency</code></li> <li>Measure the business output of the workload and the costs associated with delivering it. Use this measure to know the gains you make from increasing output and reducing costs.</li> <li><code>Stop spending money on undifferentiated heavy lifting</code></li> <li>AWS does the heavy lifting of data center operations</li> <li><code>Analyze and attribute expenditure</code></li> <li>accurately identify the usage and cost of systems, transparent attribution of IT costs to individual workload owners.</li> <li>measure return on investment (ROI) and gives workload owners an opportunity to optimize their resources and reduce costs.</li> </ul>"},{"location":"AWS/CCP/Notes/#tco-and-migration","title":"TCO and Migration","text":""},{"location":"AWS/CCP/Notes/#total-cost-of-ownership-tco","title":"Total Cost of Ownership (TCO)","text":"<p>Financial estimate intended to help buyers and owners determine the direct and indirect costs of a product or service. - useful when your company is looking to migrate from on=premise to cloud</p>"},{"location":"AWS/CCP/Notes/#capital-expenditure-capex-vs-operational-expenditure-opex","title":"Capital Expenditure (CAPEX) vs Operational Expenditure (OPEX)","text":"<ul> <li><code>CAPEX</code></li> <li>spending money on physical infrastructure</li> <li>deducting that expense from your tax bill</li> <li>eg. servers, storage, networking</li> <li><code>OPEX</code></li> <li>costs associated with an on-premises data center that has shifted the cost to the service provider</li> <li>non-physical costs</li> <li>eg. leasing software, training employees, cloud support</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-pricing-calculator","title":"AWS Pricing Calculator","text":"<p>Free cost estimate tool.</p> <p>To calculate the Total Cost of Ownership an organization needs to compare their existing cost against the AWS costs.</p>"},{"location":"AWS/CCP/Notes/#migration-evaluator","title":"Migration Evaluator","text":"<p>Estimating tool used to determine an organization existing on-premise cost so it can be compared against aws costs.</p>"},{"location":"AWS/CCP/Notes/#vm-import-export","title":"VM Import / Export","text":"<p>Used to import virtual machine images into EC2. - upload virtual image to S3 - use aws cli to import image to generate an AMI to import into EC2</p>"},{"location":"AWS/CCP/Notes/#database-migration-service-dms","title":"Database Migration Service (DMS)","text":"<p><code>AWS Database Migration Service (DMS)</code> - allows you to quickly and securely migrate one database to another - can be used to migrate on-premise database to aws</p> <p><code>AWS Schema Conversion Tool</code> - automatically convert a source database schema to a target database schema</p>"},{"location":"AWS/CCP/Notes/#billing-pricing-and-support","title":"Billing, Pricing, and Support","text":""},{"location":"AWS/CCP/Notes/#aws-support-plans","title":"AWS Support Plans","text":"<ul> <li><code>Basic</code></li> <li>Email support only</li> <li>for billing and account</li> <li>7 trusted advisor checks</li> <li><code>Developer : 29$/m</code></li> <li>ech support via email ~24 hours until reply</li> <li>general guidance &lt; 24hrs</li> <li>system impaired &lt; 24hrs</li> <li><code>Business : 100$/m</code></li> <li>production system impaired &lt; 4hrs</li> <li>production system down &lt; 1hr</li> <li>all trusted advisor checks</li> <li><code>Enterprise : 15000$/m</code></li> <li>business critical system down &lt; 15m</li> <li>personal concierge</li> <li>technical account manager</li> </ul>"},{"location":"AWS/CCP/Notes/#technical-account-manager","title":"technical Account Manager","text":"<ul> <li>Build solutions, provide technical guidance and advocate for the customer</li> <li>Ensure AWS environments remain operationally healthy whilst reducing cost and complexity</li> <li>Develop trusting relationships with customers, understanding their business needs and technical challenges</li> <li>Proactively find opportunities for customers to gain additional value from AWS</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-market-marketplace","title":"AWS Market Marketplace","text":"<ul> <li>curated digital catalog with thousands of software listings from independent software vendors.</li> </ul>"},{"location":"AWS/CCP/Notes/#consolidated-billing","title":"Consolidated Billing","text":"<p>Allows you to pay for multiple accounts with one bill. - can be used to get volume discounts across accounts</p> <p><code>Cost Explorer</code> - ca be used to visualize consolidated billing</p>"},{"location":"AWS/CCP/Notes/#aws-trusted-advisor","title":"AWS Trusted Advisor","text":"<p>A recommendation tool that automatically and actively monitors your AWS account to provide actionable recommendations across a series of categories</p> <p>Five categories of AWS Trusted Advisor: - Cost optimization - Performance - Security - Fault toleranceService limits</p>"},{"location":"AWS/CCP/Notes/#service-level-agreements-sla","title":"Service Level Agreements (SLA)","text":"<p>A formal commitment about the expected level of service between a customer and a provider - when service level is not met, and customer meets its obligations, the customer will be eligible to receive</p> <ul> <li><code>Service level indicator (SLI)</code></li> <li>a metric that indicates what measure of performance a customer is receiving at a given time</li> <li><code>Service level objective (SLO)</code></li> <li>the objective that the provider has agreed to meet</li> <li>represented as a specific target percentage over a period of time.</li> </ul>"},{"location":"AWS/CCP/Notes/#health-dashboards","title":"Health Dashboard's","text":"<ul> <li><code>Service Health Dashboard</code></li> <li>shows the general status of AWS services</li> <li><code>Personal Health Dashboard</code></li> <li>provides alerts and guidance for AWS events that might affect your environment.</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-abuse","title":"AWS Abuse","text":"<p>AWS Trust &amp; Safety is a team that specifically deals with abuses occurring on the AWS platform for the following issues: - spam - port scanning - Denial-of-Service (DoS) - Intrusion attempts - Hosting prohibited content - Distributed malware</p>"},{"location":"AWS/CCP/Notes/#aws-partner-network-apn","title":"AWS Partner Network (APN)","text":"<p>A global partner program.</p> <ul> <li>Consulting Partner</li> <li>you help companies utilize AWS</li> <li>Technology Partner</li> <li>you build technology on top of AWS as a service offering</li> <li>A partner belongs to a specific Tier: Select, Advanced, or Premier</li> </ul>"},{"location":"AWS/CCP/Notes/#aws-budgets","title":"AWS Budgets","text":"<p>AWS Budgets give you the ability to set up alerts if you exceed or are approaching your defined budget - can be tracked at the monthly, quarterly, or yearly levels, with customizable start and end dates - support EC2, RDS, Redshift, and ElastiCache reservations.</p> <p><code>AWS Budget Reports</code> - create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budget that will be emailed to specific emails.</p>"},{"location":"AWS/CCP/Notes/#aws-cost-and-usage-reports-cur","title":"AWS Cost and Usage Reports (CUR)","text":"<p>Generate a detailed spreadsheet, enabling you to better analyze and understand your AWS costs - Places the reports into S3 - Use Athena to turn the report into a queryable database - Use QuickSight to visualize your billing data as graphs</p> <p><code>Cost Allocation Tags</code>: - optional metadata that can be attached to AWS resources - can be used to do better analysis with cost and usage reports - user defined or aws generated</p>"},{"location":"AWS/CCP/Notes/#billing-alarms","title":"Billing Alarms","text":"<p>You can create your own Alarms in CloudWatch Alarms to monitor spending. They are commonly called \u201cBilling Alarms\u201d -  more flexible than AWS Budgets and ideal for more complex use-cases</p>"},{"location":"AWS/CCP/Notes/#aws-cost-explorer","title":"AWS Cost Explorer","text":"<p>Lets you visualize, understand, and manage your AWS costs and usage over time. - Specific type range and aggregation - robust filtering  and grouping functionalities - forecasting to get an idea of future costs - monthly or daily level of granularity</p>"},{"location":"AWS/CCP/Notes/#security_1","title":"Security","text":"<p>Vulnerability - A weakness in the application, which can be a design flaw, that allows an attacker to cause harm to the stakeholders of an application</p>"},{"location":"AWS/CCP/Notes/#defense-in-depth","title":"Defense in Depth","text":"<p>The 7 layers of security. 1. <code>Data</code> : access to business or customer data 2. <code>Application</code> : applications are secure and free of vulnerabilities 3. <code>Compute</code> : access to virtual machines, ports 4. <code>Network</code> : limit communication between resoures using segmentation and access controls 5. <code>Perimeter</code> : distributed denial of service (DDoS) protection 6. <code>Identity</code> and access : controlling access to infrastructure and change controls 7. <code>Physical</code> : limit access to a data center only to authorized personal</p>"},{"location":"AWS/CCP/Notes/#confidentiality-integrity-availability-cia","title":"Confidentiality, Integrity, Availability (CIA)","text":"<p>A model describing the foundation of security principles and their trade-off relationship.</p> <ol> <li><code>Confidentiality</code> : pro of data from unauthorized users, using encryption</li> <li><code>Integrity</code> : maintaining and assuring the accuracy and completeness of data over its lifecycle, using ACID compliant databases</li> <li><code>Availability</code> : information needs to be available when needed</li> </ol>"},{"location":"AWS/CCP/Notes/#encryption","title":"Encryption","text":"<p>The process of encoding information using a key and a cypher to store sensitive data.</p> <ul> <li><code>Cypher</code></li> <li>an algorithm that performs the encryption or decryption</li> <li><code>Cryptographic keys</code></li> <li>a variable used in conjunction with a cypher in order to encrypt or decrypt data</li> <li>symmetric encryption : same key is used for encoding and decoding</li> <li>asymmetric encryption : two keys are used</li> <li><code>hashing</code> </li> <li>function that accepts arbitrary size value and maps it to a fixed-size data structure, one-way process and is deterministic</li> <li>used to store passwords</li> <li><code>salting</code> passwords : a random string not known to the attacker that the hash function accepts to mitigate the deterministic nature of hashing functions</li> </ul> <p>Encryption in Transit - data is secure when moving between locations, TLS, SSL</p> <p>Encryption at Rest - data is secure when residing on storage, AES, RSA</p>"},{"location":"AWS/CCP/Notes/#digital-signatures-and-signing","title":"Digital Signatures and Signing","text":"<p>A mathematical scheme for verifying the authenticity of digital messages or documents</p> <p>Three algorithms: 1. <code>key generation</code> - generates a public and private key 2. <code>signing</code> - generating a digital signature with a private key and inputted messages 3. <code>signing verification</code> - verify the authenticity of the message with a public key</p>"},{"location":"AWS/CCP/Notes/#pen-testing","title":"Pen Testing","text":"<p>An authorized simulated cyber-attack on a computer system, performed to evaluate the security of the system.</p>"},{"location":"AWS/CCP/Notes/#aws-artifact","title":"AWS Artifact","text":"<p>A self-serve portal for on-demand access to aws compliance reports.</p>"},{"location":"AWS/CCP/Notes/#aws-inspector","title":"AWS Inspector","text":"<p>AWS Inspector runs a security benchmark against specific EC2 instances.</p> <p><code>Hardening</code> - act of eliminating as many security risks as possible</p>"},{"location":"AWS/CCP/Notes/#aws-shield","title":"AWS Shield","text":"<p>Managed DDoS protection service that safeguards applications running on AWS.</p> <p>Protects you against Layer 3, 4, and 7 attacks</p> <ul> <li>7 Application</li> <li>4 Transport</li> <li>3 Network</li> </ul>"},{"location":"AWS/CCP/Notes/#amazon-guard-duty","title":"Amazon Guard Duty","text":"<p>IDS/IPS - Intrusion Detection System and Intrusion Protection System.</p> <p><code>Guard Duty</code> is a threat detection service that continuously monitors for malicious, suspicious activity and unauthorized behavior.It uses Machine Learning to analyze the following AWS logs: - CloudTrail Logs - VPC Flow Logs - DNS logs</p>"},{"location":"AWS/CCP/Notes/#amazon-macie","title":"Amazon Macie","text":"<p>Macie is a fully managed service that continuously monitors S3 data access activity for anomalies, and generates detailed alerts when it detects the risk of unauthorized access or inadvertent data leaks.</p>"},{"location":"AWS/CCP/Notes/#aws-vpn","title":"AWS VPN","text":"<p>Lets you establish a secure and private tunnel from your network or device to the AWS global network</p> <p><code>AWS Site-to-Site VPN</code> - securely connect on-premises network or branch office site to VPC <code>AWS Client VPN</code> - securely connect users to AWS or on-premises networks</p>"},{"location":"AWS/CCP/Notes/#aws-web-application-firewall-waf","title":"AWS Web Application Firewall (WAF)","text":"<p>Protect your web applications from common web exploits - Write your own rules to ALLOW or DENY traffic based on the contents of HTTP requests - can be attached to either CloudFront or an Application Load Balancer</p>"},{"location":"AWS/CCP/Notes/#aws-key-management-service-kms","title":"AWS Key Management Service (KMS)","text":"<p>A managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. - KMS uses Envelope Encryption. <code>[KMS Master Key] encrypts \u2192 [Envelope Data Key] encrypts \u2192 [Data]</code></p>"},{"location":"AWS/CCP/Notes/#cloudhsm","title":"CloudHSM","text":"<p>Enables you to generate and use your encryption keys on FIPS 140-2 Level 3 validated hardware. - usually used when enterprise needs to meet compliance</p>"},{"location":"AWS/CCP/Notes/#variation-study","title":"Variation Study","text":""},{"location":"AWS/CCP/Notes/#know-you-initialisms","title":"Know you initialisms","text":"<p><code>IAM</code> Identity and Access Management <code>S3</code> Simple Storage Service <code>SWF</code> Simple Workflow Service <code>SNS</code> Simple Notification Service <code>SQS</code> Simple Queue Service <code>SES</code> Simple Email Service <code>SSM</code> Simple Systems Manager <code>RDS</code> Relational Database Service <code>VPC</code> Virtual Private Cloud <code>VPN</code> Virtual Private Network <code>CFN</code> CloudFormation <code>WAF</code> Web Application Firewall <code>MQ</code> Amazon ActiveMQ <code>ASG</code> Auto Scaling Groups <code>TAM</code> Technical Account Manager <code>ELB</code> Elastic Load Balancer <code>ALB</code> Application Load Balancer <code>NLB</code> Network Load Balancer <code>GWLB</code> Gateway Load Balancer <code>CLB</code> Classic Load Balancer <code>EC2</code> Elastic Cloud Compute <code>ECS</code> Elastic Container Service <code>ECR</code> Elastic Container Repository <code>EBS</code> Elastic Block Storage <code>EFS</code> Elastic File Storage <code>EMR</code> Elastic MapReduce <code>EB</code> Elastic Beanstalk <code>ES</code> Elasticsearch <code>EKS</code> Elastic Kubernetes Service <code>MSK</code> Managed Kafka Service <code>RAM</code> AWS Resource Manager <code>ACM</code> Amazon Certificate Manager <code>PoLP</code> Principle of Least Privilege <code>IoT</code> Internet of Things <code>RI</code> Reserved Instances</p>"},{"location":"AWS/CCP/Notes/#aws-config-vs-aws-appconfig","title":"AWS Config vs AWS AppConfig","text":"<p><code>AWS Config</code> is a governance tool for Compliance as Code (CoC).</p> <p><code>AWS App Config</code> is used to automate the process of deploying application configuration variable changes to your web application (s).</p>"},{"location":"AWS/CCP/Notes/#sns-vs-sqs","title":"SNS vs  SQS","text":"<p><code>Simple Notifications Service</code> - Pass Along Messages eg. PubSub - generally used for sending plain text emails </p> <p><code>Simple Queue Service</code> - Queue Up Messages, Guaranteed Delivery</p>"},{"location":"AWS/CCP/Notes/#sns-vs-ses-vs-pinpoint-vs-workmail","title":"SNS vs SES vs PinPoint vs WorkMail","text":"<p><code>Simple Notifications Service</code> - Practical and Internal Emails - plain text emails - Send notifications to subscribers of topics</p> <p><code>Simple Email Service</code> - Transactional Emails, Emails that should be triggered based on in-app actions - sends HTML emails - can create Email Templates</p> <p><code>Amazon PinPoint</code> - Promotional Emails, Create email campaigns</p> <p><code>Amazon Workmail</code> - Email Web Client</p>"},{"location":"AWS/CCP/Notes/#aws-inspector-vs-aws-trusted-advisor","title":"AWS Inspector vs AWS Trusted Advisor","text":"<p>Both are security tools and they both perform audits</p> <p><code>Amazon Inspector</code> - Audits a single EC2 instance that you\u2019ve selected - Generates a report from a long list of security checks i.e 699 checks.</p> <p><code>Trusted Advisor</code> - Trusted Advisor doesn\u2019t generate out a PDF report. - Gives you a holistic view of recommendations across multiple services and best practices</p>"},{"location":"AWS/CCP/Notes/#connect-names-services","title":"Connect Names Services","text":"<p><code>Direct Connect</code> - A Dedicated Fiber Optics Connection from your DataCenter to AWS</p> <p><code>Amazon Connect</code> - Call Center as a Service</p> <p><code>Media Connect</code> - Converts Videos to Different Video Types</p>"},{"location":"AWS/CCP/Notes/#aws-artifact-vs-amazon-inspector","title":"AWS Artifact vs Amazon Inspector","text":"<p>Both Artifact and Inspector compile out PDFs</p> <p><code>AWS Artifact</code> - Why should an enterprise trust AWS? - Generates a security report that\u2019s based on global compliance frameworks</p> <p><code>Amazon Inspector</code> - How do we know this EC2 instance is Secure? Prove It? - analyzes your EC2 instance, then generates a PDF report</p>"},{"location":"AWS/CCP/Notes/#elb-variants","title":"ELB Variants","text":"<p>Elastic Load Balancer (ELB) has 4 different types of possible load balancers.</p> <p><code>Application Load Balancer (ALB)</code> - Layer 7 - HTTP/S - create rules to change routing based on information found in an HTTP/S request - Can attach an AWS WAF</p> <p><code>Network Load Balancer (NLB)</code> - Layer 3 and 4 \u2013 TCP and UDP - Where extreme performance is required for TCP and TLS traffic - Optimized for sudden and volatile traffic patterns while using a single static IP address per Availability Zone - Capable of handling millions of requests per second while maintaining ultra-low latencies</p> <p><code>Gateway Load Balancer (GWLB)</code> - When you need to deploy a fleet of third-party virtual appliances that support GENEVE</p> <p><code>Classic Load Balancer (CLB)</code> - Layer 3,4 and 7 - Retires on Aug 15, 2022</p>"},{"location":"Algorithms/","title":"Algorithms","text":"<p>Notes on learning alogrithms</p> <p>Sources: 1. https://www.youtube.com/watch?v=0IAPZzGSbME&amp;list=PLDN4rrl48XKpZkf03iYFl-O29szjTrs_O</p>"},{"location":"Algorithms/#table-of-contents","title":"Table of Contents","text":"<p>1 . Introduction To Algorithms</p>"},{"location":"Algorithms/01_Introduction/","title":"Introduction","text":"<p>An algorithm is a step by step procedure to solve a problem.</p>"},{"location":"Algorithms/01_Introduction/#11-priori-analysis-and-posteriori-testing","title":"1.1  Priori Analysis and Posteriori Testing","text":"Priori Analysis Posteriori Testing Algorithm Program Language Independent Language dependent Hardware Independent Hardware dependent Time and Space Function Watch time and bytes"},{"location":"Algorithms/01_Introduction/#12-characteristics-of-algorithms","title":"1.2 Characteristics of Algorithms","text":"<ol> <li>Input - 0 or more</li> <li>Output - at least one output or result</li> <li>Definiteness - must be logical enough to program</li> <li>Finiteness - must have a way to stop</li> <li>Effectiveness - must serve some purpose</li> </ol>"},{"location":"Algorithms/01_Introduction/#13-how-to-write-and-analyze-algorithms","title":"1.3 How to Write and Analyze Algorithms","text":"<p>Criteria's:</p> <ol> <li>Time<ul> <li>how much time does it take</li> </ul> </li> <li>Space:<ul> <li>how much memory will it consume</li> </ul> </li> </ol> <p>represented in functions not actual measurements</p>"},{"location":"Algorithms/01_Introduction/#14-frequency-count-method","title":"1.4 Frequency Count Method","text":"<p>Count the number of times each statement is executed.</p> <p>For loops of n times: - loop will execute n + 1 times, the 1 being the last check which fails - everything under the scope of the loop executes n times</p> <p>Complexity is always taken as a function: - use frequency count method to get function ex) <code>f(n) = 3n^2 + 2</code> - simplify expression to its biggest polynomial degree ex) <code>O(n^2)</code></p> <p>Example 1. <pre><code>a = [8, 3, 9, 7, 2]\n\ndef sum(a, n):\n    s = 0               # 1 (one assignment operation)\n    for i in range(n):  # n + 1 (condition is checked n+1 times)\n        s += a[i]       # n (addition happens n times)\n    return s            # 1 (return once)\n\n# total time is 2n + 3\n# degree one polynomial\n# final time complexity function simplified to O(n)\n\n# space\n# A =n, n=s=i=1 -&gt; n + 3\n# O(n)\n</code></pre></p> <p>Example 2. <pre><code># and b are nxn matrices\n\ndef add(a, b, n):\n    for i in range(n):                  # n + 1\n        for j in range(n):              # n * (n + 1)\n            c[i][j] = a[i][j] + b[i][j] # n * n\n\n# time\n# 2n^2 + 2n + 1\n# O(n^2)\n\n# space\n# matrices : a = b = c = n^ 2\n# scalars : n = i = j = 1\n# 3n^2 + 3\n# O(n^2)\n</code></pre></p>"},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/","title":"1.5 Time Complexity","text":""},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#151","title":"1.5.1","text":""},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#for-i-in-rangen-n-1-stmt-n-on","title":"<pre><code>for i in range(n):    # n + 1  \n    stmt()      # n\n                # O(n)\n</code></pre>","text":""},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#for-i-in-rangen-n-1-for-j-in-rangen-n-n-1-stmt-n-n-on2","title":"<pre><code>for i in range(n):      # n + 1\n    for j in range(n):  # n * (n + 1)\n        stmt()          # n * n\n                        # O(n^2)\n</code></pre>","text":"<pre><code>for i in range(n):\n    j = 0\n    while j &lt; i:\n        stmt()\n</code></pre> i j stmt 0 0 0 1 0,1 1 2 0,1,2 2 3 0,1,2,3 3 n 0,1,2,..n n <p>$$ 1 + 2 + 3 ... + n = {n(n+1) \\over 2} = {n^2+n \\over 2} = O(n^2) $$</p> <pre><code>p = 0\ni = 1\nwhile p &lt;= n :\n    p += i\n    i += 1\n</code></pre> i p 1 0 + 1 = 1 2 0 + 1 + 2 3 0 + 1 + 2 + 3 k 0 + 1 + 2 ... + k <p>$$ assume : p &gt; n $$</p> <p>$$ p = {k(k+1) \\over 2} $$</p> <p>$$ {k(k+1) \\over 2} &gt; n $$</p> <p>$$ k^2 &gt; n $$</p> <p>$$ k &gt; \\sqrt n$$</p> <p>$$ O(\\sqrt n) $$</p>"},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#152","title":"1.5.2","text":"<pre><code>i = 1\nwhile i &lt; n:\n    stmt\n    i *= 2\n</code></pre> i 2^0 2^1 2^2 2^k <p>$$ assume : i &gt;= n $$</p> <p>$$ 2^k &gt;= n $$</p> <p>$$ 2^k = n $$</p> <p>$$ k = \\log _2 n  $$</p> <p>$$ O(\\lceil \\log _2 n  \\rceil)  $$</p> <p>ceil because float logs will be round up -&gt; log 10 = 3.2 -&gt; executed 4 times similar complexity with division instead of multiplication</p> <pre><code>while (i * )i &lt; n:\n    stmt\n    i += 1\n</code></pre> <p>$$ i * i &lt; n $$</p> <p>$$ i * i &gt;= n $$</p> <p>$$ i^2 = n $$</p> <p>$$ i = \\sqrt n $$</p> <p>$$ O(\\sqrt n)$$</p>"},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#for-i-in-rangen-n-1-stmt-n-for-j-in-rangen-n-1-stmt-n-2n-2-on","title":"<pre><code>for i in range(n):  # n + 1\n    stmt            # n\n\nfor j in range(n):  # n + 1\n    stmt            # n\n                    # 2n + 2\n                    # O(n)\n</code></pre>","text":""},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#p-0-i-0-while-i-n-p-1-i2-log-n-j-0-while-j-p-log-p-stmt-j2-log-log-n-olog-log-n","title":"<pre><code>p = 0\ni = 0\nwhile i &lt; n:\n    p += 1\n    I*=2        # log n\n\nj = 0\nwhile j &lt; p:    # log p\n    stmt\n    j*2\n\n                # log log n\n                # O(log log n)\n</code></pre>","text":""},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#for-i-in-rangen-n-j-1-while-j-n-nlog-n-stmt-nlog-n-j-2-2nlog-n-n-pn-log-n","title":"<pre><code>for i in range(n):  # n\n    j = 1\n    while j &lt; n:    # n(log n)\n        stmt        # n(log n)\n        j *=2\n                    # 2n(log n) + n\n                    # P(n log n)\n</code></pre>","text":""},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#153-while-and-if","title":"1.5.3 : while and if","text":"<ul> <li><code>while</code> loops can do the same thing as <code>for</code> loops</li> </ul>"},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#i-0-while-i-n-n-1-i-1-n-2n-1-on","title":"<pre><code>i = 0\nwhile i &lt; n:    # n + 1\n    i += 1      # n\n                # 2n + 1\n                #O(n)\n</code></pre>","text":"<p><pre><code>a = 1\nwhile a &lt; b:\n    a *= 2\n</code></pre> | a       | | ------- | | 1       | | 12     | | 1 2* 2 | | 2^k     |</p> <p>$$ terminate : a &gt;= b $$</p> <p>$$ 2^k &gt;= b $$</p> <p>$$ 2^k = b $$</p> <p>$$ k = log _2b  $$</p> <p>$$ O(log _2n) $$</p>"},{"location":"Algorithms/01_Introduction/01.5_Time_Complexity/#if-statement-complexity","title":"if statement complexity","text":"<pre><code>def func(n):\n\n    if n &lt; 5:               # 1\n        print(n)\n    else:\n        for i in range(n):  # n\n            print(i)\n                            # best case O(1), worst case O(n)\n</code></pre>"},{"location":"Algorithms/01_Introduction/01.6-11_Classes_of_functions/","title":"1.6 Class of functions","text":"function O(1) constant O(logn) logarithmic O(n) linear n^2 Quadratic n^3 cubic O(2^n) exponential"},{"location":"Algorithms/01_Introduction/01.6-11_Classes_of_functions/#17-compare-class-of-functions","title":"1.7 Compare Class of functions","text":"<p>$$  1 &lt; log (n) &lt; \\sqrt n &lt; n &lt; nlog(n) &lt; n^2 &lt; n^3 ... 2^n &lt; 3^n ... n^n  $$</p> <p>functions express the complexity as n increases</p>"},{"location":"Algorithms/01_Introduction/01.6-11_Classes_of_functions/#18-asymptotic-notations","title":"1.8 Asymptotic Notations","text":"<p>$$ O() : big-oh : upper bound $$</p> <p>$$ \\Omega :  big-omega : lower bound  $$</p> <p>$$ \\Theta : Theta : Average bound  $$</p>"},{"location":"Algorithms/01_Introduction/01.6-11_Classes_of_functions/#111-best-worst-and-average-case-analysis","title":"1.11 Best, worst, and average case analysis","text":""},{"location":"Algorithms/01_Introduction/01.6-11_Classes_of_functions/#example-linear-search","title":"Example : Linear Search","text":"<ul> <li>best case : searching key element present at first index</li> <li>O(1)</li> <li>Worst case : searching key at last index</li> <li>O(n)</li> <li>Average case : (all possible cast time) / number of cases</li> <li>O(n)</li> </ul>"},{"location":"Algorithms/01_Introduction/01.6-11_Classes_of_functions/#example-binary-search","title":"Example : Binary Search","text":"<ul> <li>best case : searching key element present at first index</li> <li>O(1)</li> <li>Worst case : searching key at last index</li> <li>min = O(log n), max = O(n)</li> <li>Average case : (all possible cast time) / number of cases</li> <li>O(log n)</li> </ul>"},{"location":"Boost/week02/","title":"Week02","text":""},{"location":"Boost/week02/#commands-covered","title":"commands covered","text":"<ul> <li><code>man</code> - show manual information about a command</li> <li><code>sudo</code> - do it as root user (superuser)</li> <li><code>apt</code> - a package manager, user interactively only, <code>apt-get</code> in scripts</li> <li><code>sudo apt update</code> - update all sources for packages</li> <li><code>sudo apt upgrade</code> - upgrade <code>all</code> packages to latest version</li> <li><code>apt search</code><ul> <li><code>apt search ^neo</code> search for all package starting with <code>neo</code></li> </ul> </li> <li><code>sudo apt install neofetch</code> - installs neofetch and all its dependencies</li> <li><code>sudo apt remove neofetch</code> - remove only neofetch</li> <li><code>sudo apt autoremove</code> - remove all unused packages</li> <li><code>ls</code> - list the files in current directory<ul> <li><code>-a</code> - list all files including hidden files</li> <li><code>-l</code> - list in long form (includes permisssions etc.)</li> </ul> </li> <li><code>pwd</code> print working directory</li> <li><code>cd</code> change directory</li> </ul>"},{"location":"Boost/week03/","title":"week 3 : remote connections to vm","text":"<ul> <li>In order for to connect to a remote server. The server must have a program that is always listening to connections.</li> </ul>"},{"location":"Boost/week03/#commands","title":"commands","text":"<ul> <li><code>ip a</code> - show all IP addresses</li> <li><code>clear</code> - clear screen</li> <li><code>which</code> - display path to a program</li> <li><code>users</code> - shortname of all logged in users</li> <li><code>w</code> - more information on every user logged in</li> <li><code>sudo apt install openssh-server</code> - install ssh server (if not)</li> </ul>"},{"location":"Boost/week03/#ssh-into-virtualbox","title":"ssh into VirtualBox","text":"<ul> <li> <p>make sure virtual box machine is attached ot bridge adapter to enable easy ssh</p> <ul> <li><code>Settings &gt; Network &gt; adapter 1 &gt; attached to &gt; Bridged adapter</code></li> </ul> </li> <li> <p>connect ith ssh</p> </li> <li><code>ssh -l &lt;user&gt; &lt;ip&gt;</code></li> <li> <p>enter users password</p> </li> <li> <p>virtualbox command line</p> </li> <li>add <code>export PATH=\"$PATH:/c/Program Files/Oracle/VirtualBox\"</code> to .bashrc</li> <li><code>vboxmanage startvm &lt;vmname&gt; --type headless</code> start the vm in headless mode</li> <li><code>vboxmanage list runningvms</code> list running vms</li> <li><code>vboxmanage list vms</code> list all vms</li> <li><code>vboxmanage controlvm &lt;vmname&gt; poweroff</code> power off headless vm</li> </ul>"},{"location":"Boost/week03/#vi-basics","title":"vi Basics","text":"<ul> <li><code>i</code> - change to insert mode</li> <li><code>ESC</code>- change to command mode</li> <li><code>arrow keys</code> - navigation</li> <li><code>:wq</code> - save and quit</li> <li><code>:q!</code> - quite without saving</li> <li><code>:w</code>- save without exiting</li> </ul>"},{"location":"C%2B%2B/","title":"C++","text":"<p>reference: https://www.youtube.com/watch?v=8jLOx1hD3_o</p>"},{"location":"C%2B%2B/02.DivingIn/","title":"Index","text":""},{"location":"C%2B%2B/02.DivingIn/#main-function","title":"Main function","text":"<p><pre><code>#include &lt;iostream&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"Hello World!\" &lt;&lt; std::endl;\n}\n</code></pre> - <code>#include</code> : used to include libraries  - <code>int main() {}</code> the entry point of a cpp program - function can return a value to indicate if it succeeded or failed</p> <p>extra - <code>std::ednl</code> creats new line characters - <code>std::cout</code> print to console</p>"},{"location":"C%2B%2B/02.DivingIn/#comments","title":"comments","text":"<pre><code>#include &lt;iostream&gt;\n\nint main()\n{\n    // single line comments\n\n    /* \n    multi\n    line\n    comments\n    */\n}\n</code></pre>"},{"location":"C%2B%2B/02.DivingIn/#errors","title":"Errors","text":"<ul> <li>compile time erros<ul> <li>bbinary won't be created</li> </ul> </li> <li>runtime errors<ul> <li>binray created but doesn't act as expected</li> </ul> </li> <li>warnings<ul> <li>issues not seriours enough for compiler to stop working</li> </ul> </li> </ul>"},{"location":"C%2B%2B/02.DivingIn/#statements-and-functions","title":"Statements and Functions","text":"<ul> <li><code>statements</code><ul> <li>basic unit of computation in a C++ program</li> <li>end with a semicolon <code>;</code></li> <li>executed in order from top to bottom</li> </ul> </li> <li><code>function</code><ul> <li>takes inputs and returns a value</li> <li>reusable collections of code</li> </ul> </li> </ul>"},{"location":"C%2B%2B/02.DivingIn/#inputs-and-outputs","title":"Inputs and Outputs","text":"<ul> <li><code>std::cout</code> : print data to console</li> <li><code>std::cin</code> : read data from the console</li> <li><code>std::cerr</code> : print erros to the console</li> <li> <p><code>std::clog</code> : print log messages to the console</p> </li> <li> <p><code>&gt;&gt;</code> and <code>&lt;&lt;</code> are used to indicate the direction that data travels; either from console to program or from program to console</p> </li> <li>std::cin can be chained</li> <li><code>std::getline(inputstream, buffertostoreinto)</code> used to grab lines with spaces</li> </ul>"},{"location":"C%2B%2B/02.DivingIn/#c-program-execution-model","title":"C++ Program Execution Model","text":"<ul> <li>program written in human readable c++</li> <li>program is compiled into a binary executable that is not readable by human</li> <li>program is ran in memory (RAM)</li> <li>program will execute the statments from top to bottom</li> <li>prgroam will jump to diffeerent memory addresses during execution, and that is where some optimization occcurs</li> </ul>"},{"location":"C%2B%2B/02.DivingIn/#c-core-language-vs-standard-library-vs-stl","title":"C++ core language Vs Standard library Vs STL","text":"<ul> <li><code>core</code> : basic rules and types that define how c++ programs run</li> <li><code>standard library</code> : set of ready to use, highly specialized components that can be used in programs</li> <li><code>STL</code> : a highly specialized part of the standard library</li> </ul>"},{"location":"Coding/SoftwareDesignPrinciples/","title":"SoftwareDesignPrinciples","text":""},{"location":"Coding/SoftwareDesignPrinciples/#composition-vs-inheritance","title":"Composition vs Inheritance","text":"<p>Inheritance can be used to share methods across multiple classes. However it introduces very strong coupling. A change in the parent class would effect all child classes downstream. Bad practice to try using inheritance to separate things.</p> <p>Composition can allow you to share functionality without resulting in such high amount of coupling. This is done by connecting different classes via instance variables. Reduces coupling, lowers duplication, and enables reusability.</p>"},{"location":"Coding/SoftwareDesignPrinciples/#cohesion","title":"Cohesion","text":"<p>Can also be understood as single responsibility principle. Every function or method should have one responsibility. Every class should be a collection of methods that handle a specific idea. Complex logic should be created through the interaction of these units.</p>"},{"location":"Coding/SoftwareDesignPrinciples/#low-coupling","title":"Low Coupling","text":"<p>Coupling is the degree in which functions and class need each other. High coupling occurs when a change in one function requires a change in other functions. Low coupling allows you to change code without breaking other parts of your system, and enables you to reuse your code more often.</p> <p>Best practice to follow is principle of least knowledge. Create units that only talk to and understand closely related units. </p> <ul> <li>content coupling : one method/function directly modifies data in another class</li> <li>global coupling : different functions share the same global data</li> <li>external coupling : application depends on external api</li> <li>control coupling : one module controls the flow of another part of the code</li> <li>stamp coupling : data structure needlessly coupled, ex module depend on a structure even though it doesn't use the structure details</li> <li>data coupling : when methods share data via parameters</li> <li>import coupling : importing a module</li> <li>message coupling : connect pieces of program together via events, like in gui interfaces</li> </ul>"},{"location":"Coding/SoftwareDesignPrinciples/#dependency-injection","title":"Dependency Injection","text":"<p>Make a class independent of its dependencies by separating the creation of an object and its usage. Done by passing objects to a class that uses it, rather than having the class responsible for creating it. Leads to lower coupling.</p> <p>Also makes it easier to isolate pieces of a program. Which can be useful when testing. As mocks of dependencies can be easily created and passed to a class.</p>"},{"location":"DSA/","title":"Data Structures and Algorithms","text":"<ul> <li>Data Structures and Algorithms</li> <li>Arrays</li> <li>Binary Search Trees</li> <li>Binary Trees</li> <li>Dynamic Programming</li> <li>Famous Algorithms</li> <li>Graphs</li> <li>Greedy Algorithms</li> <li>Heaps</li> <li>Linked Lists</li> <li>Recursion</li> <li>Searching</li> <li>Sorting</li> <li>Stacks</li> <li>Strings</li> <li>Tries</li> <li>Data Structures<ul> <li>Arrays</li> <li>Hash Tables</li> <li>Linked Lists</li> <li>Stacks and Queues</li> <li>Trees</li> </ul> </li> </ul>"},{"location":"DSA/#arrays","title":"Arrays","text":"<ul> <li>if <code>multiple elements in an array are being compared</code> together, </li> <li>you can <code>create a hash table that stores the elements that frequently accessed</code></li> <li>if only a <code>specific value</code> in the hash value is needed, can create a <code>variable outside the hash table and check to update it every time the hash table is updated</code></li> <li>if the <code>order of elements in an array can affect the comparison</code>, </li> <li>you can <code>sort the array and create left and right pointers</code></li> <li>if it is already sorted, <code>try to use pointers to avoid resorting</code></li> <li>if an <code>entire sequence</code> must be validated</li> <li>you can return <code>sequence index == sequence length</code></li> </ul>"},{"location":"DSA/#binary-search-trees","title":"Binary Search Trees","text":"<ul> <li>can be solved through <code>recursion</code></li> <li>if using <code>recursion</code> to find solution, try to see if the space complexity can be optimized by <code>not stacking recursive function calls</code></li> </ul>"},{"location":"DSA/#binary-trees","title":"Binary Trees","text":"<ul> <li>if you have to <code>travel to each leaf node</code>, and <code>keep track of of their order</code> then <code>recursive function calls are expected</code></li> <li>if you have to <code>travel to each leaf node</code>, and <code>keep track of of their order</code> then <code>recursive function calls are expected</code></li> <li>when working recursively, <code>returning a special value for the lead node can be potentially useful</code></li> <li>if using closures as helper functions, and they <code>increment or change a variable in the outside scope, check if the variable is mutable</code></li> </ul>"},{"location":"DSA/#dynamic-programming","title":"Dynamic Programming","text":""},{"location":"DSA/#famous-algorithms","title":"Famous Algorithms","text":""},{"location":"DSA/#graphs","title":"Graphs","text":""},{"location":"DSA/#greedy-algorithms","title":"Greedy Algorithms","text":"<p>A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. - If trying to <code>optimize on speed</code>, it may be useful to <code>sort as a first step</code></p>"},{"location":"DSA/#heaps","title":"Heaps","text":""},{"location":"DSA/#linked-lists","title":"Linked Lists","text":""},{"location":"DSA/#recursion","title":"Recursion","text":""},{"location":"DSA/#searching","title":"Searching","text":"<ul> <li>The binary search algorithm works only with sorted arrays</li> <li>if a problem involves a sorted array input, consider using the binary search algorithm</li> </ul>"},{"location":"DSA/#sorting","title":"Sorting","text":"<ul> <li><code>Bubble sorting</code> is when you iterate over each index of an array, compare adjacent elements, and swap accordingly. Repeating the cycle until no swaps occur.</li> <li><code>Insertion sorting</code> is when you iterate over each index of an array, and while the element at an index is less than the one before it, swap the elements. Essentially pushing each element as far back as possible.</li> <li><code>Selection sorting</code>is when you iterate over each index of an array. At each index, check every element at and after the current index, find the index of the smallest element. Swap the element with the smallest element with the element at the current index.</li> </ul>"},{"location":"DSA/#stacks","title":"Stacks","text":""},{"location":"DSA/#strings","title":"Strings","text":""},{"location":"DSA/#tries","title":"Tries","text":""},{"location":"DSA/#data-structures","title":"Data Structures","text":""},{"location":"DSA/#arrays_1","title":"Arrays","text":"<ul> <li>a linear collection of values that are accessible using numbered indexes</li> </ul> Operation complexity Accessing a value at given index O(1) Updating a value at a given index O(1) Inserting a value at the beginning O(n) Inserting a value in the middle O(n) Inserting a value at the end <code>dynamic</code> O(1), <code>static</code> O(n) Removing a value at the beginning O(n) Removing a value in the middle O(n) Removing a value at the end O(1) Copying the array O(n)"},{"location":"DSA/#hash-tables","title":"Hash Tables","text":"Operation complexity inserting a key/value pair O(1) Removing a key/value pair O(1) Lookup a key/value pair O(1)"},{"location":"DSA/#linked-lists_1","title":"Linked Lists","text":"<ul> <li>nodes that hold a value along with a pointer to another node or null</li> </ul> <pre><code># singly linked list\n0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null\n\n# double linked list\nnull &lt;-&gt; 0 &lt;-&gt; 1 &lt;-&gt; 2 &lt;-&gt; 3 &lt;-&gt; 4 &lt;-&gt; 5 &lt;-&gt; null\n</code></pre> Operation Time Complexity Accessing the head O(1) Accessing the tail single-O(n) double-O(1) Accessing a middle node O(n) Inserting / Removing the head O(1) Inserting / Removing the tail O(n) to access + O(1) Inserting / Removing a middle node O(n) to access + O(1) Searching for a value O(n)"},{"location":"DSA/#stacks-and-queues","title":"Stacks and Queues","text":"<ul> <li><code>stack</code> An array like data structure whose elements follow the <code>LIFO</code> rule</li> </ul> Operation Time Complexity Pushing an element onto the stack O(1) Popping an elemnt off the stack O(1) Peeking at the element on the top of the stack O(1) Search for an element in the stack O(n) <ul> <li><code>queue</code> An array like data structure whose elements follow the <code>FIFO</code> rule</li> </ul> Operation Time Complexity Enqueuing an element into the queue O(1) Dequeuing an elemnt out of the queue O(1) Peeking at the element at the front of the queue O(1) Search for an element in the queue O(n)"},{"location":"DSA/#trees","title":"Trees","text":"<ul> <li>A data structure thar consists of nodes, each with some value and pointers in to child nodes, which recursively form <code>subtrees</code></li> </ul>"},{"location":"DSA/BinaryTrees/","title":"Binary Trees Notes:","text":"<ul> <li>if you have to <code>travel to each leaf node</code>, and <code>keep track of of their order</code> then <code>recursive function calls are expected</code></li> <li>when working recursively, <code>returning a special value for the lead node can be potentially useful</code></li> <li>if using closures as helper functions, and they <code>increment or change a variable in the outside scope, check if the variable is mutable</code></li> </ul>"},{"location":"DSA/FamousAlgos/","title":"Index","text":"<p>$ Famouos Algos</p>"},{"location":"DSA/Graphs/","title":"Graph Notes","text":""},{"location":"DSA/GreedyAlgorithms/","title":"Greedy Algorithms Notes:","text":"<p>A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. - If trying to <code>optimize on speed</code>, it may be useful to <code>sort as a first step</code></p>"},{"location":"DSA/LinkedLists/","title":"Linked Lists Notes:","text":""},{"location":"DSA/Recursion/","title":"Recursion Notes:","text":""},{"location":"DSA/Searching/","title":"Searching Notes:","text":"<ul> <li>The binary search algorithm works only with sorted arrays</li> <li>if a problem involves a sorted array input, consider using the binary search algorithm</li> </ul>"},{"location":"DSA/Sorting/","title":"Sorting Notes:","text":"<ul> <li><code>Bubble sorting</code> is when you iterate over each index of an array, compare adjacent elements, and swap accordingly. Repeating the cycle until no swaps occur.</li> <li><code>Insertion sorting</code> is when you iterate over each index of an array, and while the element at an index is less than the one before it, swap the elements. Essentially pushing each element as far back as possible.</li> <li><code>Selection sorting</code>is when you iterate over each index of an array. At each index, check every element at and after the current index, find the index of the smallest element. Swap the element with the smallest element with the element at the current index.</li> </ul>"},{"location":"DSA/Strings/","title":"Index","text":"<p>Strings Notes:</p>"},{"location":"DSA/arrays/","title":"Arrays Notes:","text":"<ul> <li>if <code>multiple elements in an array are being compared</code> together, </li> <li>you can <code>create a hash table that stores the elements that frequently accessed</code></li> <li>if only a <code>specific value</code> in the hash value is needed, can create a <code>variable outside the hash table and check to update it every time the hash table is updated</code></li> <li>if the <code>order of elements in an array can affect the comparison</code>, </li> <li>you can <code>sort the array and create left and right pointers</code></li> <li>if it is already sorted, <code>try to use pointers to avoid resorting</code></li> <li>if an <code>entire sequence</code> must be validated</li> <li>you can return <code>sequence index == sequence length</code></li> </ul>"},{"location":"DSA/binarySearchTrees/","title":"Binary Search Tree Notes:","text":"<ul> <li>can be solved through <code>recursion</code></li> <li>if using <code>recursion</code> to find solution, try to see if the space complexity can be optimized by <code>not stacking recursive function calls</code></li> </ul>"},{"location":"DSA_CPP/0_2.Essentials/","title":"Essentials","text":""},{"location":"DSA_CPP/0_2.Essentials/#arrays","title":"Arrays","text":"<ul> <li>declare <code>type</code> and <code>size</code> of the array when created</li> <li>can be initialized with values upon creation and or populated afterwards</li> <li>elements in an array are access using the <code>array[idx]</code> syntax</li> <li>un initialized elements of an array have a <code>default value of 0</code></li> </ul> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nint main()\n{\n    // create an array of size 10\n    // initialize the first four elements\n    int A[10] = {1, 2, 3, 4};\n\n    // for loop using incrementation of index\n    for (int i = 0; i &lt; 10; i++)\n    {\n        cout &lt;&lt; A[i] &lt;&lt; endl;\n    }\n\n    // for loop without using index\n    for (int x : A)\n    {\n        cout &lt;&lt; x &lt;&lt; endl;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"DSA_CPP/0_2.Essentials/#structures","title":"Structures","text":"<ol> <li>Sign of structure</li> <li>Accessing Members</li> </ol> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nstruct Rectangle\n{\n    int length;\n    int breadth;\n};\n// 2. size of struct is totoal memory consumed by all of its members\n// definition doesn't consume memory, memory is only consumed when a\n// variable is created\n\nint main()\n{\n    // 3. Declaring a Structure\n    struct Rectangle r;\n    // 3.1 Declaration and initialization\n    struct Rectangle w = {10, 5};\n\n    cout &lt;&lt; w.breadth &lt;&lt; endl;\n\n    // array of structs\n    struct Rectangle shapes[2];\n    shapes[0] = {1, 2};\n    shapes[1] = {3, 4};\n\n    // can also do\n    // struct Rectangle shapes[2] = {{1, 2}, {3, 4}};\n\n    cout &lt;&lt; shapes[0].length &lt;&lt; endl;\n    cout &lt;&lt; shapes[0].breadth &lt;&lt; endl;\n    cout &lt;&lt; shapes[1].length &lt;&lt; endl;\n    cout &lt;&lt; shapes[1].breadth &lt;&lt; endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"DSA_CPP/0_2.Essentials/#pointers","title":"Pointers","text":"<p>Address variable that stores address of data. 1. Accessing heap memory 2. access resources outside of the program 3. parameter passing</p> <pre><code>#include &lt;iostream&gt;\n\nusing namespace std;\n\nint main()\n{\n    // declaration and initialization of pointer\n    int a = 10;\n    int *p;\n    // &amp; = address\n    p = &amp;a;\n\n    cout &lt;&lt; a &lt;&lt; endl;\n    cout &lt;&lt; &amp;a &lt;&lt; endl;\n    cout &lt;&lt; p &lt;&lt; endl;\n    cout &lt;&lt; *p &lt;&lt; endl;\n\n    // pointers with arrays\n    int A[5] = {1, 2, 3, 4, 5};\n    int *q;\n    q = &amp;A[0];\n\n    for (int i = 0; i &lt; 5; i++)\n    {\n        cout &lt;&lt; q[i] &lt;&lt; endl;\n    }\n\n    // pointers with heap\n    int *r;\n    r = new int[5];\n    r[0] = 1;\n    r[1] = 10;\n    r[2] = 100, r[3] = 1000;\n    r[4] = 10000;\n\n    for (int i = 0; i &lt; 5; i++)\n    {\n        cout &lt;&lt; r[i] &lt;&lt; endl;\n    }\n\n    // pointer size is independent of data type\n    cout &lt;&lt; sizeof(p) &lt;&lt; endl;\n    cout &lt;&lt; sizeof(r) &lt;&lt; endl;\n\n    // release memory in heap\n    delete[] r;\n}\n</code></pre>"},{"location":"Golang/BeginnerGoProjects/","title":"Learn Go Programming by Building 11 Projects \u2013 Full Course","text":"<p>resource: https://www.youtube.com/watch?v=jFfo23yIWac</p>"},{"location":"Golang/BeginnerGoProjects/01.WebServer/","title":"Simple WebServer","text":"<pre><code>flowchart LR\n    Server --&gt; /\n    Server --&gt; /hello\n    Server --&gt; /form\n\n    / --&gt; index.html\n    /hello --&gt; hello_func\n    /form --&gt; form_func\n\n    form_func --&gt; form.html</code></pre>"},{"location":"Golang/BuildingMicroservices/","title":"Building Microservices with Go","text":"<p>source: https://www.youtube.com/watch?v=VzBGi_n65iU&amp;list=PLmD8u-IFdreyh6EUfevBcbiuCKzFk0EW_&amp;index=2</p>"},{"location":"Golang/BuildingMicroservices/#notes","title":"Notes","text":"<ul> <li>episode 1<ul> <li><code>http.HandleFunc</code> registers the handler function for the given pattern in the DefaultServeMux</li> <li><code>http.ListenAndServe</code> listens on the TCP network address addr and then calls Serve with handler to handle requests on incoming connections</li> </ul> </li> <li>episode 2<ul> <li>organize project by moving handlers to their own package</li> <li><code>http.NewServeMux</code> create a  serve mux if you don't want to use default one</li> <li><code>http.Server</code> create server with custom config like <code>IdleTimeout</code></li> <li><code>go func()</code> to run server in goroutine</li> <li><code>make(chan os.Signal, 1)</code> to make channel for signals and <code>signal.Notify</code> to send shutdown signals</li> <li><code>http.Server.Shutdown</code> to gracefully shutdown when a signal is sent to the signal channel</li> </ul> </li> <li>episode 3<ul> <li>create a <code>data</code> package to organize code for data</li> <li><code>json.NewEncoder</code> to efficently write json</li> <li>use <code>tags</code> to control what json fields a written and what their names are</li> <li>check method before ServeHTTP in product handler</li> </ul> </li> <li>episode 4<ul> <li><code>json.NewDecoder</code> to turn json into struct</li> <li><code>r.URL.Path</code> to fine the /path after the url of the api that's hit</li> <li><code>strconv.Atoi</code> to convert string into an int</li> </ul> </li> <li>episode 5<ul> <li><code>github.com/gorilla/mux</code> frame work for servers</li> <li><code>sm.Methods(http.MethodGet).Subrouter()</code> create subroutes for different methods</li> <li><code>putRouter.HandleFunc(\"/{id:[0-9]+}\", ph.UpdateProduct)</code> define path variables with regex</li> <li><code>vars := mux.Vars(r)</code> and <code>vars[\"id\"]</code>: get variables from path that gorrilla parsed in handlers</li> <li><code>postRouter.Use(ph.MiddlewareProductValidation)</code> add middleware to a router</li> <li><code>ctx := context.WithValue</code> and <code>r = r.WithContext(ctx)</code>: update the request with context in middleware</li> <li><code>prod := r.Context().Value(KeyProduct{}).(*data.Product)</code> get variable from request context in main handler</li> </ul> </li> <li>episode 6<ul> <li><code>github.com/go-playground/validator/v10</code> for validating structs</li> <li><code>validate:\"required\"</code> tags to add to struct which will make sure the field will be validated by a func</li> <li><code>validate.RegisterValidation(\"sku\", validateSKU)</code> to register custom validation function</li> <li>add validation step to middleware</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/","title":"Go","text":"<p>Learning Go</p> <p>Resources: - https://www.youtube.com/watch?v=YS4e4q9oBaU</p>"},{"location":"Golang/GoBeginners/ArraysAndSlices/","title":"Arrays and Slices","text":"<ul> <li>Arrays and Slices</li> <li>Arrays<ul> <li>Array Creation</li> <li>Array Built-in Functions</li> <li>Working with arrays</li> </ul> </li> <li>Slices<ul> <li>Slice Creation</li> <li>Slice Built-in functions</li> <li>Working with slices</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/ArraysAndSlices/#arrays","title":"Arrays","text":""},{"location":"Golang/GoBeginners/ArraysAndSlices/#array-creation","title":"Array Creation","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n    //  arrays are fixed size collections of items of the same type\n    // [num of elements]type{elements}\n    grades := [3]int{1,2,3}\n    fmt.Printf(\"Grades: %v\", grades) // Grades: [1 2 3]\n\n    // don't have to define size if creating array  literal\n    grades2 := [...]int{1,2,3}\n\n    // filling out array using indexes\n    var students [3]string\n    students[0] = \"Lisa\"\n    student[1] = \"Ahmed\"\n    student[2] = \"Arnold\"\n    fmt.Printf(\"Students: %v\", students)\n\n}\n</code></pre>"},{"location":"Golang/GoBeginners/ArraysAndSlices/#array-built-in-functions","title":"Array Built-in Functions","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n    var students [3]string\n    students[0] = \"Lisa\"\n    student[1] = \"Ahmed\"\n    student[2] = \"Arnold\"\n\n    // built-in length function\n    fmt.Printf(\"Number of Students: %v\", len(students))\n}\n</code></pre>"},{"location":"Golang/GoBeginners/ArraysAndSlices/#working-with-arrays","title":"Working with arrays","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n\n    a := [...]int{1,2,3}\n\n    // copying array\n    b := a\n\n    // copying an array can being expensive\n    // point to same address as original array\n    c := &amp;a\n\n    fmt.Printf(a)\n    fmt.Printf(b)\n    fmt.Printf(c)\n}\n</code></pre>"},{"location":"Golang/GoBeginners/ArraysAndSlices/#slices","title":"Slices","text":""},{"location":"Golang/GoBeginners/ArraysAndSlices/#slice-creation","title":"Slice Creation","text":"<p><pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n    // initialize a slice\n    a := []int{1,2,3}\n\n    // copies naturally refer to same underlying data\n    b := a\n    fmt.Printf(\"Capacity: %v\\n\", cap(b))\n\n    // slicing syntax\n    s := []int{1,2,3,4,5,6,7,8,9,10}\n    c := s[:]    // slice of all elements\n    d := s[3:]   // slice from 4th element to the end\n    e := s[:6]   // slice first 6 elements\n    f := s[3:6]  // copy 4th through 6th element\n    fmt.Println(s)\n    fmt.Println(c)\n    fmt.Println(d)\n    fmt.Println(e)\n    fmt.Println(f)    \n}\n</code></pre> - slice can be created out of arrays</p>"},{"location":"Golang/GoBeginners/ArraysAndSlices/#slice-built-in-functions","title":"Slice Built-in functions","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n\n    // create slice using the built in make function\n    // make(slice type, num of elements, capacity)\n    a := make([]int, 3, 100)\n    fmt.Printf(\"Length: %v\", len(a))\n    fmt.Printf(\"Capacity: %v\", cap(a))\n}\n</code></pre>"},{"location":"Golang/GoBeginners/ArraysAndSlices/#working-with-slices","title":"Working with slices","text":"<p><pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n\n    // adding to a slice\n    a := []int{}\n    fmt.Printf(\"Length: %v\", len(a))\n    fmt.Printf(\"Capacity: %v\", cap(a))    \n\n    a = append(a, 1, 2, 3)\n    fmt.Printf(\"Length: %v\", len(a))\n    fmt.Printf(\"Capacity: %v\", cap(a))\n\n    // concatenate slices\n    a = append(a, []int{1, 2, 3}...)\n    fmt.Printf(\"Length: %v\", len(a))\n    fmt.Printf(\"Capacity: %v\", cap(a))\n}\n</code></pre> <pre><code>package main\n\nimport (\"fmt\")\n\nfunc main () {\n    a := []int{1,2,3,4}\n\n    // shift slice (remove from beginning)\n    b := a[1:]\n\n    // pop slice, (remove from end)\n    c:= a[:len(a)-1]\n    fmt.Printf()\n\n    // remove middle (3rd) element\n    d := append(a[:2], a[3:]...)\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/Channels/","title":"Channels","text":"<ul> <li>Channels</li> <li>Channel Basics</li> <li>Restricting Data Flow</li> <li>Buffered Channels</li> <li>Range Loops and Closing Channels</li> <li>Select Statements</li> </ul>"},{"location":"Golang/GoBeginners/Channels/#channel-basics","title":"Channel Basics","text":"<ul> <li>create a channel with <code>make</code> command</li> <li>strong typed <code>make(chan int)</code></li> <li>send and receive a message to and from an channel using arrow syntax</li> <li>send: <code>ch &lt;- val</code></li> <li>receiveL <code>val := &lt;-ch</code></li> <li>Can have multiple senders and receivers <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg = sync.WaitGroup{}\n\nfunc main() {\n    ch := make(chan int)\n    wg.Add(2)\n    go func() {\n        i := &lt;-ch\n        fmt.Println(i)\n        wg.Done()\n    }()\n    go func() {\n        ch &lt;- 0\n        wg.Done()\n    }()\n    wg.Wait()\n\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/Channels/#restricting-data-flow","title":"Restricting Data Flow","text":"<ul> <li>by default channels are bidirectional</li> <li>channel can be cast to send only or receive only versions</li> <li>send only: <code>chan &lt;- int</code></li> <li>receive only: <code>&lt;-chan int</code> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg = sync.WaitGroup{}\n\nfunc main() {\n    ch := make(chan int)\n    wg.Add(2)\n    go func(ch &lt;-chan int) {\n        i := &lt;-ch\n        fmt.Println(i)\n        wg.Done()\n    }(ch)\n    go func(ch chan&lt;- int) {\n        ch &lt;- 0\n        wg.Done()\n    }(ch)\n    wg.Wait()\n\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/Channels/#buffered-channels","title":"Buffered Channels","text":"<ul> <li>channels block </li> <li>sender side until receiver is available</li> <li>receiver side until message is available</li> <li>buffered channels contain internal data stores</li> <li>Use buffered channels when send and receive have asymmetric loading</li> </ul> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg = sync.WaitGroup{}\n\nfunc main() {\n    ch := make(chan int, 2)\n    wg.Add(2)\n    go func(ch &lt;-chan int) {\n        i := &lt;-ch\n        fmt.Println(i)\n        j := &lt;-ch\n        fmt.Println(j)\n        wg.Done()\n    }(ch)\n    go func(ch chan&lt;- int) {\n        ch &lt;- 0\n        ch &lt;- 1\n        wg.Done()\n    }(ch)\n    wg.Wait()\n\n}\n</code></pre>"},{"location":"Golang/GoBeginners/Channels/#range-loops-and-closing-channels","title":"Range Loops and Closing Channels","text":"<ul> <li>use to monitor channel and process messages as they arrive</li> <li>loop exits when channel is closed  <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg = sync.WaitGroup{}\n\nfunc main() {\n    ch := make(chan int, 2)\n    wg.Add(2)\n    go func(ch &lt;-chan int) {\n        for i := range ch {\n            fmt.Println(i)\n        }\n        wg.Done()\n    }(ch)\n    go func(ch chan&lt;- int) {\n        ch &lt;- 0\n        ch &lt;- 1\n        close(ch)\n        wg.Done()\n    }(ch)\n    wg.Wait()\n\n}\n</code></pre></li> </ul> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg = sync.WaitGroup{}\n\nfunc main() {\n    ch := make(chan int, 2)\n    wg.Add(2)\n    go func(ch &lt;-chan int) {\n        for {\n            if i, ok := &lt;-ch; ok {\n                fmt.Println(i)\n            } else {\n                break\n            }\n        }       \n        wg.Done()\n    }(ch)\n    go func(ch chan&lt;- int) {\n        ch &lt;- 0\n        ch &lt;- 1\n        close(ch)\n        wg.Done()\n    }(ch)\n    wg.Wait()\n\n}\n</code></pre>"},{"location":"Golang/GoBeginners/Channels/#select-statements","title":"Select Statements","text":"<ul> <li>allows goroutine to monitor several channels at once</li> <li>block if all channels block</li> <li>if multiple channels receive value simultaneously, behavior is undefined</li> <li>no blocking select statement can involve a dealt case <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\nconst (\n    logInfo    = \"INFO\"\n    logWarning = \"WARNING\"\n    logError   = \"ERROR\"\n)\n\ntype LogEntry struct {\n    time     time.Time\n    severity string\n    message  string\n}\n\nvar logCh = make(chan LogEntry, 50)\nvar doneCh = make(chan struct{})\n\nfunc main() {\n    go logger()\n    logCh &lt;- LogEntry{time.Now(), logInfo, \"App is startting\"}\n    logCh &lt;- LogEntry{time.Now(), logInfo, \"App is shutting down\"}\n    time.Sleep(100 * time.Millisecond)\n    doneCh &lt;- struct{}{}\n}\n\nfunc logger() {\n    for {\n        select {\n        case entry := &lt;-logCh:\n            fmt.Printf(\"%v - [%v]%v\\n\", entry.time.Format(\"2006-01-02\"), entry.severity, entry.message)\n        case &lt;-doneCh:\n            break\n        }\n    }\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/ControlFlow/","title":"Control Flow","text":"<ul> <li>Control Flow</li> <li>If Statements<ul> <li>Operators</li> <li>If-else and if-else if statements</li> </ul> </li> <li>Switch Statements<ul> <li>Simple Cases</li> <li>Cases With Multiple Tests</li> <li>Falling through</li> <li>Type Switches</li> <li>Break</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/ControlFlow/#if-statements","title":"If Statements","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    if true {\n        fmt.Println(\"This test is true\")\n    }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    pokemon := map[string]string {\n        \"Charmander\": \"Kanto\",\n        \"torchic\":  \"Hoenn\",\n        \"Chimchar\": \"Sinnoh\",\n\n    }\n    if pop, ok := pokemon[\"torchic\"]; ok {\n        fmt.Printf(pop)\n    }\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/ControlFlow/#operators","title":"Operators","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    number := 50\n    guess := 24\n\n    if guess &lt; 1 || guess &gt; 100 {\n        fmt.Println(\"The guess must between 1 and 100\")\n    }\n\n    if guess &gt;= 1 &amp;&amp; guess &lt;= 100 {\n        if guess &lt; number {\n            fmt.Println(\"To Low\")\n        }\n        if guess &gt; number {\n            fmt.Println(\"To High\")\n        }\n        if guess == number {\n            fmt.Println(\"You got it!\")\n        }\n\n        fmt.Println(number &lt;= guess, number &gt;= guess, number != guess)\n    }\n    fmt.Println(!true)\n}\n</code></pre> - go exits logical tests early (short circuiting)   - escape or test on first true condition   - escape and test on first false condition</p>"},{"location":"Golang/GoBeginners/ControlFlow/#if-else-and-if-else-if-statements","title":"If-else and if-else if statements","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n\n    // else statement\n    number := 50\n    guess := 24\n\n    if guess &lt; 1 || guess &gt; 100 {\n        fmt.Println(\"The guess must between 1 and 100\")\n    } else {\n        if guess &lt; number {\n            fmt.Println(\"To Low\")\n        }\n        if guess &gt; number {\n            fmt.Println(\"To High\")\n        }\n        if guess == number {\n            fmt.Println(\"You got it!\")\n        }\n\n        fmt.Println(number &lt;= guess, number &gt;= guess, number != guess)\n    }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    number := 50\n    guess := 24\n\n    if guess &lt; 1 {\n        fmt.Println(\"The guess must be greater 1\")\n    } else if guess &gt; 100 {\n        fmt.Println(\"The guess must be less than 100\")\n    } else {\n        if guess &lt; number {\n            fmt.Println(\"To Low\")\n        }\n        if guess &gt; number {\n            fmt.Println(\"To High\")\n        }\n        if guess == number {\n            fmt.Println(\"You got it!\")\n        }\n\n        fmt.Println(number &lt;= guess, number &gt;= guess, number != guess)\n    }\n}\n</code></pre> - go exits on the first matching if-else if case</p>"},{"location":"Golang/GoBeginners/ControlFlow/#switch-statements","title":"Switch Statements","text":"<ul> <li>special purpose if statements</li> <li>can't have overlapping cases</li> </ul>"},{"location":"Golang/GoBeginners/ControlFlow/#simple-cases","title":"Simple Cases","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    switch 1 {\n    case 1:\n        fmt.Println(\"one\")\n    case 2:\n        fmt.Println(\"two\")\n    default:\n        fmt.Println(\"not one or two\")\n    }  \n}\n</code></pre>"},{"location":"Golang/GoBeginners/ControlFlow/#cases-with-multiple-tests","title":"Cases With Multiple Tests","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    switch 1 {\n    case 1, 3, 5:\n        fmt.Println(\"one. three, or five\")\n    case 2, 4, 6:\n        fmt.Println(\"two, four, or six\")\n    default:\n        fmt.Println(\"not one through 6\")\n    }  \n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // initialize switch value\n    switch i:= 2+3;i {\n    case 1, 3, 5:\n        fmt.Println(\"one. three, or five\")\n    case 2, 4, 6:\n        fmt.Println(\"two, four, or six\")\n    default:\n        fmt.Println(\"not one through 6\")\n    }  \n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // un-tgged switch statements\n    i:= 10\n    switch {\n    case i &lt;= 10:\n        fmt.Println(\"less than or equal to ten\")\n    case i &lt;= 20:\n        fmt.Println(\"less than or equal to ten\")\n    default:\n        fmt.Println(\"greater than 20\")\n    }  \n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/ControlFlow/#falling-through","title":"Falling through","text":"<ul> <li>make go to next case <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // fallthrough key word\n    i:= 10\n    switch {\n    case i &lt;= 10:\n        fmt.Println(\"less than or equal to ten\")\n        fallthrough\n    case i &lt;= 20:\n        fmt.Println(\"less than or equal to ten\")\n    default:\n        fmt.Println(\"greater than 20\")\n    }  \n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/ControlFlow/#type-switches","title":"Type Switches","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // interface can take any type assignment\n    var i interface{} = 1\n\n    // get i  type\n    switch i.(type) {\n    case int:\n        fmt.Println(\"i is an int\")\n    case float32:\n        fmt.Println(\"i is a float32\")\n    case string:\n        fmt.Println(\"i is  a string\")\n    default:\n        fmt.Println(\"i is another type0\")\n    }  \n}\n</code></pre>"},{"location":"Golang/GoBeginners/ControlFlow/#break","title":"Break","text":"<ul> <li>exit a case statement early <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // interface can take any type assignment\n    var i interface{} = 1\n\n    // get i  type\n    switch i.(type) {\n    case int:\n        fmt.Println(\"i is an int\")\n        break\n        fmt.Println(\"this wont be printed\")\n    case float32:\n        fmt.Println(\"i is a float32\")\n    case string:\n        fmt.Println(\"i is  a string\")\n    default:\n        fmt.Println(\"i is another type0\")\n    }  \n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/DeferPanicRecover/","title":"Defer, Panic, Recover","text":"<ul> <li>Defer, Panic, Recover</li> <li>Defer</li> <li>Panic</li> <li>Recover</li> </ul>"},{"location":"Golang/GoBeginners/DeferPanicRecover/#defer","title":"Defer","text":"<ul> <li><code>defer</code> delays statement execution until function exits</li> <li>LIFO ordered</li> <li>use cases; closing resources</li> <li>group <code>open</code> and <code>close</code> functions together</li> <li>evaluates arguments at the time defer is called <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    fmt.Println(\"start\")\n    defer fmt.Println(\"middle\")\n    fmt.Println(\"end\")\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/DeferPanicRecover/#panic","title":"Panic","text":"<ul> <li>return an error to user when program cannot continue at all</li> <li>occur after defer statements <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n        fmt.Println(\"start\")\n        panic(\"something bad is happening\")\n        fmt.Println(\"end\")\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/DeferPanicRecover/#recover","title":"Recover","text":"<ul> <li>Used to recover from panics</li> <li>Only useful in deferred functions</li> <li>Current function will not continue, but higher functions in call stack will</li> </ul> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n        fmt.Println(\"start\")\n        panicker()\n        fmt.Println(\"end\")\n}\n\nfunc panicker() {\n    fmt.Println(\"About to panic\")\n    defer func() {\n        if err := recover(); err != nil {\n            log.Println(\"Error: \", err)\n            // repanic error if it can't be handled\n            // panic(err)\n        }\n    }()\n    panic(\"something bad happened\")\n}\n</code></pre>"},{"location":"Golang/GoBeginners/Looping/","title":"Looping","text":"<ul> <li>Looping</li> <li>For Statement<ul> <li>Simple Loops</li> <li>Exiting Early</li> <li>Looping through collections</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/Looping/#for-statement","title":"For Statement","text":""},{"location":"Golang/GoBeginners/Looping/#simple-loops","title":"Simple Loops","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // initializer := 0, condition, incrementor\n    // i scoped to for loop\n    for i := 0; i &lt; 10; i++ {\n        fmt.Println(i)\n    }\n\n    // j scoped to main function\n    j := 0\n    for ; j &lt; 10; j++ {\n        fmt.Println(j)\n    }    \n\n    // incrementor inside the loop\n    for i := 0; i &lt; 10;  {\n        fmt.Println(i)\n        i++\n    }\n\n    // effectively a while loop\n    // the two colons (;) around the condition are optional\n    k := 0\n    for ;k &lt; 10 ; {\n        fmt.Println(k)\n        k++\n    }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // multiple counters\n    for i, j := 0, 0; i &lt; 5; i, j = i+1, j+1 {\n        fmt.Println(i, j)\n    }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    //  nested loops\n    for i := 0; i &lt; 3; i++ {\n        for j := 0; j &lt; 3; j++ {\n            fmt.Println(i * j)\n        }\n    }\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/Looping/#exiting-early","title":"Exiting Early","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n\n    k := 0\n    for {\n        fmt.Println(k)\n        k++\n        if k == 5 {\n            // breaks out of closest loop\n            break\n        }\n    }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // breaking out of outer loop\nLoop:\n    for i := 1; i &lt;= 3; i++ {\n        for j := 1; j &lt;= 3; j++ {\n            fmt.Println(i * j)\n            if i*j &gt;= 3 {\n                break Loop\n            }\n        }\n    }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n\n    for i := 0; i &lt; 10; i++ {\n        if i%2 == 0 {\n            continue\n        }\n        fmt.Println(i)\n    }\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/Looping/#looping-through-collections","title":"Looping through collections","text":"<ul> <li>can be done for slices, arrays, maps. strings and channels</li> <li>assign  <code>_</code> to key or value when you don't plan on using it <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    s := []int{1,2,3}\n    for k, v := range s {\n        fmt.Println(k, v)\n    }\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/MapAndStructs/","title":"Maps and Structs","text":"<ul> <li>Maps and Structs</li> <li>Maps<ul> <li>What are Maps</li> <li>Creating Maps</li> <li>Manipulation</li> </ul> </li> <li>Structs<ul> <li>What are Structs</li> <li>Creating Structs</li> <li>Naming Convention</li> <li>Embedding</li> <li>Tags</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/MapAndStructs/#maps","title":"Maps","text":""},{"location":"Golang/GoBeginners/MapAndStructs/#what-are-maps","title":"What are Maps","text":"<ul> <li>Maps are collections of key value pairs</li> <li>type has to be consistent between each key value pair</li> <li>slice, maps, and function cannot be key to a map</li> <li>unordered</li> <li>copies reference same underlying data</li> </ul>"},{"location":"Golang/GoBeginners/MapAndStructs/#creating-maps","title":"Creating Maps","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    // map[key type]valueType{key:value,}\n    statePopulation := map[string]int{\n        \"California\": 39,\n        \"Texas\": 27,\n        \"Florida\": 20,\n    }\n    fmt.Println(statePopulation)\n    // map[California:39 Florida:20 Texas:27]\n\n    // creating  a map using make function\n    statePop := make(map[string]int)\n}\n</code></pre>"},{"location":"Golang/GoBeginners/MapAndStructs/#manipulation","title":"Manipulation","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    statePopulation := map[string]int{\n        \"California\": 39,\n        \"Texas\": 27,\n        \"Florida\": 20,\n    }\n\n    // accessing map items\n    fmt.Println(statePopulation[\"Texas\"])\n\n    // add to map\n    statePopulation[\"Georgia\"] = 10\n    fmt.Println(statePopulation[\"Georgia\"])\n\n    // delete from map\n    delete(statePopulation, \"Georgia\")\n    fmt.Println(statePopulation[\"Texas\"])\n\n    // check if key is in map\n    pop, ok := statePopulation[\"Ohio\"]\n    fmt.Println(pop, ok) // 0 False\n\n    // find number of elements in ma\n    fmt.Println(len(statePopulation))\n\n}\n</code></pre>"},{"location":"Golang/GoBeginners/MapAndStructs/#structs","title":"Structs","text":""},{"location":"Golang/GoBeginners/MapAndStructs/#what-are-structs","title":"What are Structs","text":"<ul> <li>collection of related information</li> <li>unlike maps and arrays, types of field in Structs can be anything</li> <li>copies create independent operators</li> <li>use <code>new := &amp;old</code> to make copies point to same references</li> </ul>"},{"location":"Golang/GoBeginners/MapAndStructs/#creating-structs","title":"Creating Structs","text":"<p><pre><code>package main\n\nimport (\"fmt\")\n\ntype Pokemon struct {\n    number int\n    name string\n    types []string\n}\n\nfunc main() {\n    aPokemon := Pokemon{\n        number: 004,\n        name: \"charmander\",\n        types: []string{\n            \"fire\",\n        },\n    }\n    fmt.Println(aPokemon)\n\n    // access value from a struct\n    fmt.Println(aPokemon.name)\n    fmt.Println(aPokemon.types[0])\n}\n</code></pre> <pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    // anonymous struct\n    d := struct{name string} {name: \"John\"}\n    fmt.Println(d)\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/MapAndStructs/#naming-convention","title":"Naming Convention","text":"<ul> <li>uppercase struct name or strut field names if you want them to be exported</li> </ul>"},{"location":"Golang/GoBeginners/MapAndStructs/#embedding","title":"Embedding","text":"<ul> <li>Go support composition instead of inheritance</li> </ul> <pre><code>package main\n\nimport (\"fmt\")\n\ntype Animal struct {\n    Name string\n    Origin string\n}\n\ntype Bird struct {\n    Animal\n    speedKPH float32\n    CanFly bool\n}\n\nfunc main() {\n    b := Bird{}\n    // Animal fields\n    b.Name = \"Swellow\"\n    b.Origin = \"Hoenn\"\n\n    // bird fields\n    b.speedKPH = 35\n    b.CanFly = true \n    fmt.Println(b)\n\n}\n</code></pre>"},{"location":"Golang/GoBeginners/MapAndStructs/#tags","title":"Tags","text":"<ul> <li>tags can act as documentation for struct fields <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"reflect\"\n)\n\ntype Animal struct {\n    Name   string `required max:\"100\"`\n    Origin string\n}\n\nfunc main() {\n    t := reflect.TypeOf(Animal{})\n    field, _ := t.FieldByName(\"Name\")\n    fmt.Println(field.Tag)\n\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/constants/","title":"Constants","text":"<ul> <li>Constants</li> <li>Naming Convention</li> <li>Typed Constants</li> <li>Untyped Constants</li> <li>Enumerated Constants</li> <li>Enumeration Expressions</li> </ul>"},{"location":"Golang/GoBeginners/constants/#naming-convention","title":"Naming Convention","text":"<ul> <li>all constants are preceded with the <code>const</code> keyword</li> <li>naming in <code>camelCase</code></li> <li>value can't be changed</li> <li>has to be assignable at compile time </li> <li>can be shadowed <pre><code>package main \n\nimport (\"fmt\")\n\nfunc main () {\n    const myConst int = 42\n    fmt.Printf(\"%v, %T\", myConst, myConst)\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/constants/#typed-constants","title":"Typed Constants","text":"<ul> <li>can be made up any of the primitive types</li> </ul>"},{"location":"Golang/GoBeginners/constants/#untyped-constants","title":"Untyped Constants","text":"<ul> <li>compiler can infer the type of a constant</li> <li>can handle implicit type conversion, if an untyped constant is used in a function</li> <li><code>go         const n = 22         var b int16 = 28         n + b // will be of type int16</code> ```go package main </li> </ul> <p>import (\"fmt\")</p> <p>func main () {     const myConst = 42     fmt.Printf(\"%v, %T\", myConst, myConst) } <pre><code>## Enumerated Constants\n- first value of `iota` is 0, which may create a false match if some of your logic expects on of the enumerated types to be 0\n```go\npackage main \n\nimport (\"fmt\")\n\n// const block\nconst (\n    a = iota\n    b = iota\n    c = iota\n)\n\nconst (\n    a2 = iota\n)\n\nfunc main () {\n    fmt.Printf(\"%v\\n\", a) // 0\n    fmt.Printf(\"%v\\n\", b) // 1\n    fmt.Printf(\"%v\\n\", c) // 2\n\n    // iota scoped to const block\n    fmt.Printf(\"%v\\n\", a2) // 0\n\n}\n</code></pre> - can use <code>_</code> as variable name, <code>write only variable</code>, for the first value if we don't care for it</p>"},{"location":"Golang/GoBeginners/constants/#enumeration-expressions","title":"Enumeration Expressions","text":"<ul> <li>can define enumeration values dynamically by using either arithmetic, bitwise, or bit-shifting operations <pre><code>package main \n\nimport (\"fmt\")\n\n// const block\nconst (\n    // can offset values in const block\n    a = iota + 5\n    b\n    c\n)\n\nfunc main () {\n    fmt.Printf(\"%v\\n\", a) // 5\n    fmt.Printf(\"%v\\n\", b) // 6\n    fmt.Printf(\"%v\\n\", c) // 7\n}\n</code></pre> <pre><code>package main \n\nimport (\"fmt\")\n\n// const block\nconst (\n    isAdmin = 1 &lt;&lt; iota\n    isHeadquarters\n    canSeeFinancial\n)\n\nfunc main () {\n    var roles byte = isAdmin | canSeeFinancial\n    fmt.Printf(\"%b/n\", roles) // 101\n\n    // apply bit mask to see if user has permission\n    fmt.Printf(\"Is Admin? %v\", isAdmin &amp; roles == isAdmin) // true\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/functions/","title":"Functions","text":"<ul> <li>Functions</li> <li>Basic Syntax</li> <li>Parameters</li> <li>Return Values</li> <li>Anonymous Functions</li> <li>Functions as types</li> <li>Methods</li> </ul>"},{"location":"Golang/GoBeginners/functions/#basic-syntax","title":"Basic Syntax","text":"<ul> <li>lower case or uppercase name of function results in it being either public or internal to the package <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\n// basic function scaffold\nfunc main() {\n\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/functions/#parameters","title":"Parameters","text":"<ul> <li>can be values, in which case Go will make a copy and pass to the function scope</li> <li>can also be pointers, in which case if the function make any changes the parameters, the changes persist outside of the functions scope</li> <li>maps and slices have internal pointers referencing their underlying data, so function will always behave as if a pointers are being passed <pre><code>// arguments to function\nfunc sayMessage(msg string, i int) {\n    fmt.Println(msg)\n    fmt.Println(i)\n}\n</code></pre> <pre><code>// arguments to function\nfunc sayMessage(msg string, name string) {\n    fmt.Println(msg, name)\n}\n\n\n// go will infer type to be the same for msg and name\nfunc sayMessage(msg , name string) {\n    fmt.Println(msg, name)\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    msg := \"Hello \"\n    name := \"Yemane\"\n    sayMessage(&amp;msg, &amp;name)\n    fmt.Println(msg, name)\n}\n\n// function takes pointers as arguments\nfunc sayMessage(msg, name *string) {\n    fmt.Println(*msg, *name)\n    *msg = \"Goodbye \"\n    *name = \"Yemane!\"\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    sum(1, 2, 3, 4, 5)\n}\n\n// variadic parameters\n// values because a slice of all int passed\nfunc sum(values ...int) {\n    fmt.Println(values)\n    result := 0\n    for _, v := range values {\n        result += v\n    }\n    fmt.Println(result)\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/functions/#return-values","title":"Return Values","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    s := sum(1, 2, 3, 4, 5)\n    fmt.Println(s)\n}\n\n// must indicate return type in function signature\nfunc sum(values ...int) int {\n    fmt.Println(values)\n    result := 0\n    for _, v := range values {\n        result += v\n    }\n    return result\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    s := sum(1, 2, 3, 4, 5)\n    fmt.Println(s)\n}\n\n// named return variable\nfunc sum(values ...int) (result int) {\n    fmt.Println(values)\n    for _, v := range values {\n        result += v\n    }\n    return \n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    s := sum(1, 2, 3, 4, 5)\n    fmt.Println(*s)\n}\n\n// must indicate return type in function signature\nfunc sum(values ...int) *int {\n    fmt.Println(values)\n    result := 0\n    for _, v := range values {\n        result += v\n    }\n    return &amp;result\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    d, err := divide(5.0, 0)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    fmt.Println(d)\n}\n\n// multiple return value, err handling\nfunc divide(a, b float64) (float64, error) {\n    if b == 0 {\n        return 0.0, fmt.Errorf(\"Cannot divide by zero\")\n    }\n    return a / b, nil\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/functions/#anonymous-functions","title":"Anonymous Functions","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // immediately invoked anonymous function\n    func(name string) {\n        fmt.Println(\"Hello \", name)\n    }(\"Yemane\")\n\n    // anonymous function as a variable\n    f := func(name string) {\n        fmt.Println(\"Hello \", name, \" Again\")\n    }\n    f(\"Yemane\")\n}\n</code></pre>"},{"location":"Golang/GoBeginners/functions/#functions-as-types","title":"Functions as types","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // declaring function type\n    var divide func(float64, float64) (float64, error)\n\n  divide = func(a, b float64) (float64, error) {\n        if b == 0 {\n            return 0.0, fmt.Errorf(\"Cannot divide by zero\")\n        }\n        return a / b, nil\n    }\n    d, err := divide(5.0, 0)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    fmt.Println(d)\n}\n</code></pre>"},{"location":"Golang/GoBeginners/functions/#methods","title":"Methods","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    //\n    g := greeter{\n        greeting: \"hello\",\n        name: \"Go\",\n    }\n    // use method\n    g.greet()\n}\n\n// greeter struct\ntype greeter struct {\n    greeting string\n    name string\n}\n\n// method is a function that executes in a known context (type)\nfunc (g *greeter) greet() {\n    fmt.Println(g.greeting, g.name)\n}\n</code></pre>"},{"location":"Golang/GoBeginners/goroutines/","title":"Goroutines","text":"<ul> <li>Goroutines<ul> <li>Creating Goroutines</li> <li>Synchronization<ul> <li>WaitGroups</li> <li>Mutexes</li> </ul> </li> <li>Parallelism</li> <li>Best Practices</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/goroutines/#creating-goroutines","title":"Creating Goroutines","text":"<ul> <li>use <code>go</code> keyword in front of function call</li> <li>when using anonymous functions, pass data as local variables (arguments) <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg = sync.WaitGroup{}\n\nfunc main() {\n    var msg string = \"Hello\"\n    wg.Add(1)\n\n    go func(msh string) {\n        fmt.Println(msg)\n        wg.Done()   \n    }(msg)\n\n    wg.Wait()\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/goroutines/#synchronization","title":"Synchronization","text":""},{"location":"Golang/GoBeginners/goroutines/#waitgroups","title":"WaitGroups","text":"<ul> <li>use sync.WaitGroup to wait for groups of goroutines to complete</li> <li><code>Add</code> : to inform the wait group that here are more goroutines for it to wait on</li> <li><code>Wait</code>: block the goroutine that is called until with wait group is compleded</li> <li><code>Done</code>: let wait group that one of the goroutine is completed</li> </ul>"},{"location":"Golang/GoBeginners/goroutines/#mutexes","title":"Mutexes","text":"<ul> <li>Use <code>synce.Mutexe</code> and <code>synce.RWMutex</code> to protect data access</li> <li>to ensure only one goroutine is manipulating the data at one time </li> </ul> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nvar wg sync.WaitGroup = sync.WaitGroup{}\nvar counter int = 0\nvar m sync.RWMutex = sync.RWMutex{}\n\nfunc main() {\n    for i := 0; i &lt; 10; i++ {\n        wg.Add(2)\n        m.RLock()\n        go sayHello()\n        m.Lock()\n        go increment()\n    }\n    wg.Wait()\n}\n\nfunc sayHello() {\n    fmt.Printf(\"Hello #%v\\n\", counter)\n    m.RUnlock()\n    wg.Done()\n}\n\nfunc increment() {\n    counter++\n    m.Unlock()\n    wg.Done()\n}\n</code></pre>"},{"location":"Golang/GoBeginners/goroutines/#parallelism","title":"Parallelism","text":"<ul> <li>By default , Go will use CPU threads equal to available cores</li> <li>Change with <code>runtime.GOMAXPROCS</code></li> <li>More threads can increase performance, but to many can slow it down <ul> <li>test when getting close to production</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/goroutines/#best-practices","title":"Best Practices","text":"<ul> <li>Don't create goroutines in libraries</li> <li>let consumer control concurrency</li> <li>When creating a goroutine, know how it will end</li> <li>avoid subtle memory leaks</li> <li>check for race conditions at compile time <pre><code>go run -race main.go\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/interfaces/","title":"Interfaces","text":"<ul> <li>Interfaces<ul> <li>Basics</li> <li>Composing Interfaces</li> <li>Type Conversion<ul> <li>The Empty Interface</li> <li>Type Switches</li> </ul> </li> <li>Implementing with Value vs Pointers</li> <li>Best Practices</li> </ul> </li> </ul>"},{"location":"Golang/GoBeginners/interfaces/#basics","title":"Basics","text":"<ul> <li>Interfaces define behavior  <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    var w Writer = &amp;ConsoleWriter{}\n    n, err := w.Write([]byte(\"Hello Go!\"))\n    fmt.Println(n, err)\n}\n\ntype Writer interface {\n    Write([]byte) (int, error)\n}\n\ntype ConsoleWriter struct {}\n\n// implementing the interface\nfunc (w *ConsoleWriter) Write(data []byte) (int, error) {\n    n, err := fmt.Println(string(data))\n    return n, err\n}\n</code></pre></li> <li>although interfaces are commonly defined along side structs, they can be created with any type <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    myInt := IntCounter(0)\n    var inc Incrementor = &amp;myInt\n    for i := 0; i &lt; 3; i++ {\n        fmt.Println(inc.Increment())\n    }\n\n}\n\ntype Incrementor interface {\n    Increment() int\n}\n\ntype IntCounter int\n\nfunc (ic *IntCounter) Increment() int {\n    *ic++\n    return int(*ic)\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/interfaces/#composing-interfaces","title":"Composing Interfaces","text":"<p><pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    var res WaiterServer = &amp;Restuarant{}\n    res.wait(\"Yemane\")\n    res.serve(\"Yemane\")\n}\n\ntype Waiter interface {\n    wait(name string) string\n}\n\ntype Server interface {\n    serve(name string) string\n}\n\ntype WaiterServer interface {\n    Waiter\n    Server\n}\n\ntype Restuarant struct{}\n\nfunc (r *Restuarant) wait(name string) string {\n    fmt.Println(\"May I take your order \", name)\n    return name\n}\n\nfunc (r *Restuarant) serve(name string) string {\n    fmt.Println(\"Serving your food! \", name)\n    return name\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // passing only the Waiter part of the Restuarant\n    var res Waiter = &amp;Restuarant{}\n    res.wait(\"Yemane\")\n\n    // will not be implemented and will result in error\n    res.serve(\"Yemane\")\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/interfaces/#type-conversion","title":"Type Conversion","text":"<pre><code>func main() {\n    var r WaiterServer = &amp;Restuarant{}\n    r.wait(\"Yemane\")\n    r.serve(\"Yemane\")\n\n    // type conversion\n    rs := r.(WaiterServer)\n    fmt.Println(rs)\n}\n</code></pre>"},{"location":"Golang/GoBeginners/interfaces/#the-empty-interface","title":"The Empty Interface","text":"<ul> <li>interface with no methods</li> <li>every type in Go implements the empty interface <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    var myObj EmptyInterface = &amp;Bartender{}\n    if w, ok := myObj.(Waiter); ok {\n        w.wait(\"Yemane\")\n    }\n}\n\ntype EmptyInterface interface{}\n\ntype Waiter interface {\n    wait(name string)\n}\n\ntype Bartender struct{}\n\nfunc (b *Bartender) wait(name string) {\n    fmt.Println(\"Would you like a drink? \", name)\n}\n</code></pre></li> </ul>"},{"location":"Golang/GoBeginners/interfaces/#type-switches","title":"Type Switches","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    // empty interface\n    var myObj interface{} = 0\n    switch myObj.(type) {\n    case int:\n        fmt.Println(\"myObj is an int\")\n    case string:\n        fmt.Println(\"myObj is a string\")\n    default:\n        fmt.Println(\"myObj is of unknown type\")\n    }\n}\n</code></pre>"},{"location":"Golang/GoBeginners/interfaces/#implementing-with-value-vs-pointers","title":"Implementing with Value vs Pointers","text":"<ul> <li>method set of <code>value</code> is all methods with value receivers</li> <li>method set of <code>pointer</code> is all methods, regardless of receiver type</li> <li></li> </ul>"},{"location":"Golang/GoBeginners/interfaces/#best-practices","title":"Best Practices","text":"<ul> <li>Use many, small interfaces over large monolithic ones</li> <li>Don't export interfaces for types that will be consumed</li> <li>do export interfaces for types that will be used by package</li> <li>Design functions and methods to receive interfaces whenever possible</li> </ul>"},{"location":"Golang/GoBeginners/pointers/","title":"Pointers","text":"<ul> <li>Pointers</li> <li>Creating Pointers</li> <li>Dereferencing Pointers</li> <li>The new Function</li> <li>Working with nil</li> <li>Types With Internal Pointers</li> </ul>"},{"location":"Golang/GoBeginners/pointers/#creating-pointers","title":"Creating Pointers","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    var a int = 42\n\n  // *int is a pointer to int type\n    // &amp; gets the address of a\n  var b *int = &amp;a\n  fmt.Println(&amp;a, b)     \n}\n</code></pre>"},{"location":"Golang/GoBeginners/pointers/#dereferencing-pointers","title":"Dereferencing Pointers","text":"<pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    var a int = 42\n    var b *int = &amp;a\n  fmt.Println(&amp;a, b)\n\n  // precede pointer with * to dereference the pointer\n    fmt.Println(a, *b)\n\n}\n</code></pre>"},{"location":"Golang/GoBeginners/pointers/#the-new-function","title":"The new Function","text":"<p><pre><code>package main\n\nimport (\"fmt\")\n\n\nfunc main() {\n    var ms *myStruct\n  // initializing a variable to a pointer to an object\n  ms = &amp;myStruct{foo: 21}\n    fmt.Println(ms)\n}\n\ntype myStruct struct {\n    foo int\n}\n</code></pre> - another way to create pointers to an object   - can't initialize the fields at the same time <pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    var ms *myStruct\n    ms = new(myStruct)\n\n    // go dereference object pointers automatically\n    ms.foo = 32 // (*ms).foo = 32\n    fmt.Println(ms.foo) // fmt.Println((*ms).foo)\n}\n\ntype myStruct struct {\n    foo int\n    bar string\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/pointers/#working-with-nil","title":"Working with nil","text":"<pre><code>package main\n\nimport (\"fmt\")\n\n\nfunc main() {\n    var ms *myStruct\n\n  // print uninitialized pointer: &lt;nil&gt;\n  fmt.Println(ms)\n\n  // initializing a variable to a pointer to an object\n  ms = &amp;myStruct{foo: 21}\n    fmt.Println(ms)\n}\n\ntype myStruct struct {\n    foo int\n}\n</code></pre>"},{"location":"Golang/GoBeginners/pointers/#types-with-internal-pointers","title":"Types With Internal Pointers","text":"<ul> <li>all assignment operations in Go are copy operations</li> <li>slices and maps contain internal pointers, so copies point to same underlying data</li> </ul> <p><pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n  // since slices are made up of pointers, bot a, b change\n    a := []int{1, 2, 3}\n    b := a\n    fmt.Println(a, b)\n    a[1] = 31\n    fmt.Println(a, b)\n\n  // [1 2 3] [1 2 3]\n  // [1 31 3] [1 31 3]\n}\n</code></pre> <pre><code>package main\n\nimport (\"fmt\")\n\nfunc main() {\n    a := map[string]string{\"foo\": \"bar\", \"baz\": \"buz\"}\n    b := a\n    fmt.Println(a, b)\n    a[\"foo\"] = \"qux \"\n    fmt.Println(a, b)\n\n  // map[baz:buz foo:bar] map[baz:buz foo:bar]\n  // map[baz:buz foo:qux ] map[baz:buz foo:qux ]\n}\n</code></pre></p>"},{"location":"Golang/GoBeginners/primitives/","title":"Primitive Types","text":"<ul> <li>Primitive Types</li> <li>Boolean type</li> <li>Numeric types<ul> <li>Integers</li> <li>Floating</li> <li>Complex numbers</li> </ul> </li> <li>Text types</li> </ul>"},{"location":"Golang/GoBeginners/primitives/#boolean-type","title":"Boolean type","text":"<p><pre><code>var n bool = false\nvar y bool = true\n</code></pre> <pre><code>// get boolean by comparing\nn := 1 == 1 // true\nm := 1 == 2 // false\n</code></pre> <pre><code>// every initialized variable has a default 0 value\n// bool of 0 is false\nvar n bool\nfmt.Printf(\"%v, %T\", n, n)\n</code></pre></p>"},{"location":"Golang/GoBeginners/primitives/#numeric-types","title":"Numeric types","text":""},{"location":"Golang/GoBeginners/primitives/#integers","title":"Integers","text":"<ul> <li>options</li> <li>signed integers<ul> <li>int8, int16, int32, int64</li> </ul> </li> <li>unsigned integers<ul> <li>uint8, uint16, uint32, uint64</li> </ul> </li> </ul> <p><pre><code>package main\n\nimport (\"fmt\")\n\n// basic arithmetic operations\nfunc main() {\n  a := 10\n  b := 3\n  fmt.Println(a + b)\n  fmt.Println(a - b)\n  fmt.Println(a / b)\n  fmt.Println(a % b)  \n}\n</code></pre> <pre><code>package main\n\nimport (\"fmt\")\n\n// basic bit operations\nfunc main() {\n  a := 10 // 1010\n  b := 3 // 0011\n  fmt.Println(a &amp; b) // ANG 1011\n  fmt.Println(a | b) // OR 1011\n  fmt.Println(a ^ b) // XOR 1001\n  fmt.Println(a &amp;^ b)  // AND NOT 1000\n}\n</code></pre></p> <pre><code>package main\n\nimport (\"fmt\")\n\n// bit shifting\nfunc main() {\n  a := 8 // 2^3\n  fmt.Println(a &lt;&lt; 3) // 2^3 * 2^3 = 2^6\n  fmt.Println(a &gt;&gt; 3) // 2^3 / 2^3 = 2^0\n}\n</code></pre>"},{"location":"Golang/GoBeginners/primitives/#floating","title":"Floating","text":"<p><pre><code>package main\n\nimport (\"fmt\")\n\n// \nfunc main() {\n  a := 3.14\n  n = 13.9e10 // declare using exponential notation\n}\n</code></pre> - same arithmetic operators as integers are available - bit operators are not available</p>"},{"location":"Golang/GoBeginners/primitives/#complex-numbers","title":"Complex numbers","text":"<p><pre><code>package main\n\nimport (\"fmt\")\n\n\nfunc main() {\n  // go parser understands the i literal as an imaginary number\n  var n  complex64 = 1 + 2i\n  fmt.Printf(\"%v, %T\", n, n)\n\n  var j  complex64 = complex(1, 2)\n  fmt.Printf(\"%v, %T\", j, j)\n\n  // isolate the real and imaginary components\n  fmt.Printf(\"%v, %T\", real(n), imag(n))\n}\n</code></pre> - arithmetic operators available</p>"},{"location":"Golang/GoBeginners/primitives/#text-types","title":"Text types","text":"<ul> <li>string are UTF-8 and immutable <pre><code>package main\n\nimport (\"fmt\")\n\n// \nfunc main() {\n  s := \"This is a string\"\n  fmt.Printf(\"%v, %T\", s, s)\n\n  // string can be sliced, type conversion required to get string back\n  fmt.Printf(\"%v, %T\", string(s[2]), s)\n\n  // concatenation\n  s2 := \"This is also a string\"\n  fmt.Printf(\"%v, %T\", s + s2, s + s2)\n\n  // convert string to a collection of byes\n  b := []byte(s)\n  fmt.Printf(\"%v, %T\", s, s)\n}\n</code></pre></li> <li>Rune is UTF-32</li> </ul>"},{"location":"Golang/GoBeginners/variables/","title":"Variables","text":"<ul> <li>Variables</li> <li>Variable Declaration</li> <li>Re-declaration and shadowing</li> <li>Visibility</li> <li>Naming Conventions</li> <li>Type Conversion</li> </ul>"},{"location":"Golang/GoBeginners/variables/#variable-declaration","title":"Variable Declaration","text":"<p><pre><code>// declare  a variable and then assign a value\nvar i int\ni = 42\n\n// one-liner declare and assign\nvar j int = 42\n\n// don't declare type, (let go interpret)\n// generally the preferred method\nk := 42\n</code></pre> <pre><code>// block of variable declaration at package level\nvar (\n    name string = \"Hello\"\n    date string = \"Halloween\"\n    age int = 24\n)\n</code></pre></p>"},{"location":"Golang/GoBeginners/variables/#re-declaration-and-shadowing","title":"Re-declaration and shadowing","text":"<ul> <li>Variable can only be declared once within each scope, or an error will occur</li> <li>Variable with the inner most scope takes precedence (shadowing)</li> <li>all declared variable must be used</li> </ul>"},{"location":"Golang/GoBeginners/variables/#visibility","title":"Visibility","text":"<ul> <li>lowercase variables are scoped to the package</li> <li>all src files in that package have access to it</li> <li>uppercase variable, are exposed to the outside word</li> </ul>"},{"location":"Golang/GoBeginners/variables/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>length of variable name should reflect the life of the variable</li> <li>short names are preferred</li> <li>acronym should be in all uppercase (ex URL)</li> </ul>"},{"location":"Golang/GoBeginners/variables/#type-conversion","title":"Type Conversion","text":"<p><pre><code>// declare and set variable i\nvar i int = 42\n// declare variable j\nvar j float32\n\n// use type as a function to convert types\nj = float32(i)\n</code></pre> <pre><code>// import to handle conversion to string\nimport (\"strconv\")\n\n\nvar i int = 42\n\n//  integer to ascii\nj = strconv.Itoa(i)\n</code></pre></p>"},{"location":"Golang/LearnGoWithTests/BuildAnApplication/","title":"Notes","text":"<ol> <li>Implement <code>http.Handler</code> to create web servers</li> <li><code>http.HandlerFunc</code> turns ordinary functions into http.Handlers</li> <li>Use <code>httptest.NewRecorder</code> to pass in as a <code>ResponseWriter</code> to let you spy on the responses your handler sends</li> <li>Use <code>http.NewRequest</code> to construct the requests you expect to come in to your system</li> <li>Using <code>interfaces</code> and <code>mocking</code> allows your to start coding in small steps. Because you don't need to create large requirements (like a db) and focus on the logic.</li> <li>TDD to drive out the interfaces you need</li> <li>use <code>http.NewServeMux()</code> for routing</li> <li>use <code>Type embedding</code>to compose new structs or interfaces. Should only expose what's appropriate</li> <li>use <code>encoding/json</code> to serialize and deserialize json</li> <li><code>Seeker</code> interface to go to beginning of file</li> <li><code>sort.Slice</code> for sorting slices</li> <li><code>os.File</code> to handle files, <code>Truncate</code> to change size of file</li> <li><code>os.Stdin</code> and <code>bufio.Scanner</code> to read from standard input</li> <li>test using <code>mypackage_test</code> package name to see how others use your code</li> <li>export test helpers and stubs to make them easier to use and allow others importing the code easier ways to test</li> <li>update package structure to include <code>folders for each separate application</code>. reuse domain code by importing module</li> <li><code>time.Afterfunc</code>: scheduling a function call after a specific duration</li> <li><code>time.After(duration)</code>: returns a chan Time when the duration has expired. So if you wish to do something after a specific time, this can help.</li> <li><code>time.NewTicker(duration)</code>: returns a Ticker which is similar to the above in that it returns a channel but this one \"ticks\" every duration, rather than just once. This is very handy if you want to execute some code every N duration.</li> <li><code>separation of concerns</code>: separate the responsibilities of dealing with user input and responses away from domain code</li> <li><code>messy tests</code><ol> <li>If your tests look messy try and refactor them</li> <li>If you've done this and they're still a mess it is very likely pointing to a flaw in your design</li> </ol> </li> <li><code>function implementing an interface</code>: <ol> <li>When you define an interface with one method in it you might want to consider defining a MyInterfaceFunc type to complement it so users can implement your interface with just a function.</li> <li>In Go you can add methods to types, not just structs. This is a very powerful feature, and you can use it to implement interfaces in more convenient ways.</li> </ol> </li> <li>user helper functions to retry assertions and add timeouts</li> <li>use go routines to ensure the assertions don't block anything and then use channels to let them signal that they have finished, or not</li> </ol>"},{"location":"Golang/LearnGoWithTests/Fundamentals/01.hello_world/","title":"Hello World","text":""},{"location":"Golang/LearnGoWithTests/Fundamentals/01.hello_world/#general","title":"General","text":"<ul> <li><code>go mod init &lt;name&gt;</code> to initialize your project as a go module</li> <li><code>func</code>  define a function</li> <li><code>varName := value</code> to declare a variable</li> <li><code>const</code> keyword to define constants </li> <li><code>//</code> used to create comments</li> <li>functions can be created an assigned to variables within other functions</li> <li>function arguments must have types declared</li> <li>function return value must have type declared</li> <li><code>(name type)</code> syntax is used to define a name return value</li> <li>will create a \"name\" variable and assign a type appropriate zero value to it</li> <li>can do <code>return</code> instead of <code>return name</code></li> <li><code>default</code> in the <code>switch</code> case will be branched to if none of the other case statements match</li> <li><code>public</code> functions start with a <code>capital</code> letter and <code>private</code> ones start with a <code>lowercase</code></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/01.hello_world/#testing","title":"Testing","text":"<ul> <li>separate domain code from side-effects</li> <li>ie) separate the creation of a string from the act of printing it</li> <li>file name needs to follow pattern of <code>xxx_test.go</code></li> <li>test function must start with the word TEST</li> <li>test function takes one argument only <code>t *testing.T</code></li> <li>to use the *testing.T type, you need to import <code>\"testing\"</code></li> <li><code>t.Errorf</code> print a message when a test fails</li> <li>helper functions, it's a good idea to accept a <code>testing.TB</code></li> <li><code>t.Helper()</code>  tells the test suite that this method is a helper</li> <li>line number reported will be in our function call rather than inside our test helper</li> </ul> <p>Cycle - Write a test - make the compiler pass - run the test, check if it fails - write enough code to make code pass - refactor</p>"},{"location":"Golang/LearnGoWithTests/Fundamentals/02.Integers/","title":"Integers","text":"<ul> <li>can add <code>documentation</code> to function <code>with comments</code> immediately above them</li> <li>appear in Go Doc, and in intellisense</li> <li>go <code>examples</code> can be defined similar like tests, and reside in the test files</li> <li>examples can be <code>ran as tests</code></li> <li>examples <code>show up in the Go Docs</code> </li> <li><code>go get</code> to download packages</li> <li><code>go install</code> to install the packages to go path</li> <li><code>golang.org/x/tools/cmd/godoc</code> go package to give GoDoc</li> <li><code>godoc -http=:6060</code> serve go docs in <code>http://localhost:6060/pkg/</code></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/03.Iteration/","title":"Iteration","text":"<ul> <li>only way to loop in go is using a <code>for loop</code></li> <li><code>benchmarking</code> written like tests</li> <li>used to test the average time a function takes to perform its task</li> <li></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/04.ArraysAndSlices/","title":"Arrays and Slices","text":"<ul> <li><code>arrays</code> define size in declaration</li> <li><code>slices</code> are like array</li> <li>have fixed capacity but can create new ones from old ones using <code>slice = append(slice, additionalSlice)</code></li> <li>can slice slices using <code>[start:stop]</code> syntax</li> <li><code>len</code> can get the length of of an array or slice</li> <li><code>go test -cover</code> to get test coverage</li> <li><code>reflect.DeepEqual</code> to compare slices</li> <li>can reduce type safety of code<ul> <li>can use a wrapper function to handle type unsafety</li> </ul> </li> <li>slices can be made of elements of any type, even other slices</li> <li>ex) <code>[][]string</code></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/05.StructsMethodsInterfaces/","title":"Struct, Methods, &amp; Interfaces","text":"<ul> <li><code>struct</code> is a named collection of fields where you can store data</li> <li>can access fields of a struct with <code>struct.field</code> syntax</li> <li><code>g</code> will print a <code>more precise decimal</code> number than <code>f</code></li> <li><code>method</code> is a function with a receiver (a struct)</li> <li>called by invoking them on an instance of a particular type</li> <li><code>interface</code> define functions that can be used by different types</li> <li>if a struct has those methods then they implement the interface</li> <li>used to decouple as the internals of a struct can be hidden if they are referred to thorough there interface</li> <li><code>table driven tests</code> are used to test a list of test cases that can be tested in the same manner</li> <li><code>%#v</code> will print a struct with the fields and values</li> <li></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/06.PointersErrors/","title":"Pointers and Errors","text":"<ul> <li><code>pointers</code></li> <li>go copies values when you pass them to functions/methods</li> <li>if you are writing a function that needs to mutate state you'll need it to take a pointer to the thing you want to change</li> <li>done with the <code>*variable</code> syntax</li> <li><code>nil</code></li> <li>pointers can be nil</li> <li>when a function returns a pointer, check if it is nil to avoid runtime exceptions</li> <li><code>Errors</code></li> <li>errors are the way to signify failure when calling a function</li> <li>The <code>var</code> keyword allows us to define values global to the package.</li> <li>can be set as global variables to be more useful</li> <li><code>type CustomName OriginalType</code>: Create new type from existing ones</li> <li><code>CustsomName(inputs)</code> to create an instance of the type</li> <li>Implement the <code>String()</code> interface on a custom struct to define how type is printed when used with <code>%s</code></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/07.Maps/","title":"Maps","text":"<ul> <li><code>maps</code> allow you to store items in a manner similar to a dictionary</li> <li>can create a new type out of dictionary and add methods to it</li> <li><code>var dictionary = map[string]string{}</code> or <code>var dictionary = make(map[string]string)</code></li> <li><code>d[word] = definition</code> adding or updating a key/value pair in the dictionary</li> <li><code>delete(d, word)</code> to delete key <code>word</code> from dictionary <code>d</code></li> <li>can create custom types, that implement the error interface</li> <li>improves reusability</li> <li>declare with <code>const</code> to make them immutable</li> <li>having specific errors gives better information about what went wrong</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/08.DependencyInjection/","title":"Dependency Injection","text":"<ul> <li>by having familiarity with the <code>io.Writer</code> interface</li> <li>we were able to use the <code>bytes.Buffer</code> in our tests</li> <li> <p>and the <code>use other writers</code> in the standard library to use our function in a command line or web server</p> </li> <li> <p><code>test code</code></p> </li> <li>if you can't test a function easily, it's usually because of dependencies being hard wired into a function or global state</li> <li>dependency injection motivates you to inject a dependency via an interface to mock out the resource which enables testing</li> <li><code>separate concerns</code></li> <li>decouple where data goes from how it is generated</li> <li><code>allow code ot be reusable</code></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/09.Mocking/","title":"Mocking","text":"<p><code>Take a thin slice of functionality and make it work end-to-end, backed by tests.</code></p> <ul> <li><code>mocking</code> create interfaces that real code and test code can implement</li> <li>allows you to use dependency injection</li> <li><code>Spies</code> are a kind of mock which can <code>record how a dependency is used</code>. They can record the arguments sent in, how many times it has been called, etc.</li> <li>introduce tight coupling with tests and implementation, use only if important</li> <li><code>favour testing expected behavior</code> rather than the implementation</li> <li>if mocking code become complicated</li> <li>The thing you are testing is having to do too many things</li> <li>Your test is too concerned with implementation details</li> <li><code>a lot of mocking points to bad abstraction in your code.</code></li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/10.Concurrency/","title":"Concurrency","text":"<ul> <li><code>goroutines</code> the basic unit of concurrency in go</li> <li><code>anonymous functions</code></li> <li>don't need name</li> <li>can be called as their defined</li> <li>used to start each of the concurrent processes</li> <li><code>channels</code> help organize and control the communication between the different processes</li> <li>helps avoid race conditions</li> <li><code>channel &lt;- data</code> or <code>x :=channel</code> </li> <li>send statements to control the direction data moves in channels</li> <li><code>go test -bench=\".\"</code> to benchmark tests</li> <li><code>go test -race</code> check fo race conditions</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/10.Concurrency/#quotes","title":"quotes","text":"<p>Make it work, make it right, make it fast - Kent Beck</p> <p>premature optimization is the root of all evil - Donald Knuth</p>"},{"location":"Golang/LearnGoWithTests/Fundamentals/11.Select/","title":"Select","text":"<ul> <li><code>select</code> helps you wait on multiple channels</li> <li>use <code>time.After</code> to prevent system from blocking forever</li> <li><code>httptest</code> helps create reliable http tests</li> <li>same interfaces as the \"real\" <code>net/http</code> servers</li> <li><code>defer</code> allows you to call function after the end of the containing function</li> <li>keep the instruction near where resource is created for clarity</li> <li><code>struct{}</code> is the smallest data type available from a memory perspective</li> <li><code>Always make channels</code></li> <li>using var declaration gives it a 0 value of nil, which won't allow you to pass data to it</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/12.Reflection/","title":"Reflection","text":"<ul> <li><code>interface{}</code> is a way to type hint any type</li> <li><code>any</code> is an alisa for interface{}</li> <li><code>reflection</code> is a package that helps inspect tha value and type of stuff</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/13.Sync/","title":"Sync","text":"<ul> <li><code>Mutex</code> allows us to add locks to data</li> <li>helps when concurrent processes are updating the same data</li> <li><code>WaitGroup</code> is a means of waiting for a group of goroutines to finish there jobs</li> <li><code>mutex vs channels</code> : use channels when apssing ownership of data and mutexes for managing state</li> <li><code>go vet</code> command used to alert you if there are subtle bugs in code</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/14.Context/","title":"Context","text":"<p>Software often kicks off long-running, resource-intensive processes (often in goroutines). If the action that caused this gets cancelled or fails for some reason you need to stop these processes in a consistent way through your application.</p> <ul> <li><code>context</code> package helps manage long running processes</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/15.PropertyTests/","title":"Property Based Tests","text":"<ul> <li><code>property based tests</code> are a way to test our code against the rules of the domain</li> <li><code>testing/quick</code> is a package in the standard library that helps with property based tests</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/16.Maths/","title":"Maths","text":"<ul> <li>created multiple packages</li> <li>make clockface functions public apis and allow the other packages to import them</li> <li>floats are inaccurate'</li> <li><code>encoding/xml</code> is a package used to parse xml</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/17.ReadingFiles/","title":"Notes","text":"<ul> <li><code>bufio</code> : scanner to read file line by line</li> <li><code>fs.FS</code> : read data from file systems and test them simply</li> <li><code>strings</code> : built in functions for string manipulation</li> <li><code>bytes.Buffer</code> + <code>fmt</code> : print to buffer to concatenate text</li> <li>create <code>cmd</code> folder to use code as an executable</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/17.ReadingFiles/posts/post1/","title":"Post 1","text":"<p>This is my first blog post!</p>"},{"location":"Golang/LearnGoWithTests/Fundamentals/17.ReadingFiles/posts/post2/","title":"Post 2","text":"<p>This is my second blog post!</p>"},{"location":"Golang/LearnGoWithTests/Fundamentals/18.Templating/","title":"Templating Notes","text":"<ul> <li>create, compose, and render HTML</li> <li>Approval tests : helps test ugly output of things like files</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/19.Generics/","title":"Generics","text":"<ul> <li>generics</li> <li>make functions/structs able to work with more than one type, reducing duplication</li> <li> <p>constrained function/structs to only work with one type at a time (can't mix apples and oranges)</p> </li> <li> <p>generics vs interface{} : similarities</p> </li> <li>both can be used to define types that accept many types</li> <li> <p>for example define a function that can work with both string and ints</p> </li> <li> <p>interface{}</p> </li> <li>Less safe (mix apples and oranges), requires more error handling</li> <li>Less expressive, interface{} tells you nothing about the data</li> <li>More likely to rely on reflection, type-assertions etc which makes your code more difficult to work with and more error prone as it pushes checks from compile-time to runtime</li> </ul>"},{"location":"Golang/LearnGoWithTests/Fundamentals/20.ArraysAndSlicesWithGenerics/","title":"Notes","text":"<ul> <li>can have second type constraint for generic functions</li> <li>higher-order functions like these will make your code simpler to read and maintain</li> </ul>"},{"location":"Javascript/","title":"Javascript","text":"<p>Learning Javascript</p>"},{"location":"Javascript/21-Day-Challenge/","title":"21-day-javascript","text":"<p>Doing javascript for 21 days to learn a new skill and cultivate productive habits</p>"},{"location":"Javascript/Beginners-Javascript/","title":"Learning the basics of Javascript","text":"<p>Following This video</p>"},{"location":"Kubernetes/","title":"Kubernetes Notes","text":"<p>Sources: - https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/</p>"},{"location":"Kubernetes/00-core/","title":"Core","text":""},{"location":"Kubernetes/00-core/#main-k8s-cluster-architecture","title":"Main K8s Cluster Architecture","text":"<ul> <li>Main Node:</li> <li><code>etcd</code>: A distributed key-value store that stores the cluster's configuration data and provides a reliable source of truth.</li> <li><code>kube-scheduler</code>: Assigns Pods to nodes based on resource availability and other constraints.</li> <li><code>kube-controller-manager</code>: runs controller processes that loop to regulate the state of the system to achieve the desired state.</li> <li> <p><code>kube-api server</code>: Exposes the Kubernetes API and acts as the front end for the Kubernetes control plane.</p> </li> <li> <p>Worker Nodes:</p> </li> <li><code>kubelet</code>: An agent that runs on each worker node and ensures that containers are running in a Pod.</li> <li><code>kube-proxy</code>: Maintains network rules on nodes, enabling communication between Pods and external traffic.</li> </ul> <pre><code>graph TD\n  subgraph Main_Node\n    kube_api_server --&gt;|Exposes API| controllers\n    kube_api_server --&gt;|Exposes API| kube_scheduler\n    kube_api_server --&gt;|Exposes API| etcd\n  end\n\n  subgraph Worker_Node_1\n    kube_api_server --&gt;|Exposes API| kubelet1\n    kube_api_server --&gt;|Exposes API| kube_proxy1\n    kubelet1 --&gt;|Manages| Pod1.1\n\n    kube_proxy1 --&gt;|Manages| Pod1.1 \n  end\n\n  subgraph Worker_Node_2\n    kube_api_server --&gt;|Exposes API| kubelet2\n    kube_api_server --&gt;|Exposes API| kube_proxy2\n    kubelet2 --&gt;|Manages| Pod2.1\n    kubelet2 --&gt;|Manages| Pod2.2\n\n    kube_proxy2 --&gt;|Manages| Pod2.1\n    kube_proxy2 --&gt;|Manages| Pod2.2\n  end</code></pre>"},{"location":"Kubernetes/00-core/#main-k8s-resources","title":"Main K8s Resources","text":"<ul> <li>Pods: The smallest deployable units in Kubernetes, consisting of one or more containers and shared resources, enabling the deployment of applications.</li> <li>ReplicaSets: Kubernetes controllers that ensure a specified number of pod replicas are running, providing scalability, fault tolerance, and the ability to manage multiple identical instances.</li> <li>Deployment: A higher-level abstraction managing ReplicaSets, enabling declarative updates, rollbacks, and the definition of desired application states in Kubernetes.</li> </ul> <pre><code>graph TD\n  subgraph Cluster\n    subgraph Node1\n      pod1.1\n      pod1.2\n      pod1.3\n    end\n\n    subgraph Node2\n      pod2.1\n      pod2.2\n      pod3.3\n    end\n\n    subgraph ReplicaSet\n      replicaset1 --&gt;|Creates| Node1\n      replicaset1 --&gt;|Creates| Node2\n    end\n\n    subgraph Deployment\n      deployment --&gt;|Manages| ReplicaSet\n    end\n  end</code></pre> <ul> <li>Services: Abstractions that define logical sets of pods and policies for accessing them, facilitating communication within the cluster or from external sources, with types such as NodePort, ClusterIP, and LoadBalancer.</li> <li>NodePort: Exposes a service on a static port on each cluster node, enabling external access to the service.</li> <li>ClusterIP: Exposes a service on a cluster-internal IP address, allowing communication only within the cluster.</li> <li>LoadBalancer: Exposes a service externally using a cloud provider's load balancer, distributing incoming traffic across multiple pods for scalability and availability.</li> </ul> <pre><code>graph TD\n  NodePort1 --&gt; Node1\n  NodePort2 --&gt; Node2\n  subgraph Cluster\n\n    subgraph ClusterIP_Services\n      db_svc &lt;----&gt; Node1\n      api_svc &lt;----&gt; Node2\n      db_svc &lt;----&gt; api_svc\n    end\n    subgraph Node1\n      dbpod1.1\n      dbpod1.2\n      dbpod1.3\n    end\n    subgraph Node2\n      apipod2.1\n      apipod2.2\n      apipod3.3\n    end\n  end\n  subgraph LoadBalancerService\n    LoadBalancer --&gt; Node1\n    LoadBalancer --&gt; Node2\n  end</code></pre> <ul> <li>Namespaces: Virtual clusters within a Kubernetes cluster, providing a way to partition and isolate resources, enabling multiple teams or projects to share the same cluster without interference.</li> </ul> <pre><code>graph TD\n  subgraph Cluster\n    subgraph Namespace1\n      pod1\n      pod2\n      service1 --&gt;|In Namespace1| pod1\n    end\n\n    subgraph Namespace2\n      pod3\n      pod4\n      service2 --&gt;|In Namespace2| pod3\n    end\n\n    subgraph Namespace3\n      pod5\n      pod6\n      service3 --&gt;|In Namespace3| pod5\n    end\n  end</code></pre>"},{"location":"Kubernetes/01-scheduling/","title":"01 scheduling","text":""},{"location":"Kubernetes/01-scheduling/#concepts","title":"Concepts","text":"Concept Explanation Example Labels Key-value pairs for object categorization in Kubernetes. Labeling a pod as \"app=web\" for later grouping or selection. Selectors Criteria to filter and match objects based on labels. Selecting pods with the label \"environment=production\" using a selector. Taints Properties applied to nodes to influence pod scheduling. Tainting a node as \"NoSchedule\" to prevent certain pod types from being scheduled on it. Tolerations Declarations allowing pods to tolerate specific taints. Specifying toleration in a pod to be scheduled on nodes with specific taints. Node Selectors Specifications in pod definitions constraining scheduling based on node labels. Assigning a pod to nodes with the label \"type=worker\" using node selectors. Node Affinity Rules in pod specifications influencing pod scheduling based on node labels. <code>More Expressive than node selectors</code> Specifying node affinity to schedule pods on nodes with specific labels, like \"zone=us-west\" OR \"zone=ca-central-1\". Resource Requests Initial resource claims by a container upon scheduling. Requesting 0.5 CPU cores and 256 MiB of memory for a pod. Resource Limits Maximum resource usage allowed for a container. Setting limits of 1 CPU core and 512 MiB of memory for a container. Limit Range Constraints on resource requests and limits for pods in a namespace. Specifying that all pods in a namespace must have a minimum CPU request of 0.1 cores using a Limit Range. Resource Quota Limits on aggregate resource usage within a namespace. Setting a Resource Quota to limit the total CPU cores and memory usage in a namespace. DaemonSets Controllers ensuring a copy of a pod runs on all or selected nodes in a cluster. Using a DaemonSet to deploy a log collector on each node. Static Pods Pods with configurations managed directly by the kubelet on a node. Configuring a kubelet to start and manage pods based on static pod manifest files. Multiple Schedulers Kubernetes feature supporting the use of different schedulers for pod placement decisions. Employing a specialized scheduler for certain nodes or pods based on custom requirements. <p>This table provides a concise overview of each concept in Kubernetes, along with an example illustrating its usage.</p>"},{"location":"Kubernetes/01-scheduling/#comparisons","title":"Comparisons","text":"<ul> <li>scheduling:</li> <li>taints and tolerations: deny pods from being scheduled in nodes</li> <li>node selector &amp; node affinity: select nodes a pod can be sceheduled in.</li> </ul>"},{"location":"Kubernetes/02-logging-and-monitoring/","title":"02 logging and monitoring","text":""},{"location":"Kubernetes/02-logging-and-monitoring/#concepts","title":"Concepts","text":"Concept Explanation Metric Server A component in Kubernetes that collects resource utilization data (CPU, memory) and provides it as an API. Pod Logs The recorded output or events produced by a container within a pod, accessible for troubleshooting and monitoring."},{"location":"Kubernetes/03-application-lifecycle-management/","title":"03 application lifecycle management","text":""},{"location":"Kubernetes/03-application-lifecycle-management/#concepts","title":"Concepts","text":"Concept Explanation Rolling Updates A strategy in Kubernetes deployments where a new version of a pod is gradually deployed while old pods are gradually terminated. This ensures continuous availability. Rollbacks Reverting to a previous version of a deployment if issues are detected with the new version, ensuring a quick and reliable rollback to a known-good state. Pod Command The command to be executed when the container starts. Pod Args Command-line arguments passed to the container command . Environment Variables Methods in Kubernetes for configuring environment variables: using the <code>env</code> field in pod specifications, ConfigMaps, or Secrets. ConfigMap A Kubernetes resource for storing configuration data as key-value pairs, separate from pod specifications. Secrets A Kubernetes resource for securely storing sensitive information, such as passwords or API keys, separate from pod specifications. Encoded but not encrypted. Not encrypted at rest in etcd. Multi-Container Pods Pods with multiple containers that share the same network namespace, storage, and can communicate with each other. Init Containers Containers that run and complete before the main containers in a pod, used for setup or preparatory tasks, and share the same network namespace."},{"location":"Kubernetes/04-cluster-maintenance/","title":"04 cluster maintenance","text":""},{"location":"Kubernetes/04-cluster-maintenance/#concepts","title":"Concepts","text":"Concept Explanation Drain Draining a node in Kubernetes involves gracefully evicting pods from the node, preparing it for maintenance or decommissioning. Cordon Cordoning a node in Kubernetes prevents new pods from being scheduled onto the node. Existing pods continue to run. Uncordon Uncordoning a node in Kubernetes allows new pods to be scheduled onto the node after it was previously cordoned. Backup and Restore Backup and restore in Kubernetes can be performed from configuration files or directly from etcd, the key-value store. Export ETCDCTL_API=3 Sets the environment variable to specify the etcdctl API version (version 3 in this case). etcdctl snapshot save etcdctl command to create a snapshot of the etcd key-value store, capturing the current state for backup or migration. etcdctl snapshot restore etcdctl command to restore the etcd key-value store from a previously created snapshot. etcd needs cacert, cert, endpoints, key etcd requires certain parameters for secure communication: CA certificate (--cacert), client certificate (--cert), --endpoints, and client key (--key)."},{"location":"Kubernetes/04-cluster-maintenance/#extra","title":"Extra","text":"<ul> <li>restoring from etcd server</li> <li>create a snapshot</li> <li>restore a snapshot to a folder</li> <li>update etcd service <code>data-dir</code> to use new folder</li> </ul>"},{"location":"Linux-Command-Line/","title":"Linux-Command-Line","text":"<p>Learning the Linux Command Line</p>"},{"location":"Linux-Command-Line/ShellScripting/","title":"Shell Scripting","text":"<p>A shell script is a file containing a series of commands. The shell reads the file and carries out the commands as if they have been entered directly on the command line.</p>"},{"location":"Linux-Command-Line/ShellScripting/#how-to-write-a-shell-script","title":"How to Write a Shell Script","text":"<ol> <li>Write a script. Shell scripts are ordinary files, so they are made with a text editor. Preferably with syntax highlightin.</li> <li>Make the script executable.</li> <li>Put the script somewhere the shell can find i</li> </ol>"},{"location":"Linux-Command-Line/ShellScripting/#script-file-format","title":"Script file format","text":"<p><pre><code>#!/bin/bash\n# This is out first script\n\necho 'Hello World!'\n</code></pre> Comments designated by # symbol can also be at the end of a code line.</p>"},{"location":"Linux-Command-Line/ShellScripting/#executable-permission","title":"Executable Permission","text":"<pre><code>chmod 755 hello_world\n</code></pre>"},{"location":"Linux-Command-Line/ShellScripting/#script-file-location","title":"Script File Location","text":"<p>With the permissions set, we can now execute our script. For the script to run, we must precede the script name with an explicit path. <pre><code>./hello_world\n</code></pre> To use script without stating the path. You will have to add it to a directory that is registered in the PATH variable.</p> <pre><code>export PATH=~/bin:\"$PATH\"\n</code></pre>"},{"location":"Linux-Command-Line/ShellScripting/#good-locations-for-scripts","title":"Good Locations for Scripts","text":"<ul> <li>personal use: ~/bin</li> <li>everyone: ~/usr/local/bin</li> <li>sys admin: ~/usr/local/sbin</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#more-formatting-tricks","title":"More Formatting Tricks","text":"<ul> <li>If a command has both  long and short option, use the short option</li> <li>If command is long, readability can be enhanced by spreading it over several lines and indenting<ul> <li>Using line continuation backslash \\</li> </ul> </li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#variables-and-constants","title":"variables and constants","text":"<p>variables are mutable, constants are immutable <code>variable=value</code> : no spaces in-between name of variable and its value <code>CONSTANT=VALUE</code> : CONSTANTS in al caps, if you want to enforce immutability, must make it read only. <code>declare -r CONSTANT=VALUE</code></p>"},{"location":"Linux-Command-Line/ShellScripting/#parameter-expansion","title":"parameter expansion","text":"<ul> <li>Use <code>parameter expansion</code> if a variable is going be used more than once in a file <pre><code>name=\"yemane\"\necho=\"This is ${name} using path expansion twice with variable ${name}\"\n</code></pre></li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#here-document","title":"Here Document","text":"<p>In the format of ... <pre><code>command &lt;&lt; _EOF_\ntext\n_EOF_\n</code></pre> - single and double quotes within here documents  lose their special meaning to the shell.</p>"},{"location":"Linux-Command-Line/ShellScripting/#shell-functions","title":"Shell Functions","text":"<p>Shell functions are \"mini-scripts\" that are located inside other scripts and can act as autonomous programs. <pre><code># formal form\nfunction name {\n    commands\n    return\n}\n\n# simpler and preferred form\nname () {\n    commands\n    return\n}\n</code></pre> Thr following script demonstrates the use of functions <pre><code>#!bin/bash\n\nfunction step2 {\n    echo \"Step 2\"\n    return 0 # shell functions can return an exit status by including an integer argument to the return command\n}\n\n# Main program starts here\n\necho \"Step 1\"\nstep2\necho \"Step 2\"\n</code></pre></p>"},{"location":"Linux-Command-Line/ShellScripting/#local-variables","title":"Local Variables","text":"<ul> <li>Global variables are available throughout the the programm</li> <li>Local variables are accessible only within the shell function they   are defined in</li> </ul> <pre><code>#!/bin/bash\n\nfoo=0 # global variable foo\n\nfunc () {\n    local foo # variable foo local to func\n    foo=1\n    echo \"funct_1: foo = $foo\"\n}\n\necho \"global: foo = $foo\"\nfunc\n</code></pre>"},{"location":"Linux-Command-Line/ShellScripting/#if-statement","title":"If Statement","text":"<pre><code>#!/bin/bash\nx=5\n\nif [\"$x\" -eq 5]; then # quotes ensure it is always a string, avoiding errors\n    echo \"x equals 5\"\nelse\n    echo \"x does not equal 5\"\nfi\n</code></pre>"},{"location":"Linux-Command-Line/ShellScripting/#test","title":"test","text":"<p>The test command performs a variety of checks and comparisons. It has two equivalent expression. It returns an exit status of 0 if true, and 1 if false. - test expression - [expression]</p> <p>Compound command - [[expression]]   - string1 =~ regex ; adds regex support</p>"},{"location":"Linux-Command-Line/ShellScripting/#designed-for-integers","title":"(( ))\u2014Designed for Integers","text":"<p>(( )) is used to perform arithmetic truth tests.</p>"},{"location":"Linux-Command-Line/ShellScripting/#combining-expressions","title":"Combining Expressions","text":"Operation test [[ ]] and (( )) AND -a &amp;&amp; OR -o <code>||</code> NOT ! ! ### Control Operators: Another Way to Branch bash provides two control operators that can perform branching. The &amp;&amp; (AND) and (OR) operators work like the logical operators in the [[ ]] compound command. <ul> <li>With the &amp;&amp; operator, command1 is executed, and command2 is executed if, and only if, command1 is successful.</li> <li>With the || operator, command1 is executed and command2 is executed if, and only if, command1 is unsuccessful</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#reading-keyboard-input","title":"Reading Keyboard Input","text":"<ul> <li>read assigns fields from standard input to the specified variables. </li> <li><code>read [-options] [variable...]</code></li> </ul> <p><pre><code>#!/bin/bash\n\necho -n \"Enter one or more values &gt; \"\n\nread var1 var2 \n\necho \"var1 = '$var1'\"\necho \"var2 = '$var2'\"\n</code></pre> - If read receives fewer than the expected number, the extra variables are empty <code>''</code> -  an excessive amount of input results in the final variable containing all of the extra input -  - If no variable name is supplied, the shell variable REPLY contains the line of data.</p> Option Description -a array Assign the input to array, starting with index zero. -d delimiter The first character in the string delimiter is used to indicate the end of input, rather than a newline character. -e Use Readline to handle input. This permits input editing in the same manner as the command line. -i string Use string as a default reply if the user simply presses enter. Requires the -e option. -n num Read num characters of input, rather than an entire line. -p prompt Display a prompt for input using the string prompt. -r Raw mode. Do not interpret backslash characters as escapes. -s Silent mode. Do not echo characters to the display as they are typed. This is useful when inputting passwords and other confidential information. -t seconds Timeout. Terminate input after seconds. read returns a non-zero exit status if an input times out. -u fd Use input from file descriptor fd, rather than standard input."},{"location":"Linux-Command-Line/ShellScripting/#ifs","title":"IFS","text":"<ul> <li>shell normally performs word splitting using white spaces </li> <li>behavior is configured by shell variable IFS</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#menus","title":"Menus","text":"<pre><code>#!/bin/bash\n# read-menu: a menu driven system information program\nclear\necho \"\nPlease Select:\n1. Display System Information\n2. Display Disk Space\n3. Display Home Space Utilization\n0. Quit\n\"\nread -p \"Enter selection [0-3] &gt; \"\necho \"You have chosen $REPLY\"\n</code></pre>"},{"location":"Linux-Command-Line/ShellScripting/#while-loop","title":"while loop","text":"<ul> <li><code>while commands; do commands; done</code></li> </ul> <p><pre><code>#!/bin/bash\n# while-count: display a series of numbers\ncount=1\n\nwhile [[ \"$count\" -le 5 ]]; do\n    echo \"$count\"\n    count=$((count + 1))\ndone\necho \"Finished.\"\n</code></pre> bash provides two builtin commands that can be used to control program  flow inside loops - break: immediately terminate the loop, and resume with next statement - continue: skip remainder of loop, and resume with next iteration of the loop</p>"},{"location":"Linux-Command-Line/ShellScripting/#until-loop","title":"until loop","text":"<p><pre><code>#!/bin/bash\n# until-count: display a series of numbers\ncount=1\n\nuntil [[ \"$count\" -gt 5 ]]; do\n    echo \"$count\"\n    count=$((count + 1))\ndone\necho \"Finished.\"\n</code></pre> - An until loop continues until it receives a zero exit status.</p>"},{"location":"Linux-Command-Line/ShellScripting/#case","title":"Case","text":"<p><pre><code>case word in\n    [pattern [| pattern]...) commands ;;]...\nesac\n</code></pre> - patterns used by case are the same as those used by pathname expansion - Patterns are terminated with a ) character</p> Pattern Description a) Matches if word equals a. [[:alpha:]]) Matches if word is a single alphabetic character. ???) Matches if word is exactly three characters long. *.txt) Matches if word ends with the characters .txt. *) Matches any value of word. <ul> <li>can also use | to combine patterns as an <code>or</code> conditional</li> <li>add the <code>;;&amp;</code> notation to terminate each action to enable performing multiple actions</li> </ul> <pre><code>#!/bin/bash\n# case4-2: test a character\nread -n 1 -p \"Type a character &gt; \"\necho\ncase \"$REPLY\" in\n    [[:upper:]]) echo \"'$REPLY' is upper case.\" ;;&amp;\n    [[:lower:]]) echo \"'$REPLY' is lower case.\" ;;&amp;\n    [[:alpha:]]) echo \"'$REPLY' is alphabetic.\" ;;&amp;\n    [[:digit:]]) echo \"'$REPLY' is a digit.\" ;;&amp;\n    [[:graph:]]) echo \"'$REPLY' is a visible character.\" ;;&amp;\n    [[:punct:]]) echo \"'$REPLY' is a punctuation symbol.\" ;;&amp;\n    [[:space:]]) echo \"'$REPLY' is a whitespace character.\" ;;&amp;\n    [[:xdigit:]]) echo \"'$REPLY' is a hexadecimal digit.\" ;;&amp;\nesac\n</code></pre>"},{"location":"Linux-Command-Line/ShellScripting/#for-loops","title":"for loops","text":"<ul> <li> <p>traditional shell form <pre><code>for variable [in words]; do\n commands\ndone\n</code></pre> <pre><code>#!/bin/bash\n\nfor i in 0 1 2 3 4; do\n        echo \"$i\"\ndone\n</code></pre></p> </li> <li> <p>C Language form <pre><code>for (( expression1; expression2; expression3 )); do\n commands\ndone\n</code></pre></p> </li> </ul> <pre><code>#!/bin/bash\n# simple_counter: demo of C style for command\n\nfor (( i=0; i&lt;5; i=i+1 )); do\n    echo $i\ndone\n</code></pre> <ul> <li>expression1 initializes the variable i</li> <li>expression2 sets the condition for loop to run until</li> <li>expression3 increments the value of i by 1 each time the loop repeats</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#positional-parameters","title":"Positional Parameters","text":"<ul> <li>The shell provides a set of variables called positional parameters that contain the individual words on the command line. The variables are named 0 through 9.</li> <li>0 is script name</li> <li>can be accessed like $0 $1 $2 etc.</li> <li>determine number of arguments: <code>$#</code> </li> <li>to access many arguments use <code>shift</code></li> <li>cause each parameter to \"move down one\" each time it is executed</li> </ul> <p><pre><code>#!/bin/bash\n# posit-param2: script to display all arguments\ncount=1\nwhile [[ $# -gt 0 ]]; do\n    echo \"Argument $count = $1\"\n    count=$((count + 1))\n    shift\ndone\n</code></pre> - To handle many positional parameters at once use <code>\"$@\"</code></p>"},{"location":"Linux-Command-Line/ShellScripting/#parameter-expansion_1","title":"Parameter Expansion","text":"<ul> <li>Expansions to Manage Empty Variables</li> <li>expansion with defaults to manage empty variables<ul> <li><code>${parameter:-default}</code></li> </ul> </li> <li>expansion with defaults to manage empty variables and assign default value to parameter if used<ul> <li><code>${parameter:=default}</code></li> </ul> </li> <li>exit with an error if parameter is unset<ul> <li><code>${parameter:?default}</code></li> </ul> </li> <li>expand to default if parameter exists, else return nothing<ul> <li><code>${parameter:+default}</code></li> </ul> </li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#expansions-that-return-variable-names","title":"Expansions That Return Variable Names","text":"<ul> <li>to return the names of variables</li> <li><code>{!prefix*}</code></li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#string-operations","title":"String Operations","text":"<ul> <li>expand into length of string contained in parameter</li> <li><code>${#parameter}</code></li> <li>getting a slice of a string</li> <li><code>${parameter:offset:length}</code></li> <li>remove portion of string matching a pattern</li> <li><code>${parameter#pattern}</code> or <code>${parameter##pattern}</code></li> <li>a search-and-replace operation</li> <li><code>${parameter/pattern/string}</code></li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#case-conversion","title":"Case Conversion","text":"<ul> <li>using <code>declare</code> force a variable to always contain the desired format</li> <li><code>declare -u upper</code></li> <li><code>declare -l lower</code></li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#arithmetic-evaluation-and-expansion","title":"Arithmetic Evaluation and Expansion","text":"<p><code>$((expression))</code></p> Operator Description + Addition - Subtraction * Multiplication / Integer division ** Exponentiation % Modulo (remainder)"},{"location":"Linux-Command-Line/ShellScripting/#assignment","title":"Assignment","text":"Notation Description parameter = value Simple assignment. parameter += value parameter = parameter + value. parameter++ parameter = parameter + 1"},{"location":"Linux-Command-Line/ShellScripting/#logic","title":"Logic","text":"Operator Description &lt;= Less than or equal to. &gt;= Greater than or equal to. &lt; Less than. &gt; Greater than. == Equal to. != Not equal to. &amp;&amp; Logical AND. <code>||</code> Logical OR. expr1?expr2:expr3 If expr1  then expr2; else expr3"},{"location":"Linux-Command-Line/ShellScripting/#arrays","title":"Arrays","text":"<p>Limited to one dimension - single value: <code>name[subscript]=value</code> - multiple values: <code>name=(value1 value2 ...)</code> - subscript <code>@</code> can be used to access every element in an array - number of array elements: <code>o ${#a[@]}</code> - find subscript used in array: <code>${!array[@]}</code> - appending to an array: <code>array+=value</code> - delete an array: <code>unset array</code> - sort an array : <code>($(for i in \"${a[@]}\"; do echo $i; done | sort))</code> - Associative arrays (string indexes): <code>array[\"string\"]=value</code></p>"},{"location":"Linux-Command-Line/ShellScripting/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Syntactic Errors</li> <li>Missing Quotes</li> <li>Missing or Unexpected Tokens</li> <li>Unanticipated Expansions</li> <li>Logical Errors</li> <li>Incorrect conditional expressions</li> <li>\u201cOff by one\u201d errors</li> <li>Unanticipated situations.</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#defensive-programming","title":"Defensive Programming","text":"<ul> <li>Important to verify assumptions when programming, like whether a directory exists before using <code>cd</code></li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#verifying-input","title":"Verifying Input","text":"<ul> <li>A general rule of good programming is that if a program accepts input,  it must be able to deal with anything it receives.</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#testing","title":"Testing","text":"<ul> <li>To perform useful testing, it\u2019s important to develop and apply good test cases.</li> <li>This is done by carefully choosing input data or operating conditions that reflect edge and corner cases.</li> </ul>"},{"location":"Linux-Command-Line/ShellScripting/#debugging","title":"Debugging","text":"<ul> <li>find problem area by commenting out sections</li> <li>use informative echo messages to trace the flow of program</li> <li>examine values during execution using echo</li> </ul>"},{"location":"Python/","title":"Python","text":"<p>Mastering Python</p>"},{"location":"Vault/","title":"VAULT","text":"<p>sources: - https://www.youtube.com/watch?v=ae72pKpXe-s</p>"},{"location":"Vault/#what-is-hashicorp-vault","title":"What is HashiCorp Vault","text":"<ul> <li><code>cloud agnostic</code> secrets management system</li> <li>API Driven</li> <li>safely store and manage sensitive data in hybrid cloud environemts</li> <li>used to generate <code>dynamic short lived</code> credentials or <code>encrypt</code> application data on the fly </li> </ul>"},{"location":"Vault/#use-cases","title":"Use Cases","text":"<ul> <li>Secrets Management<ul> <li>centrally store, access, and distribute secrets</li> <li>ex) KV secrets engine</li> </ul> </li> <li>Encrypting Application Data<ul> <li>keep application data secure with centralized key management</li> <li>ex) Encryption as a Service or Transit secrets engine</li> </ul> </li> <li>Identity-based Access<ul> <li>authenticate and access different clouds, systems, and endpoints using trusted identities</li> <li>ex) dynamic aws creds</li> </ul> </li> </ul>"},{"location":"Vault/#basic-vault-cli-commands","title":"Basic Vault CLI Commands","text":"<ul> <li><code>vault</code> list of vault commands</li> <li><code>vault version</code></li> <li><code>vault read</code> read secrets from vault</li> <li><code>vault write</code> write secrets to vault</li> <li><code>-h, -help, --help</code> get help for any command</li> </ul>"},{"location":"Vault/#vault-server-modes","title":"Vault Server Modes","text":"<ul> <li><code>Dev</code> intended for development<ul> <li>not secure</li> <li>stores everything in memory</li> <li>automatically `unsealed</li> <li>root token can be specified before launching</li> <li>never actually store real secrets in dev mode</li> </ul> </li> <li><code>Prod</code> used in QA and production</li> </ul>"},{"location":"Vault/#common-commands","title":"Common Commands","text":"<ul> <li><code>vault server -dev -dev-root-token-id=root</code> start vault in dev mode with root use named root</li> <li><code>vault status</code> basic meta data about server</li> <li><code>vault login</code> login to vault</li> <li><code>vault kv put secret/foo bar=baz</code> put the kv (bar=baz) in the path (secret/foo)</li> <li><code>vault kv get secret/foo</code> read metadata and the recent version of the secret</li> <li><code>vault kv delete secret/foo</code> dekete a key from vaualt</li> <li><code>vault kv undelete -versions=2 secret/foo</code> undelete a secret</li> </ul>"},{"location":"Vault/#vault-architecture-interals","title":"Vault Architecture - Interals","text":"<ul> <li>Core</li> <li>API</li> <li>Storage Backend</li> <li>Barrier</li> </ul>"},{"location":"Vault/#seal-and-unseal-concept","title":"Seal and Unseal Concept","text":"<ul> <li>vault starts up in a <code>sealed state</code></li> <li>vault can access the physical storage, but can't decrypt it</li> <li>no operations are possible with vault exept to <code>unseal</code> the vault and check the <code>status</code> if the vault</li> <li><code>vault data is</code>encrypted<code>using the</code>encryption key<code>in the</code>keyring`</li> <li>the keyring is encrypted by the <code>master key</code></li> <li>and the <code>master key</code> is encrypted by the <code>unseal key</code></li> <li>shamir is the <code>default</code> mechanism to <code>split</code> the key into shards</li> <li>a certain <code>threshold</code> of shards is required to reconstruct the unseal key</li> <li><code>vault operator unseal</code> command to unseal</li> <li><code>vault operator seal</code> command to seal the vault</li> <li>sealing removes the master key in memory an requires another unseal prcoess to restore it</li> <li>sealing requires a <code>single</code> operator with `root privileges</li> <li> <p>once a vault node is unsealed, it remains unsealed until</p> <ul> <li>it is resealed via the api</li> <li>the server is resetarted</li> <li>vaults storage layer encounters an unrecoverable error</li> </ul> </li> <li> <p><code>Auto unseal</code> reduces the operational complexity of keeping the unseal key secure</p> </li> <li>delegates the responsibility of securing the unseal key from <code>uers</code> to a <code>trusted device or service</code>'</li> <li> <p>at startup vault will connect to the device or service implementing the seal and ask it to <code>decrypt</code> the master key vault read from storage</p> </li> <li> <p>initialization with shamir -&gt; unseal keys</p> </li> <li>initialization with auto unseal -&gt; recovery keys</li> </ul>"},{"location":"Vault/#the-configuration-file","title":"The Configuration File","text":"<p>Running a vault server in <code>prod</code> mode involves multiple steps:</p> <ul> <li>specify the configuration in a configuration file<ul> <li>can be JSON or HCL</li> </ul> </li> <li>start the server</li> <li>initialize the server to get unseal keys and an initial root token</li> <li> <p>unseal the vault server with the unseal keys</p> </li> <li> <p><code>vault server -config=./vault-config.hcl</code> to runa  production server</p> </li> </ul>"},{"location":"Vault/#authentication-overview","title":"Authentication Overview","text":"<ol> <li>authenticate to a specific path</li> <li>Once authenticated, vault returns a <code>token</code> and a <code>policy</code></li> <li>token has a time to live, allow user to extract secrets from vault</li> <li>the policy lets vault know what you allowed to use</li> </ol>"},{"location":"Vault/#types-of-auth-moethds","title":"Types of Auth Moethds","text":"<ul> <li> <p>Methods for Users</p> <ul> <li>userpass</li> <li>github</li> <li>LDAP</li> <li>JWT</li> <li>OIDC</li> <li>Okta</li> </ul> </li> <li> <p>Methods for Applications</p> <ul> <li>AppRoles</li> <li>AWS</li> <li>Azure</li> <li>Google Cloud</li> <li>AliCloud</li> <li>Kubernetes</li> </ul> </li> <li> <p>most vault auth methods need to explicitly enabled</p> </li> <li>this is done with <code>vault auth enable</code> command</li> <li>each auth method has a default path</li> <li>alternate paths can be specified to enable multiple instances</li> <li>custom paths must be specified in CLI commands and API calls</li> </ul>"},{"location":"Vault/#policies-overview","title":"Policies Overview","text":"<p>Vault uses policies to : - govern the behaviour of clients - instrument role based access control by specifying access priviledges with least priviledged principle - authorization to access secrets - deny by default</p> <p>Built-in Poliies:</p> <ul> <li> <p>default policy</p> <ul> <li>can't be removed but can be modified</li> <li>attached to all tokens</li> <li>can be explicitly excluded at token creation time<ul> <li><code>vault token create -no-default-policy</code></li> </ul> </li> <li>allows basic functionality such as letting the token look up data about itself and use its cubbyhole data</li> </ul> </li> <li> <p>root policy</p> <ul> <li>can't be removed nor modified</li> <li>aany user associated with this policy becomes a root user</li> <li>best practice to revoke root tokens in production<ul> <li><code>vault token revoke &lt;token&gt;</code></li> </ul> </li> <li>one is created to allow root user to do the initial configuration, therafter initial root token should be revoked and more strictly controlled usersand authentication should be used</li> </ul> </li> </ul>"},{"location":"Vault/#token-overview","title":"Token Overview","text":"<ul> <li>tokens are the core metod for authentication within vault</li> <li>tokens can be used directly or dynamically generated by the auth methods</li> <li>clients need valid tokens to interact with vault</li> </ul>"},{"location":"Vault/#the-token-store","title":"The Token Store","text":"<ul> <li>same as the <code>token authentication backend</code></li> <li>responsible for creating and storing tokens</li> <li>can't be disabled</li> <li>only auth method with no login capability</li> <li>built-in and automatically available at path <code>/auth/token</code></li> <li>you can create tokens directly and bypass other auth methods</li> </ul>"},{"location":"Vault/#secrets-engine-overview","title":"Secrets Engine Overview","text":"<ul> <li>the component of the vault architecture that manages secrets</li> <li>store, generate, and encrypt data</li> <li>enabled at a path</li> <li>flexible and pluggable</li> </ul> <p>Multiple functions - store and read data - connect to external services and generate dynamic credentials on demand - provide encryption as a service - one time password generation - certificates</p> <p>Life Cycle - <code>enable</code> at a given path,      - by default they are enabled at their \"type\" (aws enables at aws/) - <code>disable</code>     - when a secrets engine is disabled, all of its <code>secrets are revoked</code>     - all the data stored for that engine in the physical storage layer is <code>deleted</code> - <code>move</code>     - move the path for an existing secrets engine     - reokves all secrets, since secret leases are tied to the path they were created at     - configuration data stored for the engine persists through the move - <code>Tune</code>     - tunes global configuration for secrets engine such as TTLs</p> <p>Commands:</p> <ul> <li><code>vault secrets list</code> list all  enabled  egines</li> <li><code>vault secrets enable database</code> enable a new secrets engine</li> </ul>"},{"location":"Vault/#replication-design-with-dr-and-pr","title":"Replication Design with DR and PR","text":"<ul> <li><code>performance replication clusters</code> are active services put in different regions<ul> <li>no dynamic secreets, leases, or tokens get replicated across</li> <li>static kv secrets engine is replicaated</li> </ul> </li> <li><code>disaster revovery replication clusters</code> are warm standbys<ul> <li>receive data from their respective primary data clusters</li> <li>everything gets replicated</li> <li>manually promote if primary fails</li> </ul> </li> </ul>"},{"location":"Vault/#vault-agent-overview","title":"Vault Agent Overview","text":"<ul> <li>a <code>client daemon</code> that uses the same vault binary</li> <li>ussed to easily adopt vault without making code changes in your apps</li> </ul> <p>What is does: - authentication into vault (Auto-Auth) - manage the token renewal process (Auto-Auth) - client side caching of tokens and leased secrets / dynamic secrets (caching) - templating which allows dropping serets into a config file following your desired format</p> <ul> <li><code>vault agent -config=/etc/vault/agent-config.hcl</code></li> </ul>"},{"location":"ddia/","title":"Design Data Intensive Application Notes","text":"<p>Part 1 1. reliability, scalability, and maintainability 2. different data models and query languages 3. storage engines, how databases arrange data on disk 4. data encoding and evolution of schemas</p> <p>Part 2 1. replication 2. partitioning / sharding 3. transactions 4. problems with distributed systems 5. consistency and consensus in a distributed system</p> <p>Part 3 1. batch processing 2. stream processing 3. building reliable, scalable, and maintainable applications</p>"},{"location":"ddia/Part1/","title":"Foundations of Data Systems","text":"<ul> <li>an application has to meet two types of requirements in order to be useful</li> <li><code>functional</code> : what it does</li> <li><code>non functional</code> : general properties like reliability, scalability, maintainability</li> </ul>"},{"location":"ddia/Part1/#1-reliable-scalable-and-maintainable-applications","title":"1. Reliable, Scalable, and Maintainable Applications","text":""},{"location":"ddia/Part1/#reliability","title":"Reliability","text":"<ul> <li>system should work even when faced with faults and prevent failures from ocurring</li> <li><code>fault</code>: one component of system deviates from its spec</li> <li><code>failure</code>: entire system stops providing service</li> </ul> <p><code>hardware faults</code>: - add redundancy to hardware components - use fault-tolerannt software to tolerate loss of entire machines</p> <p><code>software errors</code> - tend to cause more issues since software is shared accross nodes - think about assumptions, testing, isolate processes, allow process to crash and restart,  monitor system in production</p> <p><code>human errors</code> - design systems to minimize opportunites for error   - don't make to limiting as people will try to get around it - decouple place where people make mistakes from where they can cause failures   - sandbox environments - test thoroughly: unit tests, system wide integration tetss, manual tests - make it fast to roll back configuration changes</p>"},{"location":"ddia/Part1/#scalability","title":"Scalability","text":"<ul> <li>systems ability to cope with increased load</li> <li>depends on system architecture. load can be volume of reads, volume of writes, volume of data to store, complexity of data, response times, access patterns</li> </ul> <p><code>describing load</code> - <code>load parameters</code> depends on architecture of system: requests per second, reads and writes to database, number of simulatneous users - focus on average case vs bottleneck cases</p> <p><code>describe performance</code> - when load parameter is increased and system resources are unchanged, how is performance affected - when you increase a load parameter, how much do you need to increase the resources to keep performance unchanged - <code>reponse time</code>: what client sees. inclues service time, network delays, queing delays etc. - <code>latency</code> : time requests is waiting to be handled - use <code>percentiles</code> to understand median performance, and outliers - queing delays account for a large part of the reponse time at high percentiles   - <code>head of line blocking</code>: early slow request delay others behind in queue. which is why it's important to measure response times on the client side   - or when several backend calls are required to serve a single request</p> <p><code>approaches for coping with load</code> - <code>scaling up / verical scaling</code>: moving to a more powerful machine   - common wisodom to scale databases up for simplicity - <code>scaling out / horizontal scaling</code>: distubuting the load across multiple smaller machines, also known as shared-nothing   - <code>elastic</code>: can automatically add computing resources  when load increases.</p>"},{"location":"ddia/Part1/#maintainability","title":"Maintainability","text":"<ul> <li> <p>it should be reasonable to expect different people can work on the system productively </p> </li> <li> <p>majority of cost of software is ongoing maintenence</p> </li> <li><code>operability</code>: make it easy to keep the system running smoothly</li> <li>good visibility into systems health</li> <li><code>simplicity</code>: make it easy for new engineers to understand</li> <li>complexity increases risk of bugs by hiding assumptions, unintedned consequences, and unexpected interactions</li> <li><code>abstractions</code>: can hide implementation detail</li> <li><code>evolvability</code>: make it easy to make changes to the system in the future</li> </ul>"},{"location":"ddia/Part1/#2-data-models-and-query-languages","title":"2. Data Models and Query Languages","text":"<ul> <li>application are built by layering data models</li> <li>objects/data structures &gt; JSON/XML &gt; bytes in memory/disk/network &gt; electrical currents</li> <li>data models embody assumptions about how it is going to be used</li> <li>some usage are easy and some are not supported; some operations are fast and some perform badly</li> <li>One model can be emulated in terms of another model, but the result is often awkward.</li> <li>different systems for different purposes, not a single one-size-fits-all solution.</li> <li>schemas can either be explicit (enforced on write) or implicit (handled on read)</li> </ul>"},{"location":"ddia/Part1/#relational-model-versus-document-model","title":"Relational Model Versus Document Model","text":"<p><code>Models</code> - <code>hierarchal model</code>   - represented all data as a tree of records nested within records   - not good for representing many-to-many relationships - <code>network model</code>   - record could have multiple parents     - ex: one record for a location which is linked to many users, allowing many to one and many to many   - many access paths, became cumbersome, updating code for querying and updating the database complicated and inflexible - <code>relational model</code>   - all data in a relation (table) made of up of rows   - no complicated access paths, query optimizer decides optimal path - <code>document model</code>   - similar storage as hierarchal model, with data in JSON   - greater scalability than relational, very large datasets or very high write throughput</p> <p><code>Comparison</code></p> Relational Document joins better support for joins, many to one, and many to many relationships joins not natively supported, work of joins shifted to application code, use document reference (unique identifier) to point to other documents and resolve using follow up queries schema schema enforced at write time, changes require migrations schema-on-read approach (don't enforce any schema), more flexible, can immediately write new document structures and have application code handle variants data locality if getting document like data, requires access multiple different tables can enjoy better data locality if application needs most of document since entire document will be loaded, since updates can require rewriting of entire document it is recommended to keep documents small Object-Relational Mismatch require translation layer between objects in application code and the database model JSON model in document databases is usually similar to objects in application code, resulting in easier translation <p>Extra - Many-to-One and Many-to-Many Relationships   - if a certain data is used by many different entities, it can make sense to store it as an ID     - enforce consistency across entities     - avoid ambiguity by making distinctions     - localization support (for translation)     - better search     - ease of updating, as changes only has to happen in one place       - less write overhead and inconsistencies       - removing duplication helps normalization in databases</p> <ul> <li>convergence of document and relational databases</li> <li>most relational databases support xml and json</li> <li>some document databases support relational like joins</li> </ul>"},{"location":"ddia/Part1/#query-languages-for-data","title":"Query Languages for Data","text":"<p><code>imperative vs declarative</code>   - <code>imperative</code>: tell computer to perform certain operations in a specific order   - <code>declarative</code>: specify end goal but not how to get there     - concise, easier to  use, hides implementation details, more room for optimization, lends itself to parallel execution which is hard for imperative     - <code>SQL is declarative</code> </p> <ul> <li><code>map reduce querying</code></li> <li>programming model for processing large amounts of data in bulk across many machines</li> <li><code>in between declarative and imperative</code></li> <li>logic of the query is expressed with snippets of code, which are called repeatedly by the processing framework</li> </ul>"},{"location":"ddia/Part1/#graph-like-data-models","title":"Graph-Like Data Models","text":"<ul> <li>graph made of two objects: <code>vertices/nodes and edges</code></li> </ul> <p><code>Graph models</code> - <code>property graphs</code>   - each vertex consists of:     - a unique identifier, set of outgoing edges, set of incoming edges, collection of properties (key-value pairs)   - each edge consists of:     - a unique identifier, vertex where it starts (tail), vertex where it ends (head), label describing the relationship between the two vertices, collection of properties (key-value pairs)</p> <ul> <li><code>triple-stores</code></li> <li>mostly similar to property graph model</li> <li>all information is stored in three part statement: <code>subject, predicate, object</code></li> <li>subject is a vertex</li> <li> <p>object can be either</p> <ul> <li>a value in a primitive datatype such as string or number</li> <li>in which case the predicate and object are the key value of property on the subject vertex</li> <li>another vertex in the graph</li> <li>in which case the predicate is and edge, the subject is the tail, and the object is the head</li> </ul> </li> <li> <p><code>graph store</code>:</p> </li> <li>represent property graph using a relational schema </li> <li>two relational tables, one for vertices and one for edges</li> <li>any vertex can have an edge connecting it with any other vertex (no schema restriction)</li> <li>given any vertex you can efficiently find both its incoming and outgoing edges, and traverse the graph</li> <li>can store different kinds of information using labels while still keeping a clean data model</li> </ul> <p><code>Querying</code> - <code>cypher query language</code>   - declarative query language for property graphs for Neo4j graph database - <code>graph queries in sql</code>   - can be done using the WITH RECURSIVE syntax to follow paths, but is difficult - <code>sparql query language</code>   - query language for triple-stores using the rdf data model - <code>datalog</code>   - query language of datomic   - data model similar to triple store, writes a <code>predicate(subject, object)</code>   - define rules that it uses to find data, less convenient for simple one off queries by copes better with complex data</p> <p><code>Comparison</code></p> Network Model Graph Database schema had schema specifying which record type could be nested within other record type no restrictions access traverse an access path to reach a record refer directly to any vertex by unique id or use an index to find vertices with a particular value order data is ordered and database maintained ordering no order querying imperative support declarative"},{"location":"ddia/Part1/#3-storage-and-retrieval","title":"3. Storage and Retrieval","text":"<ul> <li>database should be able to do two things:</li> <li>store data</li> <li>give back data at a later time</li> <li>application developer should know how databases handle storage and retrieval of data</li> <li>to make decision on what storage engine to use in your project</li> </ul>"},{"location":"ddia/Part1/#data-structures-that-power-your-database","title":"Data Structures That Power Your Database","text":"<ul> <li><code>index</code></li> <li>an addition structure containing metadata about primary data to speed up reads</li> <li>slows down writes, because the index will have to be updated every time data is written</li> <li>application developer creates index depending on typical queries that are expected</li> </ul>"},{"location":"ddia/Part1/#log-structured","title":"Log Structured","text":"<ul> <li>designed around the concept of many append-only log segment files</li> <li>new data and modifications are appended to a log file sequentially</li> <li>periodically <code>compact</code> and <code>merge</code>: to remove old duplicates of keys</li> </ul> <p><code>hash indexes</code>   - in memory hash map where every key is mapped to the location where the value can be found   - <code>crash recovery</code>: if database is restarted in memory hash maps are lost, store snapshot of each segments hash map on disk which can be loaded into memory more quickly - <code>SSTables</code>   - same log structure as hash index   - <code>sorted string table sstable</code> : sort sequence of key-value pairs by key   - range queries are more efficient since data is sorted   - <code>merging</code> is more efficient : look at files side by side and copy lowest key   - <code>save memory</code>: don't need to keep index of all keys in memory can use a <code>sparse index</code>   - <code>save disk space and i/o bandwidth</code> : compress blocks that use same index key</p> <p><code>LSM-Trees</code>   - log structured merge-tree   - cascade of SSTables that are merged in the background   - writes go to sorted in memory structure which is eventually written to disk</p> <ul> <li><code>bloom filters</code> : tell you if keys appears in database, to avoid wasting time looking for non existent keys</li> </ul>"},{"location":"ddia/Part1/#page-oriented","title":"Page-Oriented","text":"<ul> <li>B-Trees are the most widely used index, standard index implementation in relational databases</li> <li>break database down into fix sized blocks/pages</li> <li>each page can be identified using and address/location</li> <li>one page is designated as the root of the b-tree</li> <li>page contains several keys and references to child pages</li> <li>each child is responsible for a continuous range of keys and references to their children nodes</li> <li>update: search for the leaf page containing the key, and change the value, write to disk</li> <li>adding: find page whose range encompasses the new key and add it to that page</li> <li> <p>if there isn't enough space in page, then page is split into two half-full pages and parent is updated to account for new subdivision of key ranges</p> </li> <li> <p><code>write-ahead log (WAL/redo log):</code> append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree</p> </li> <li>used after crash to restore database to consistent state</li> </ul>"},{"location":"ddia/Part1/#comparison","title":"Comparison","text":"<ul> <li><code>Log-Structured Storage Engines</code></li> <li>LSM-trees typically have <code>higher write throughput</code></li> <li>smaller files by merging produce smaller files, more read and write requests within the available I/O</li> <li>compaction process can interfere with ongoing reads and writes and consumes bandwidth</li> <li>growing number of unmerged segments can slow down reads</li> <li><code>Page-Oriented Storage Engines</code></li> <li>B-Trees typically have faster reads</li> <li><code>Read Performance</code>: B-Tree structure reduces the number of disk accesses needed to locate data, faster reads</li> <li><code>Predictable Latency</code>: Random read times are relatively stable</li> <li>fragmentation when page is split or when a row cannot fit into an existing page</li> </ul>"},{"location":"ddia/Part1/#other-index-structures","title":"Other Index Structures","text":"<p><code>Storing values within the index</code> - key in an index is the thing that queries search for, but the value can be one of two things - <code>nonclustered index</code>   - storing only references to the data within the index   - avoids duplicating data when multiple secondary indexes are present: each index references a location in the heap file and the actual data is kept in one place - <code>clustered index</code>   - store all row data within the index   - hop from the index to the heap file is too much of a performance penalty - <code>covering index or index with included columns</code>   - stores some of a table\u2019s columns within the index   - compromise between a clustered index and a nonclustered index - clustered and covering indexes can speed up reads, but they require additional storage and can add overhead on writes</p> <p><code>Full-text search and fuzzy indexes</code> - search for one word expands to include synonyms, ignore grammatical variations, and to search for occurrences of words near each other in the same document - <code>Lucene</code> is able to search text for words within a certain edit distance</p> <p><code>Keeping everything in memory</code> - <code>inmemory</code> databases - performance boost comes from avoiding the overhead of encoding in-memory data structures in a form that can be written to disk  - providing data models that are difficult to implement with disk-based indexes: priority queues and sets - some intended for caching use only, where it's acceptable for data to be lost if a machine is restarted - for persistence: write periodic snapshots to disk, or  replicate the in-memory state to other machines</p>"},{"location":"ddia/Part1/#transaction-processing-or-analytics","title":"Transaction Processing or Analytics","text":"<ul> <li>Online Transaction Processing (OLTP)</li> <li>access pattern: look up, insert, update, small number of records based on users input</li> <li>interactive</li> <li>Online Analytic Processing (OLAP)</li> <li>access pattern: scan over a huge number of records, usually a few columns, calculate aggregate statistics</li> </ul> Property Online Transaction Processing (OLTP) Online Analytic Processing (OLAP) Read Pattern small number of records per query, fetched by key Aggregate over large number of records Write Pattern random-access, low latency writes from user input Bulk import (ELT) or event stream Used By end user/customer, via web application Internal analyst, for decision support Data Represents latest state of data History of event over time Dataset Size Gigabytes to Terabytes Terabytes to Petabytes Bottleneck Disk seek time Disk bandwidth"},{"location":"ddia/Part1/#data-warehousing","title":"Data Warehousing","text":"<ul> <li>Data Warehouse</li> <li>database separate from the ones used for OLTP systems</li> <li>analyst can query without affecting OLTP operations</li> <li>contains read-only copy of data in all the various OLTP systems of company</li> <li>data extracted from OLTP databases, transformed into analysis friendly schema, and then loaded into data warehouse (ELT)</li> <li>data warehouse can be optimized for analytic access patterns</li> <li>data model usually relational as SQL is good for analytics</li> </ul>"},{"location":"ddia/Part1/#schemas-for-analytics","title":"Schemas For Analytics","text":"<ul> <li>star schema (dimensional modeling)</li> <li>fact table: each row represents an event that happened at some point in time, created by referencing a bunch of dimensional tables that give data about event</li> <li>simple</li> <li>snowflake schema</li> <li>dimensions are further broken down into sub-dimensions</li> <li>more normalized</li> </ul>"},{"location":"ddia/Part1/#column-oriented-storage","title":"Column-Oriented Storage","text":"<ul> <li>row oriented : store all values from one row together</li> <li>loads all attributes for each row before filtering</li> <li>column oriented: store all values from each column together</li> <li>queries only loads required columns to save time</li> </ul>"},{"location":"ddia/Part1/#column-compression","title":"Column Compression","text":"<ul> <li>column oriented storage lends itself well to compression, which reduces disk throughput demand by compressing data</li> <li>number of distinct values in a column are small compared to number of rows</li> <li>can use bitmap encoding</li> </ul> <p><code>Memory bandwidth and vectorized processing</code> - column compression allows more rows from a column to fit in the same amount of L1 cache - vectorized processing: operate on such chunks of compressed column data directly</p>"},{"location":"ddia/Part1/#sort-order-in-column-storage","title":"Sort Order In Column Storage","text":"<ul> <li>can impose an order for how the rows are stored</li> <li>data sorted entire row at a time, the key chosen based on common query</li> <li>a second column can be used to sort rows that have same value in first sort column</li> <li>helps with compression since sorting creates long sequences of repeating number if there isn't many distinct values</li> </ul> <p><code>Several different sort orders</code> - redundantly store data sorted in different ways - different queries benefit from different sort orders</p>"},{"location":"ddia/Part1/#writing-to-column-oriented-storage","title":"Writing To Column-Oriented Storage","text":"<ul> <li>read optimizations make writes a little more difficult</li> <li>like LSM-Tree, writes go to in memory sorted structure before being written to disk</li> <li>queries will have to examine both column data on disk and recent writes in memory</li> </ul>"},{"location":"ddia/Part1/#aggregation-data-cubes-and-materialized-views","title":"Aggregation: Data Cubes and Materialized Views","text":"<ul> <li>data warehouse queries often involve an aggregate function, such as COUNT, SUM, AVG, MIN, or MAX in SQL</li> <li>materialized view: copy of the query results, written to disk</li> <li>need to be updated when underlying data changes</li> <li>updates make writes more expensive, make more sense in read heavy warehouse</li> <li>virtual view: shortcut for writing queries</li> <li>SQL engine expands it into the view\u2019s underlying query on the fly and then processes the expanded query</li> <li>data cube or OLAP cube: common special case of a materialized view</li> <li>grid of aggregates grouped by different dimensions (example: SUM sales for each combination of date and product)</li> <li>certain queries become very fast because they have effectively been precomputed</li> <li>not as flexible as querying raw data: used to improve specific queries</li> </ul>"},{"location":"ddia/Part1/#summary","title":"Summary","text":"<ul> <li>OLTP: user facing, small queries, large volume of requests,  disk seek time is bottleneck</li> <li>OLAP: analyst facing, large queries, lower volume of requests, disk bandwidth is bottleneck</li> <li>large number of rows, indexes are much less relevant.</li> <li>becomes important to encode data very compactly, to minimize the amount of data that the query needs to read from disk</li> <li>log-structure: append to files and delete obsolete files but never update in place</li> <li>systematically turn random-access writes into sequential writes on disk to enable higher write throughput</li> <li>Bitcask, SSTables, LSM-trees, LevelDB, Cassandra, HBase, Lucene</li> <li>page-oriented: treats the disk as a set of fixed-size pages that can be overwritten</li> <li>BTrees, RDBMS</li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#_1","title":"","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#_2","title":",","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#d","title":"d","text":"<ul> <li>Post 1</li> <li>Post 1</li> <li>Post 2</li> <li>Post 2</li> </ul>"},{"location":"tags/#g","title":"g","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#o","title":"o","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"},{"location":"tags/#t","title":"t","text":"<ul> <li>Post 1</li> <li>Post 2</li> </ul>"}]}