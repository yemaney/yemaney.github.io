{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Versus Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Essentials\n",
    "\n",
    "The first thing to know is that there are two basic built-in types for binary sequences: the `im‐mutable bytes type` introduced in Python 3 and the `mutable bytearray`, added inPython 2.6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'cafe'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'cafe'\n",
    "b = s.encode(encoding='utf8')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'caf\\xc3\\xa9'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe = bytes('café', encoding='utf_8')\n",
    "cafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'c'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'caf\\xc3\\xa9')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe_arr = bytearray(cafe)\n",
    "cafe_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'\\xa9')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe_arr[-1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. bytes can be built from a str, given an encoding.\n",
    "2. Each item is an integer in range(256).\n",
    "3. Slices of bytes are also bytes—even slices of a single byte.\n",
    "4. There is no literal syntax for bytearray: they are shown as bytearray() with a\n",
    "bytes literal as argument.\n",
    "5. A slice of bytearray is also a bytearray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both bytes and bytearray support every str method except those that do formatting (format, format_map) and a few others that depend on Unicode data, including case fold, isdecimal, isidentifier, isnumeric, isprintable, and encode\n",
    "\n",
    "Binary sequences have a class method that str doesn’t have, called fromhex, which builds a binary sequence by parsing pairs of hex digits optionally separated by spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1K\\xce\\xa9'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes.fromhex('31 4B CE A9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xfe\\xff\\xff\\xff\\x00\\x00\\x01\\x00\\x02\\x00'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing bytes from the raw data of an arra\n",
    "import array\n",
    "numbers = array.array('h', [-2, -1, 0, 1, 2])\n",
    "octets = bytes(numbers)\n",
    "octets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Typecode 'h' creates an array of short integers (16 bits).\n",
    "2. octets holds a copy of the bytes that make up numbers.\n",
    "3. These are the 10 bytes that represent the five short integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bytes or bytearray object from any buffer-like source will always copy the bytes. In contrast, memoryview objects let you share memory between binary data structures. To extract structured information from binary sequences, the struct module is invaluable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structs and Memory Views\n",
    "\n",
    "The struct module provides functions to parse packed bytes into a tuple of fields of different types and to perform the oposite conversion, from a tuple into packed bytes. struct is used with bytes, bytesarray, and memoryview objects.\n",
    "\n",
    "the memoryview class does not let you create or store byte sequences, but provides shared memory access to slices of data from other binary sequences, packed arrays, and buffers such as Python Imaging Library\n",
    "(PIL) images without copying the bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . Using memoryview and struct to inspect a GIF image heade\n",
    "import struct\n",
    "fmt = '<3s3sHH'\n",
    "with open('filter', 'rb') as fp:\n",
    "    img = memoryview(fp.read())\n",
    "header = img[:10]\n",
    "bytes(header)\n",
    "struct.unpack(fmt, header)\n",
    "del header\n",
    "del img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. struct format: < little-edian; 3s3s two sequences of 3bytes; HH two 16-bit integers\n",
    "2. Create memory view from file contents in memory...\n",
    "3. ... then another memoryview by slicing the first one; no bytes are copied here\n",
    "4. Convert to bytes for display only; 10 bytes are copied here\n",
    "5. Unpack memoryview into typle of; type, version, width and height\n",
    "6. Delete references to release the memory associated with the memory view instances\n",
    "\n",
    "\n",
    "even less byte copying would happen if I used the mmap module to open the image as a memory-mapped file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Encoders / Decoders\n",
    "\n",
    "The python distribution bundles more than 100 codec (encoder/decoder) for text to byte conversion and vice versa. Each codec has a name like `utf_8` and often aliases such as `utf8`, `utf-8`, which you can us as the encoding arguement in functions like open(), str.encode(), bytes.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin_1\tb'El Nino'\n",
      "utf_8\tb'El Nino'\n",
      "utf_16\tb'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00n\\x00o\\x00'\n"
     ]
    }
   ],
   "source": [
    "for codec in ['latin_1', 'utf_8', 'utf_16']:\n",
    "    print(codec, 'El Nino'.encode(encoding=codec), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Encode/Decode Problems\n",
    "\n",
    "Although there is a generic UnicodeError exception, the error reported is almost always  more specific; either a UnicodeEncodError (when converting str to binary sequences) or a UnicodeDecodeError (when reading binary sequences into str). Loading python modules may also generate a SyntaxError when the source encoding is unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coping with UnicodeEncodeError\n",
    "\n",
    "Most non-UTF codecs handle only a small subset of the Unicode characters. When converting text to bytes, if a character is not defined in the target encoding, UnicodeEncodeError will be raised, unless special handling is provided by passing an error argument to the encoding method or funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'S\\xc3\\xa3o Paulo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'São Paulo'\n",
    "city.encode('utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-064a572fd5b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cp437'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp437.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, errors)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "city.encode('cp437')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'So Paulo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.encode('cp437', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'S?o Paulo'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.encode('cp437', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'S&#227;o Paulo'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.encode('cp437', errors='xmlcharrefreplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The 'utf_?' encodings handle any str.\n",
    "2. 'cp437' can’t encode the 'ã' (“a” with tilde). The default error handler\n",
    "—'strict'—raises UnicodeEncodeError.\n",
    "3. The error='ignore' handler silently skips characters that cannot be encoded;\n",
    "this is usually a very bad idea.\n",
    "4. When encoding, error='replace' substitutes unencodable characterswith '?';\n",
    "data is lost, but users will know something is amiss.\n",
    "5. 'xmlcharrefreplace' replaces unencodable characters with an XML entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coping with UnicodeDecodeError\n",
    "\n",
    "Not every bytes holds a valid ASCII character, and not every byte sequence is valid UTF-8, therfore when you assume one of these encodings while converting a binary  sequence to text, you will get a UnicodeDecodeError if unexpected bytes are found.\n",
    "\n",
    "On the other hand, many legacy 8-bit encodings like 'cp1252' are able to decode any stream of bytes, including random noise, without generating errors. Therefore, if your program assumes the wrong 8-bit encoding, it will silently decode garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Montréal'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octets = b'Montr\\xe9al'\n",
    "octets.decode('cp1252') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Montrιal'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octets.decode('iso8859_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-afaa3d3916c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moctets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf_8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "octets.decode('utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Montr�al'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octets.decode('utf_8', errors='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. These bytes are the characters for “Montréal” encoded as latin1; '\\xe9' is the byte for “é”\n",
    "2. Decoding with 'cp1252' (Windows 1252) works because it is a proper superset of latin1.\n",
    "3. ISO-8859-7 is intended for Greek, so the '\\xe9' byte is misinterpreted, and no error is issued.\n",
    "4. The 'utf_8' codec detects that octets is not valid UTF-8, and raises Unicode DecodeError.\n",
    "5. Using 'replace' error handling, the \\xe9 is replaced by “�” (code point U+FFFD), the official Unicode REPLACEMENT CHARACTER intended to represent unknown characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SyntaxError When Loading Modules with Unexpected Encoding\n",
    "\n",
    "UTF-8 is the default source encoding for Python 3, just as ASCII was the default for Python 2 (starting with 2.5). If you load a .py module containing non-UTF-8 data and no encoding declaration, you get a message like this:\n",
    "\n",
    "    SyntaxError: Non-UTF-8 code starting with '\\xe1' in file ola.py on line\n",
    "    1, but no encoding declared; see http://python.org/dev/peps/pep-0263/\n",
    "    for details\n",
    "\n",
    "Because UTF-8 is widely deployed in GNU/Linux and OSX systems, a likely scenario is opening a .py file created on Windows with cp1252. Note that this error happens even in Python for Windows, because the default encoding for Python 3 is UTF-8 across all platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Discover the Encoding of a Byte Sequence\n",
    "\n",
    "How do you find the encoding of a byte sequence? Short answer: you can’t. You must be told.\n",
    "\n",
    "`Use python package Chardet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOM: A Useful Gremlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00\\xf1\\x00o\\x00'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u16 = 'El Niño'.encode('utf_16')\n",
    "u16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bytes are b'\\xff\\xfe'. That is a BOM—byte-order mark—denoting the “little\u0002endian” byte ordering of the Intel CPU where the encoding was performed. On a little-endian machine, for each code point the least significant byte comes first:the letter 'E', code point U+0045 (decimal 69), is encoded in byte offsets 2 and 3 as 69 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(u16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a big-endian CPU, the encoding would be reversed; 'E' would be encoded as 0 and 69. To avoid confusion, the UTF-16 encoding prepends the text to be encoded with the special character ZERO WIDTH NO-BREAK SPACE (U+FEFF), which is invisible. On a little\u0002endian system, that is encoded as b'\\xff\\xfe' (decimal 255, 254). Because, by design, there is no U+FFFE character, the byte sequence b'\\xff\\xfe' must mean the ZERO\n",
    "WIDTH NO-BREAK SPACE on a little-endian encoding, so the codec knows which byt ordering to use. There is a variant of UTF-16—UTF-16LE—that is explicitly little-endian, and another one explicitly big-endian, UTF-16BE. If you use them, a BOM is not generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text Files\n",
    "\n",
    "The best practice for handling text is the “Unicode sandwich” (Figure 4-2).4 This means that bytes should be decoded to str as early as possible on input (e.g., when opening a file for reading). The “meat” of the sandwich is the business logic of your program, where text handling is done exclusively on str objects. You should never be encoding or decoding in the middle of other processing. On output, the str are encoded to bytes as late as possible. Most web frameworks work like that, and we rarely touch byteswhen using them.\n",
    "\n",
    "\n",
    "Also make sure to explicitly specify the encoding standards for a file when reading and writing to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Defaults: A Madhouse\n",
    "\n",
    "Several settings affect the encoding defaults for I/O in Python. See the default_encodings.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " locale.getpreferredencoding() -> 'cp1252'\n",
      "                 type(my_file) -> <class '_io.TextIOWrapper'>\n",
      "              my_file.encoding -> 'cp1252'\n",
      "           sys.stdout.isatty() -> False\n",
      "           sys.stdout.encoding -> 'UTF-8'\n",
      "            sys.stdin.isatty() -> False\n",
      "            sys.stdin.encoding -> 'utf-8'\n",
      "           sys.stderr.isatty() -> False\n",
      "           sys.stderr.encoding -> 'UTF-8'\n",
      "      sys.getdefaultencoding() -> 'utf-8'\n",
      "   sys.getfilesystemencoding() -> 'utf-8'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import locale\n",
    "\n",
    "expressions = \"\"\"\n",
    " locale.getpreferredencoding()\n",
    " type(my_file)\n",
    " my_file.encoding\n",
    " sys.stdout.isatty()\n",
    " sys.stdout.encoding\n",
    " sys.stdin.isatty()\n",
    " sys.stdin.encoding\n",
    " sys.stderr.isatty()\n",
    " sys.stderr.encoding\n",
    " sys.getdefaultencoding()\n",
    " sys.getfilesystemencoding()\n",
    " \"\"\"\n",
    "my_file = open('dummy', 'w')\n",
    "\n",
    "for expression in expressions.split():\n",
    " value = eval(expression)\n",
    " print(expression.rjust(30), '->', repr(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you omit the encoding argument when opening a file, the default is given by\n",
    "locale.getpreferredencoding() ('cp1252' in Example 4-12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Unicode for Saner Comparisons\n",
    "\n",
    "String comparisons are complicated by the fact that Unicode has combining characters: diacritics and other marks that attach to the preceding character, appearing as one when printed.\n",
    "For example, the word “café” may be composed in two ways, using four or five code points, but the result looks exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('café', 'café')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'café'\n",
    "s2 = 'cafe\\u0301'\n",
    "s1, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code point U+0301 is the COMBINING ACUTE ACCENT. Using it after “e” renders “é”. In the Unicode standard, sequences like 'é' and 'e\\u0301' are called `“canonical equivalents,”` and applications are supposed to treat them as the same. \n",
    "But Python sees two different sequences of code points, and considers them not equal. The solution is to use Unicode normalization, provided by the `unicodedata.normalize `function. The first argument to that function is one of four strings: `'NFC', 'NFD','NFKC', and 'NFKD'.` Let’s start with the first two.\n",
    "\n",
    "Normalization Form C (NFC) composes the code points to produce the shortest equivalent string, while NFD decomposes, expanding composed characters into base characters and separate combining characters. Both of these normalizations make comparisons work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "len(s1), len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalize('NFC', s1)), len(normalize('NFC', s2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalize('NFD', s1)), len(normalize('NFD', s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize('NFD', s1) == normalize('NFD', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Folding\n",
    "\n",
    "Case folding is essentially converting all text to lowercase, with some additional transformations. It is supported by the `str.casefold`method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilirt Functions for Normalized Text Matching\n",
    "\n",
    "As we’ve seen, NFC and NFD are safe to use and allow sensible comparisons between Unicode strings. NFC is the best normalized form for most applications. str.casefold() is the way to go for case-insensitive comparisons\n",
    "\n",
    "If you work with text in many languages, a pair of functions like `nfc_equal` and `fold_equal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "\n",
    "def nfc_equal(str1, str2):\n",
    "    return normalize('NFC', str1) == normalize('NFC', str2)\n",
    "\n",
    "\n",
    "def fold_equal(str1, str2):\n",
    "    return (normalize('NFC', str1).casefold() ==\n",
    "            normalize('NFC', str2).casefold())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme “Normalization”: Taking Out Diacritics\n",
    "\n",
    "Removing diacritics is not a proper form of normalization because it often changes the meaning of words and may produce false positives when searching. But it helps coping with some facts of life: people sometimes are lazy or ignorant about the correct use of diacritics, and spelling rules change over time, meaning that accents come and go in living lan‐\n",
    "guages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\"Remove all diacritic marks\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt\n",
    "                     if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caffe'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shave_marks('caffè')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Unicode Text\n",
    "\n",
    "Python sorts sequences of any type by comparing the items in each sequence one by one. For strings, this means comparing the code points. Unfortunately, this produces unacceptable results for anyone who uses non-ASCII characters.\n",
    "\n",
    "\n",
    "The standard way to sort non-ASCII text in Python is to use the locale.strxfrm function which, according to the locale module docs, “transforms a string to one that can be used in locale-aware comparisons.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acerola', 'atemoia', 'açaí', 'caju', 'cajá']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "sorted(fruits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.UTF-8'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['açaí', 'acerola', 'atemoia', 'cajá', 'caju']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "sorted_fruits = sorted(fruits, key=locale.strxfrm)\n",
    "sorted_fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting with the Unicode Collation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    ">>> import pyuca\n",
    ">>> coll = pyuca.Collator()\n",
    ">>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    ">>> sorted_fruits = sorted(fruits, key=coll.sort_key)\n",
    ">>> sorted_fruits\n",
    "['açaí', 'acerola', 'atemoia', 'cajá', 'caju']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Unicode Database\n",
    "\n",
    "The Unicode standard provides an entire database—in the form of numerous structured\n",
    "text files—that includes not only the table mapping code points to character names, but\n",
    "also metadata about the individual characters and how they are related. For example,\n",
    "the Unicode database records whether a character is printable, is a letter, is a decimal\n",
    "digit, or is some other numeric symbol. That’s how the str methods isidentifier,\n",
    "isprintable, isdecimal, and isnumeric work. str.casefold also uses information\n",
    "from a Unicode table.\n",
    "The unicodedata module has functions that return character metadata; for instance,\n",
    "its official name in the standard, whether it is a combining character (e.g., diacritic like\n",
    "a combining tilde), and the numeric value of the symbol for humans (not its code point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+0031\t  1   \tre_dig\tisdig\tisnum\t 1.00\tDIGIT ONE\n",
      "U+00bc\t  ¼   \t-\t-\tisnum\t 0.25\tVULGAR FRACTION ONE QUARTER\n",
      "U+00b2\t  ²   \t-\tisdig\tisnum\t 2.00\tSUPERSCRIPT TWO\n",
      "U+0969\t  ३   \tre_dig\tisdig\tisnum\t 3.00\tDEVANAGARI DIGIT THREE\n",
      "U+136b\t  ፫   \t-\tisdig\tisnum\t 3.00\tETHIOPIC DIGIT THREE\n",
      "U+216b\t  Ⅻ   \t-\t-\tisnum\t12.00\tROMAN NUMERAL TWELVE\n",
      "U+2466\t  ⑦   \t-\tisdig\tisnum\t 7.00\tCIRCLED DIGIT SEVEN\n",
      "U+2480\t  ⒀   \t-\t-\tisnum\t13.00\tPARENTHESIZED NUMBER THIRTEEN\n",
      "U+3285\t  ㊅   \t-\t-\tisnum\t 6.00\tCIRCLED IDEOGRAPH SIX\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "re_digit = re.compile(r'\\d')\n",
    "sample = '1\\xbc\\xb2\\u0969\\u136b\\u216b\\u2466\\u2480\\u3285'\n",
    "for char in sample:\n",
    "    print('U+%04x' % ord(char),\n",
    "          char.center(6),\n",
    "          're_dig' if re_digit.match(char) else '-',\n",
    "          'isdig' if char.isdigit() else '-',\n",
    "          'isnum' if char.isnumeric() else '-',\n",
    "          format(unicodedata.numeric(char), '5.2f'),\n",
    "          unicodedata.name(char),\n",
    "          sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual-Mode str and bytes API's\n",
    "\n",
    "The standard library has functions that accept str or bytes arguments and behave differently depending on the type. Some examples are in the re and os modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str Versus bytes in Regular Expressions\n",
    "\n",
    "If you build a regular expression with bytes, patterns such as \\d and \\w only match ASCII characters; in contrast, if these patterns are given as str, they match Unicode digits or letters beyond ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " 'Ramanujan saw ௧௭௨௯ as 1729 = 1³ + 12³ = 9³ + 10³.'\n",
      "Numbers\n",
      " str : ['௧௭௨௯', '1729', '1', '12', '9', '10']\n",
      " bytes: [b'1729', b'1', b'12', b'9', b'10']\n",
      "Words\n",
      " str : ['Ramanujan', 'saw', '௧௭௨௯', 'as', '1729', '1³', '12³', '9³', '10³']\n",
      " bytes: [b'Ramanujan', b'saw', b'as', b'1729', b'1', b'12', b'9', b'10']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re_numbers_str = re.compile(r'\\d+')\n",
    "re_words_str = re.compile(r'\\w+')\n",
    "re_numbers_bytes = re.compile(rb'\\d+')\n",
    "re_words_bytes = re.compile(rb'\\w+')\n",
    "\n",
    "text_str = (\"Ramanujan saw \\u0be7\\u0bed\\u0be8\\u0bef\"\n",
    "            \" as 1729 = 1³ + 12³ = 9³ + 10³.\")\n",
    "text_bytes = text_str.encode('utf_8')\n",
    "print('Text', repr(text_str), sep='\\n ')\n",
    "print('Numbers')\n",
    "print(' str :', re_numbers_str.findall(text_str))\n",
    "print(' bytes:', re_numbers_bytes.findall(text_bytes))\n",
    "print('Words')\n",
    "print(' str :', re_words_str.findall(text_str))\n",
    "print(' bytes:', re_words_bytes.findall(text_bytes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str Versus bytes on os Functions\n",
    "\n",
    "The GNU/Linux kernel is not Unicode savvy, so in the real world you may find filenames\n",
    "made of byte sequences that are not valid in any sensible encoding scheme, and cannot\n",
    "be decoded to str. File servers with clients using a variety of OSes are particularly prone\n",
    "to this problem.\n",
    "In order to work around this issue, all os module functions that accept filenames or\n",
    "pathnames take arguments as str or bytes. If one such function is called with a str\n",
    "argument, the argument will be automatically converted using the codec named by\n",
    "sys.getfilesystemencoding(), and the OS response will be decoded with the same\n",
    "codec. This is almost always what you want, in keeping with the Unicode sandwich best\n",
    "practice.\n",
    "But if you must deal with (and perhaps fix) filenames that cannot be handled in that\n",
    "way, you can pass bytes arguments to the os functions to get bytes return values. This\n",
    "130 | Chapter 4: Text versus Bytes\n",
    "feature lets you deal with any file or pathname, no matter how many gremlins you may\n",
    "find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ch2_AnArrayOfSequences.ipynb',\n",
       " 'Ch3_DictionariesAndSets.ipynb',\n",
       " 'Ch4_Text_Vs_Bytes.ipynb',\n",
       " 'dummy',\n",
       " 'floats.bin']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Ch2_AnArrayOfSequences.ipynb',\n",
       " b'Ch3_DictionariesAndSets.ipynb',\n",
       " b'Ch4_Text_Vs_Bytes.ipynb',\n",
       " b'dummy',\n",
       " b'floats.bin']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(b'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Summary\n",
    "\n",
    "`We started the chapter by dismissing the notion that 1 character == 1 byte.` As the world adopts Unicode (80% of websites already use UTF-8), we need to keep the concept of text strings separated from the binary sequences that represent them in files, andPython 3 enforces this separation.\n",
    "\n",
    "After a brief overview of the binary sequence data types—`bytes, bytearray, and memoryview`—we jumped into encoding and decoding, with a sampling of important codecs,followed by approaches to prevent or deal with the infamous `UnicodeEncodeError, UnicodeDecodeError`, and the `SyntaxError` caused by wrong encoding in Python source files.\n",
    "\n",
    "We then considered the theory and practice of encoding detection in the absence of metadata: in theory, it can’t be done, but in practice the `Chardet package` pulls it offpretty well for a number of popular encodings. `Byte order marks` were then presented as the only encoding hint commonly found in UTF-16 and UTF-32 files—sometimes in UTF-8 files as well.\n",
    "\n",
    "In the next section, we demonstrated opening text files, an easy task except for one pitfall: the e`ncoding= keyword argument is not mandatory when you open a text file, but it should be.` If you fail to specify the encoding, you end up with a program that manages to generate “plain text” that is incompatible across platforms, due to conflicting\n",
    "default encodings. We then exposed the different encoding settings that Python uses as defaults and how to detect them: locale getpreferredencoding(), sys.getfilesys temencoding(), sys.getdefaultencoding(), and the encodings for the standard I/O files (e.g., sys.stdout.encoding). A sad realization for Windows users is that these\n",
    "settings often have distinct values within the same machine, and the values are mutually incompatible; GNU/Linux and OSX users, in contrast, live in a happier place where UTF-8 is the default pretty much everywhere.\n",
    "\n",
    "`Text comparisons are surprisingly complicated because Unicode provides multiple ways of representing some characters, so normalizing is a prerequisite to text matching.` In addition to explaining normalization and case folding, we presented some utility functions that you may adapt to your needs, including drastic transformations like removing all accents. We then saw how to sort Unicode text correctly by leveraging the standard locale module—with some caveats—and an alternative that does not depend on tricky locale configurations: the external PyUCA package.\n",
    "\n",
    "Finally, we glanced at the Unicode database (a source of metadata about every character), and wrapped up with brief discussion of dual-mode APIs (e.g., the `re and os modules, where some functions can be called with str or bytes arguments, prompting different yet fitting results`)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d58cd66771b86600ed196df3ae1631a1e510be967e3cfbdfd84098f57ff410b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
